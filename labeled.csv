,Unnamed: 0,Article_Title,Label,Link,article
0,0,Absolute Value,1,https://en.wikipedia.org/wiki/Absolute_value," in mathematics, the absolute value or modulus of a real number x {\displaystyle x} , denoted | x | {\displaystyle |x|} , is the non-negative value of x {\displaystyle x} without regard to its sign. namely, | x | = x {\displaystyle |x|=x} if x is a positive number, and | x | = − x {\displaystyle |x|=-x} if x {\displaystyle x} is negative (in which case negating x {\displaystyle x} makes − x {\displaystyle -x} positive), and |   | =   {\displaystyle | |= } . for example, the absolute value of   is  , and the absolute value of −  is also  . the absolute value of a number may be thought of as its distance from zero. generalisations of the absolute value for real numbers occur in a wide variety of mathematical settings. for example, an absolute value is also defined for the complex numbers, the quaternions, ordered rings, fields and vector spaces. the absolute value is closely related to the notions of magnitude, distance, and norm in various mathematical and physical contexts. in     , jean-robert argand introduced the term module, meaning unit of measure in french, specifically for the complex absolute value, and it was borrowed into english in      as the latin equivalent modulus. the term absolute value has been used in this sense from at least      in french and      in english. the notation |x|, with a vertical bar on each side, was introduced by karl weierstrass in     . other names for absolute value include numerical value and magnitude. in programming languages and computational software packages, the absolute value of x is generally represented by abs(x), or a similar expression. the vertical bar notation also appears in a number of other mathematical contexts: for example, when applied to a set, it denotes its cardinality; when applied to a matrix, it denotes its determinant. vertical bars denote the absolute value only for algebraic objects for which the notion of an absolute value is defined, notably an element of a normed division algebra, for example a real number, a complex number, or a quaternion. a closely related but distinct notation is the use of vertical bars for either the euclidean norm or sup norm of a vector in r n {\displaystyle \mathbb {r} ^{n}} , although double vertical bars with subscripts ( ‖ ⋅ ‖   {\displaystyle \|\cdot \|_{ }} and ‖ ⋅ ‖ ∞ {\displaystyle \|\cdot \|_{\infty }} , respectively) are a more common and less ambiguous notation. for any real number x {\displaystyle x} , the absolute value or modulus of x {\displaystyle x} is denoted by | x | {\displaystyle |x|} , with a vertical bar on each side of the quantity, and is defined as the absolute value of x {\displaystyle x} is thus always either a positive number or zero, but never negative. when x {\displaystyle x} itself is negative ( x <   {\displaystyle x< } ), then its absolute value is necessarily positive ( | x | = − x >   {\displaystyle |x|=-x> } ). from an analytic geometry point of view, the absolute value of a real number is that number's distance from zero along the real number line, and more generally the absolute value of the difference of two real numbers is the distance between them. the notion of an abstract distance function in mathematics can be seen to be a generalisation of the absolute value of the difference (see ""distance"" below). since the square root symbol represents the unique positive square root, when applied to a positive number, it follows that the absolute value has the following four fundamental properties (a, b are real numbers), that are used for generalization of this notion to other domains: "
1,1,addition theorem,1,https://en.wikipedia.org/wiki/Addition_theorem,"in mathematics, an addition theorem is a formula such as that for the exponential function: that expresses, for a particular function f, f(x + y) in terms of f(x) and f(y). slightly more generally, as is the case with the trigonometric functions sin and cos, several functions may be involved; this is more apparent than real, in that case, since there cos is an algebraic function of sin (in other words, we usually take their functions both as defined on the unit circle). the scope of the idea of an addition theorem was fully explored in the nineteenth century, prompted by the discovery of the addition theorem for elliptic functions. to ""classify"" addition theorems it is necessary to put some restriction on the type of function g admitted, such that in this identity one can assume that f and g are vector-valued (have several components). an algebraic addition theorem is one in which g can be taken to be a vector of polynomials, in some set of variables. the conclusion of the mathematicians of the time was that the theory of abelian functions essentially exhausted the interesting possibilities: considered as a functional equation to be solved with polynomials, or indeed rational functions or algebraic functions, there were no further types of solution. in more contemporary language this appears as part of the theory of algebraic groups, dealing with commutative groups. the connected, projective variety examples are indeed exhausted by abelian functions, as is shown by a number of results characterising an abelian variety by rather weak conditions on its group law. the so-called quasi-abelian functions are all known to come from extensions of abelian varieties by commutative affine group varieties. therefore, the old conclusions about the scope of global algebraic addition theorems can be said to hold. a more modern aspect is the theory of formal groups. "
2,2,Adjoint of a matrix,2,https://en.wikipedia.org/wiki/Adjugate_matrix,"in linear algebra, the adjugate or classical adjoint of a square matrix is the transpose of its cofactor matrix. it is also occasionally known as adjunct matrix, though this nomenclature appears to have decreased in usage. the adjugate has sometimes been called the ""adjoint"", but today the ""adjoint"" of a matrix normally refers to its corresponding adjoint operator, which is its conjugate transpose. the adjugate of a is the transpose of the cofactor matrix c of a, in more detail, suppose r is a unital commutative ring and a is an n × n matrix with entries from r. the (i, j)-minor of a, denoted mij, is the determinant of the (n −  ) × (n −  ) matrix that results from deleting row i and column j of a. the cofactor matrix of a is the n × n matrix c whose (i, j) entry is the (i, j) cofactor of a, which is the (i, j)-minor times a sign factor: the adjugate of a is the transpose of c, that is, the n × n matrix whose (i, j) entry is the (j, i) cofactor of a, the adjugate is defined as it is so that the product of a with its adjugate yields a diagonal matrix whose diagonal entries are the determinant det(a). that is, where i is the n × n identity matrix. this is a consequence of the laplace expansion of the determinant. the above formula implies one of the fundamental results in matrix algebra, that a is invertible if and only if det(a) is an invertible element of r. when this holds, the equation above yields the adjugate of any non-zero   ×   matrix (complex scalar) is i = [   ] {\displaystyle \mathbf {i} ={\begin{bmatrix} \end{bmatrix}}} . by convention, adj( ) =  . the adjugate of the   ×   matrix "
3,3,Algebra of sets,2,https://en.wikipedia.org/wiki/Algebra_of_sets,"in mathematics, the algebra of sets, not to be confused with the mathematical structure of an algebra of sets, defines the properties and laws of sets, the set-theoretic operations of union, intersection, and complementation and the relations of set equality and set inclusion. it also provides systematic procedures for evaluating expressions, and performing calculations, involving these operations and relations. any set of sets closed under the set-theoretic operations forms a boolean algebra with the join operator being union, the meet operator being intersection, the complement operator being set complement, the bottom being ∅ {\displaystyle \varnothing } and the top being the universe set under consideration. the algebra of sets is the set-theoretic analogue of the algebra of numbers. just as arithmetic addition and multiplication are associative and commutative, so are set union and intersection; just as the arithmetic relation ""less than or equal"" is reflexive, antisymmetric and transitive, so is the set relation of ""subset"". it is the algebra of the set-theoretic operations of union, intersection and complementation, and the relations of equality and inclusion. for a basic introduction to sets see the article on sets, for a fuller account see naive set theory, and for a full rigorous axiomatic treatment see axiomatic set theory. the binary operations of set union ( ∪ {\displaystyle \cup } ) and intersection ( ∩ {\displaystyle \cap } ) satisfy many identities. several of these identities or ""laws"" have well established names. the union and intersection of sets may be seen as analogous to the addition and multiplication of numbers. like addition and multiplication, the operations of union and intersection are commutative and associative, and intersection distributes over union. however, unlike addition and multiplication, union also distributes over intersection. two additional pairs of properties involve the special sets called the empty set ø and the universe set u {\displaystyle u} ; together with the complement operator ( a c {\displaystyle a^{c}} denotes the complement of a {\displaystyle a} . this can also be written as a ′ {\displaystyle a'} , read as a prime). the empty set has no members, and the universe set has all possible members (in a particular context). the identity expressions (together with the commutative expressions) say that, just like   and   for addition and multiplication, ø and u are the identity elements for union and intersection, respectively. unlike addition and multiplication, union and intersection do not have inverse elements. however the complement laws give the fundamental properties of the somewhat inverse-like unary operation of set complementation. the preceding five pairs of formulae—the commutative, associative, distributive, identity and complement formulae—encompass all of set algebra, in the sense that every valid proposition in the algebra of sets can be derived from them. "
4,4,Algorithm,2,https://en.wikipedia.org/wiki/Algorithm," in mathematics and computer science, an algorithm (/ˈælɡərɪðəm/ (listen)) is a finite sequence of well-defined instructions, typically used to solve a class of specific problems or to perform a computation. algorithms are used as specifications for performing calculations and data processing. by making use of artificial intelligence, algorithms can perform automated deductions (referred to as automated reasoning) and use mathematical and logical tests to divert the code through various routes (referred to as automated decision-making). using human characteristics as descriptors of machines in metaphorical ways was already practiced by alan turing with terms such as ""memory"", ""search"" and ""stimulus"". in contrast, a heuristic is an approach to problem solving that may not be fully specified or may not guarantee correct or optimal results, especially in problem domains where there is no well-defined correct or optimal result. as an effective method, an algorithm can be expressed within a finite amount of space and time, and in a well-defined formal language for calculating a function. starting from an initial state and initial input (perhaps empty), the instructions describe a computation that, when executed, proceeds through a finite number of well-defined successive states, eventually producing ""output"" and terminating at a final ending state. the transition from one state to the next is not necessarily deterministic; some algorithms, known as randomized algorithms, incorporate random input. the concept of algorithm has existed since antiquity. arithmetic algorithms, such as a division algorithm, were used by ancient babylonian mathematicians c.      bc and egyptian mathematicians c.      bc. greek mathematicians later used algorithms in     bc in the sieve of eratosthenes for finding prime numbers, and the euclidean algorithm for finding the greatest common divisor of two numbers. arabic mathematicians such as al-kindi in the  th century used cryptographic algorithms for code-breaking, based on frequency analysis. the word algorithm is derived from the name of the  th-century persian mathematician muḥammad ibn mūsā al-khwārizmī, whose nisba (identifying him as from khwarazm) was latinized as algoritmi (arabized persian الخوارزمی c.    –   ). muḥammad ibn mūsā al-khwārizmī was a mathematician, astronomer, geographer, and scholar in the house of wisdom in baghdad, whose name means 'the native of khwarazm', a region that was part of greater iran and is now in uzbekistan. about    , al-khwarizmi wrote an arabic language treatise on the hindu–arabic numeral system, which was translated into latin during the   th century. the manuscript starts with the phrase dixit algorizmi ('thus spake al-khwarizmi'), where ""algorizmi"" was the translator's latinization of al-khwarizmi's name. al-khwarizmi was the most widely read mathematician in europe in the late middle ages, primarily through another of his books, the algebra. in late medieval latin, algorismus, english 'algorism', the corruption of his name, simply meant the ""decimal number system"". in the   th century, under the influence of the greek word ἀριθμός (arithmos), 'number' (cf. 'arithmetic'), the latin word was altered to algorithmus, and the corresponding english term 'algorithm' is first attested in the   th century; the modern sense was introduced in the   th century. indian mathematics was predominantly algorithmic. algorithms that are representative of the indian mathematical tradition range from the ancient śulbasūtrās to the medieval texts of the kerala school. in english, the word algorithm was first used in about      and then by chaucer in     . english adopted the french term, but it was not until the late   th century that ""algorithm"" took on the meaning that it has in modern english. another early use of the word is from     , in a manual titled carmen de algorismo composed by alexandre de villedieu. it begins with: haec algorismus ars praesens dicitur, in qua / talibus indorum fruimur bis quinque figuris."
5,5,Altitude (triangle),2,https://en.wikipedia.org/wiki/Altitude_(triangle),"in geometry, an altitude of a triangle is a line segment through a vertex and perpendicular to (i.e., forming a right angle with) a line containing the base (the side opposite the vertex). this line containing the opposite side is called the extended base of the altitude. the intersection of the extended base and the altitude is called the foot of the altitude. the length of the altitude, often simply called ""the altitude"", is the distance between the extended base and the vertex. the process of drawing the altitude from the vertex to the foot is known as dropping the altitude at that vertex. it is a special case of orthogonal projection. altitudes can be used in the computation of the area of a triangle: one half of the product of an altitude's length and its base's length equals the triangle's area. thus, the longest altitude is perpendicular to the shortest side of the triangle. the altitudes are also related to the sides of the triangle through the trigonometric functions. in an isosceles triangle (a triangle with two congruent sides), the altitude having the incongruent side as its base will have the midpoint of that side as its foot. also the altitude having the incongruent side as its base will be the angle bisector of the vertex angle. it is common to mark the altitude with the letter h (as in height), often subscripted with the name of the side the altitude is drawn to. in a right triangle, the altitude drawn to the hypotenuse c divides the hypotenuse into two segments of lengths p and q. if we denote the length of the altitude by hc, we then have the relation for acute triangles the feet of the altitudes all fall on the triangle's sides (not extended). in an obtuse triangle (one with an obtuse angle), the foot of the altitude to the obtuse-angled vertex falls in the interior of the opposite side, but the feet of the altitudes to the acute-angled vertices fall on the opposite extended side, exterior to the triangle. this is illustrated in the adjacent diagram: in this obtuse triangle, an altitude dropped perpendicularly from the top vertex, which has an acute angle, intersects the extended horizontal side outside the triangle. the three (possibly extended) altitudes intersect in a single point, called the orthocenter of the triangle, usually denoted by h. the orthocenter lies inside the triangle if and only if the triangle is acute (i.e. does not have an angle greater than or equal to a right angle). if one angle is a right angle, the orthocenter coincides with the vertex at the right angle. let a, b, c denote the vertices and also the angles of the triangle, and let a = |bc|, b = |ca|, c = |ab| be the side lengths. the orthocenter has trilinear coordinates sec ⁡ a : sec ⁡ b : sec ⁡ c = cos ⁡ a − sin ⁡ b sin ⁡ c : cos ⁡ b − sin ⁡ c sin ⁡ a : cos ⁡ c − sin ⁡ a sin ⁡ b , {\displaystyle \sec a:\sec b:\sec c=\cos a-\sin b\sin c:\cos b-\sin c\sin a:\cos c-\sin a\sin b,} and barycentric coordinates "
6,6,Angular Bisector Theorem,1,https://en.wikipedia.org/wiki/Angle_bisector_theorem,"in geometry, the angle bisector theorem is concerned with the relative lengths of the two segments that a triangle's side is divided into by a line that bisects the opposite angle. it equates their relative lengths to the relative lengths of the other two sides of the triangle. consider a triangle abc. let the angle bisector of angle a intersect side bc at a point d between b and c. the angle bisector theorem states that the ratio of the length of the line segment bd to the length of segment cd is equal to the ratio of the length of side ab to the length of side ac: and conversely, if a point d on the side bc of triangle abc divides bc in the same ratio as the sides ab and ac, then ad is the angle bisector of angle ∠ a. the generalized angle bisector theorem states that if d lies on the line bc, then this reduces to the previous version if ad is the bisector of ∠ bac. when d is external to the segment bc, directed line segments and directed angles must be used in the calculation. the angle bisector theorem is commonly used when the angle bisectors and side lengths are known. it can be used in a calculation or in a proof. an immediate consequence of the theorem is that the angle bisector of the vertex angle of an isosceles triangle will also bisect the opposite side. in the above diagram, use the law of sines on triangles abd and acd: "
7,7,Antiderivative,2,https://en.wikipedia.org/wiki/Antiderivative,"in calculus, an antiderivative, inverse derivative, primitive function, primitive integral or indefinite integral[note  ] of a function f is a differentiable function f whose derivative is equal to the original function f. this can be stated symbolically as f' = f. the process of solving for antiderivatives is called antidifferentiation (or indefinite integration), and its opposite operation is called differentiation, which is the process of finding a derivative. antiderivatives are often denoted by capital roman letters such as f and g. antiderivatives are related to definite integrals through the second fundamental theorem of calculus: the definite integral of a function over a closed interval where the function is riemann integrable is equal to the difference between the values of an antiderivative evaluated at the endpoints of the interval. in physics, antiderivatives arise in the context of rectilinear motion (e.g., in explaining the relationship between position, velocity and acceleration). the discrete equivalent of the notion of antiderivative is antidifference. the function f ( x ) = x     {\displaystyle f(x)={\tfrac {x^{ }}{ }}} is an antiderivative of f ( x ) = x   {\displaystyle f(x)=x^{ }} , since the derivative of x     {\displaystyle {\tfrac {x^{ }}{ }}} is x   {\displaystyle x^{ }} , and since the derivative of a constant is zero, x   {\displaystyle x^{ }} will have an infinite number of antiderivatives, such as x     , x     +   , x     −   {\displaystyle {\tfrac {x^{ }}{ }},{\tfrac {x^{ }}{ }}+ ,{\tfrac {x^{ }}{ }}- } , etc. thus, all the antiderivatives of x   {\displaystyle x^{ }} can be obtained by changing the value of c in f ( x ) = x     + c {\displaystyle f(x)={\tfrac {x^{ }}{ }}+c} , where c is an arbitrary constant known as the constant of integration. essentially, the graphs of antiderivatives of a given function are vertical translations of each other, with each graph's vertical location depending upon the value c. more generally, the power function f ( x ) = x n {\displaystyle f(x)=x^{n}} has antiderivative f ( x ) = x n +   n +   + c {\displaystyle f(x)={\tfrac {x^{n+ }}{n+ }}+c} if n ≠ − , and f ( x ) = ln ⁡ | x | + c {\displaystyle f(x)=\ln |x|+c} if n = − . in physics, the integration of acceleration yields velocity plus a constant. the constant is the initial velocity term that would be lost upon taking the derivative of velocity, because the derivative of a constant term is zero. this same pattern applies to further integrations and derivatives of motion (position, velocity, acceleration, and so on). antiderivatives can be used to compute definite integrals, using the fundamental theorem of calculus: if f is an antiderivative of the integrable function f over the interval [ a , b ] {\displaystyle [a,b]} , then: because of this, each of the infinitely many antiderivatives of a given function f may be called the ""indefinite integral"" of f and written using the integral symbol with no bounds: if f is an antiderivative of f, and the function f is defined on some interval, then every other antiderivative g of f differs from f by a constant: there exists a number c such that g ( x ) = f ( x ) + c {\displaystyle g(x)=f(x)+c} for all x. c is called the constant of integration. if the domain of f is a disjoint union of two or more (open) intervals, then a different constant of integration may be chosen for each of the intervals. for instance is the most general antiderivative of f ( x ) =   / x   {\displaystyle f(x)= /x^{ }} on its natural domain ( − ∞ ,   ) ∪ (   , ∞ ) . {\displaystyle (-\infty , )\cup ( ,\infty ).} "
8,8,Area under curve,3,https://en.wikipedia.org/wiki/Area_under_the_curve_(pharmacokinetics),"in the field of pharmacokinetics, the area under the curve (auc) is the definite integral of a curve that describes the variation of a drug concentration in blood plasma as a function of time (this can be done using liquid chromatography–mass spectrometry ). in practice, the drug concentration is measured at certain discrete points in time and the trapezoidal rule is used to estimate auc. the auc (from zero to infinity) represents the total drug exposure across time. auc is a useful metric when trying to determine whether two formulations of the same dose (for example a capsule and a tablet) result in equal amounts of tissue or plasma exposure. another use is in the therapeutic drug monitoring of drugs with a narrow therapeutic index. for example, gentamicin is an antibiotic that can be nephrotoxic (kidney damaging) and ototoxic (hearing damaging); measurement of gentamicin through concentrations in a patient's plasma and calculation of the auc is used to guide the dosage of this drug. auc becomes useful for knowing the average concentration over a time interval, auc/t. also, auc is referenced when talking about elimination. the amount eliminated by the body (mass) = clearance (volume/time) * auc (mass*time/volume). in pharmacokinetics, bioavailability generally refers to the fraction of drug that is absorbed systemically and is thus available to produce a biological effect. this is often measured by quantifying the ""auc"". in order to determine the respective aucs, the serum concentration vs. time plots are typically gathered using c-   labeled drugs and ams (accelerated mass spectrometry). bioavailability can be measured in terms of ""absolute bioavailability"" or ""relative bioavailability"". absolute bioavailablity refers to the bioavailability of drug when administered via a non-intravenous (non-iv) dosage form (i.e. oral tablet, suppository, subcutaneous, etc.) compared with the bioavailability of the same drug administered intravenously (iv). this is done by comparing the auc of the non-intravenous dosage form with the auc for the drug administered intravenously. this fraction is normalized by multiplying by each dosage form's respective dose. relative bioavailability compares the bioavailability between two different dosage forms. again, the relative aucs are used to make this comparison and relative doses are used to normalize the calculation. "
9,9,Arithmetic mean,1,https://en.wikipedia.org/wiki/Arithmetic_mean,"in mathematics and statistics, the arithmetic mean ( /ˌærɪθˈmɛtɪk ˈmiːn/ air-ith-met-ik) or arithmetic average, or simply just the mean or the average (when the context is clear), is the sum of a collection of numbers divided by the count of numbers in the collection. the collection is often a set of results of an experiment or an observational study, or frequently a set of results from a survey. the term ""arithmetic mean"" is preferred in some contexts in mathematics and statistics, because it helps distinguish it from other means, such as the geometric mean and the harmonic mean. in addition to mathematics and statistics, the arithmetic mean is used frequently in many diverse fields such as economics, anthropology and history, and it is used in almost every academic field to some extent. for example, per capita income is the arithmetic average income of a nation's population. while the arithmetic mean is often used to report central tendencies, it is not a robust statistic, meaning that it is greatly influenced by outliers (values that are very much larger or smaller than most of the values). for skewed distributions, such as the distribution of income for which a few people's incomes are substantially greater than most people's, the arithmetic mean may not coincide with one's notion of ""middle"", and robust statistics, such as the median, may provide better description of central tendency. given a data set x = { x   , … , x n } {\displaystyle x=\{x_{ },\ldots ,x_{n}\}} , the arithmetic mean (or mean or average), denoted x ¯ {\displaystyle {\bar {x}}} (read x {\displaystyle x} bar), is the mean of the n {\displaystyle n} values x   , x   , … , x n {\displaystyle x_{ },x_{ },\ldots ,x_{n}} . the arithmetic mean is the most commonly used and readily understood measure of central tendency in a data set. in statistics, the term average refers to any of the measures of central tendency. the arithmetic mean of a set of observed data is defined as being equal to the sum of the numerical values of each and every observation, divided by the total number of observations. symbolically, if we have a data set consisting of the values a   , a   , … , a n {\displaystyle a_{ },a_{ },\ldots ,a_{n}} , then the arithmetic mean a {\displaystyle a} is defined by the formula: (for an explanation of the summation operator, see summation.) for example, consider the monthly salary of    employees of a firm:     ,     ,     ,     ,     ,     ,     ,     ,     ,     . the arithmetic mean is if the data set is a statistical population (i.e., consists of every possible observation and not just a subset of them), then the mean of that population is called the population mean, and denoted by the greek letter μ {\displaystyle \mu } . if the data set is a statistical sample (a subset of the population), then we call the statistic resulting from this calculation a sample mean (which for a data set x {\displaystyle x} is denoted as x ¯ {\displaystyle {\overline {x}}} ). the arithmetic mean can be similarly defined for vectors in multiple dimension, not only scalar values; this is often referred to as a centroid. more generally, because the arithmetic mean is a convex combination (coefficients sum to  ), it can be defined on a convex space, not only a vector space. the arithmetic mean has several properties that make it useful, especially as a measure of central tendency. these include: "
10,10,Arithmetic progression,1,https://en.wikipedia.org/wiki/Arithmetic_progression,"an arithmetic progression or arithmetic sequence is a sequence of numbers such that the difference between the consecutive terms is constant. for instance, the sequence  ,  ,  ,   ,   ,   , . . . is an arithmetic progression with a common difference of  . if the initial term of an arithmetic progression is a   {\displaystyle a_{ }} and the common difference of successive members is d {\displaystyle d} , then the n {\displaystyle n} -th term of the sequence ( a n {\displaystyle a_{n}} ) is given by: and in general a finite portion of an arithmetic progression is called a finite arithmetic progression and sometimes just called an arithmetic progression. the sum of a finite arithmetic progression is called an arithmetic series. computation of the sum   +   +   +    +   . when the sequence is reversed and added to itself term by term, the resulting sequence has a single repeated value in it, equal to the sum of the first and last numbers (  +    =   ). thus    ×   =    is twice the sum. the sum of the members of a finite arithmetic progression is called an arithmetic series. for example, consider the sum: this sum can be found quickly by taking the number n of terms being added (here  ), multiplying by the sum of the first and last number in the progression (here   +    =   ), and dividing by  : in the case above, this gives the equation: this formula works for any real numbers a   {\displaystyle a_{ }} and a n {\displaystyle a_{n}} . for example: to derive the above formula, begin by expressing the arithmetic series in two different ways: "
11,11,Arithmetic Geometric Sequence,2,https://en.wikipedia.org/wiki/Arithmetico%E2%80%93geometric_sequence,"in mathematics, arithmetico–geometric sequence is the result of term-by-term multiplication of a geometric progression with the corresponding terms of an arithmetic progression. put plainly, the nth term of an arithmetico–geometric sequence is the product of the nth term of an arithmetic sequence and the nth term of a geometric one. arithmetico–geometric sequences arise in various applications, such as the computation of expected values in probability theory. for instance, the sequence is an arithmetico–geometric sequence. the arithmetic component appears in the numerator (in blue), and the geometric one in the denominator (in green). the summation of this infinite sequence is known as an arithmetico–geometric series, and its most basic form has been called gabriel's staircase: the denomination may also be applied to different objects presenting characteristics of both arithmetic and geometric sequences; for instance the french notion of arithmetico–geometric sequence refers to sequences of the form u n +   = a u n + b {\displaystyle u_{n+ }=au_{n}+b} , which generalise both arithmetic and geometric sequences. such sequences are a special case of linear difference equations. the first few terms of an arithmetico–geometric sequence composed of an arithmetic progression (in blue) with difference d {\displaystyle d} and initial value a {\displaystyle a} and a geometric progression (in green) with initial value b {\displaystyle b} and common ratio r {\displaystyle r} are given by: for instance, the sequence is defined by d = b =   {\displaystyle d=b= } , a =   {\displaystyle a= } , and r =     {\displaystyle r={\frac { }{ }}} . the sum of the first n terms of an arithmetico–geometric sequence has the form where a i {\displaystyle a_{i}} and g i {\displaystyle g_{i}} are the ith terms of the arithmetic and the geometric sequence, respectively. this sum has the closed-form expression "
12,12,Arithmetico–geometric sequence,3,https://en.wikipedia.org/wiki/Arithmetico–geometric_sequence,
13,13,Asymptote,2,https://en.wikipedia.org/wiki/Asymptote,"in analytic geometry, an asymptote (/ˈæsɪmptoʊt/) of a curve is a line such that the distance between the curve and the line approaches zero as one or both of the x or y coordinates tends to infinity. in projective geometry and related contexts, an asymptote of a curve is a line which is tangent to the curve at a point at infinity. the word asymptote is derived from the greek ἀσύμπτωτος (asumptōtos) which means ""not falling together"", from ἀ priv. + σύν ""together"" + πτωτ-ός ""fallen"". the term was introduced by apollonius of perga in his work on conic sections, but in contrast to its modern meaning, he used it to mean any line that does not intersect the given curve. there are three kinds of asymptotes: horizontal, vertical and oblique. for curves given by the graph of a function y = ƒ(x), horizontal asymptotes are horizontal lines that the graph of the function approaches as x tends to +∞ or −∞. vertical asymptotes are vertical lines near which the function grows without bound. an oblique asymptote has a slope that is non-zero but finite, such that the graph of the function approaches it as x tends to +∞ or −∞. more generally, one curve is a curvilinear asymptote of another (as opposed to a linear asymptote) if the distance between the two curves tends to zero as they tend to infinity, although the term asymptote by itself is usually reserved for linear asymptotes. asymptotes convey information about the behavior of curves in the large, and determining the asymptotes of a function is an important step in sketching its graph. the study of asymptotes of functions, construed in a broad sense, forms a part of the subject of asymptotic analysis. the idea that a curve may come arbitrarily close to a line without actually becoming the same may seem to counter everyday experience. the representations of a line and a curve as marks on a piece of paper or as pixels on a computer screen have a positive width. so if they were to be extended far enough they would seem to merge, at least as far as the eye could discern. but these are physical representations of the corresponding mathematical entities; the line and the curve are idealized concepts whose width is   (see line). therefore, the understanding of the idea of an asymptote requires an effort of reason rather than experience. consider the graph of the function f ( x ) =   x {\displaystyle f(x)={\frac { }{x}}} shown in this section. the coordinates of the points on the curve are of the form ( x ,   x ) {\displaystyle \left(x,{\frac { }{x}}\right)} where x is a number other than  . for example, the graph contains the points ( ,  ), ( ,  . ), ( ,  . ), (  ,  . ), ... as the values of x {\displaystyle x} become larger and larger, say    ,  ,   ,   ,    ..., putting them far to the right of the illustration, the corresponding values of y {\displaystyle y} , .  , .   , .    , ..., become infinitesimal relative to the scale shown. but no matter how large x {\displaystyle x} becomes, its reciprocal   x {\displaystyle {\frac { }{x}}} is never  , so the curve never actually touches the x-axis. similarly, as the values of x {\displaystyle x} become smaller and smaller, say .  , .   , .    , ..., making them infinitesimal relative to the scale shown, the corresponding values of y {\displaystyle y} ,    ,  ,   ,   ,    ..., become larger and larger. so the curve extends farther and farther upward as it comes closer and closer to the y-axis. thus, both the x and y-axis are asymptotes of the curve. these ideas are part of the basis of concept of a limit in mathematics, and this connection is explained more fully below. the asymptotes most commonly encountered in the study of calculus are of curves of the form y = ƒ(x). these can be computed using limits and classified into horizontal, vertical and oblique asymptotes depending on their orientation. horizontal asymptotes are horizontal lines that the graph of the function approaches as x tends to +∞ or −∞. as the name indicates they are parallel to the x-axis. vertical asymptotes are vertical lines (perpendicular to the x-axis) near which the function grows without bound. oblique asymptotes are diagonal lines such that the difference between the curve and the line approaches   as x tends to +∞ or −∞. the line x = a is a vertical asymptote of the graph of the function y = ƒ(x) if at least one of the following statements is true: where lim x → a − {\displaystyle \lim _{x\to a^{-}}} is the limit as x approaches the value a from the left (from lesser values), and lim x → a + {\displaystyle \lim _{x\to a^{+}}} is the limit as x approaches a from the right. "
14,14,Basis vector,2,https://en.wikipedia.org/wiki/Basis_(linear_algebra),"in mathematics, a set b of vectors in a vector space v is called a basis if every element of v may be written in a unique way as a finite linear combination of elements of b. the coefficients of this linear combination are referred to as components or coordinates of the vector with respect to b. the elements of a basis are called basis vectors. equivalently, a set b is a basis if its elements are linearly independent and every element of v is a linear combination of elements of b. in other words, a basis is a linearly independent spanning set. a vector space can have several bases; however all the bases have the same number of elements, called the dimension of the vector space. this article deals mainly with finite-dimensional vector spaces. however, many of the principles are also valid for infinite-dimensional vector spaces. a basis b of a vector space v over a field f (such as the real numbers r or the complex numbers c) is a linearly independent subset of v that spans v. this means that a subset b of v is a basis if it satisfies the two following conditions: the scalars a i {\displaystyle a_{i}} are called the coordinates of the vector v with respect to the basis b, and by the first property they are uniquely determined. a vector space that has a finite basis is called finite-dimensional. in this case, the finite subset can be taken as b itself to check for linear independence in the above definition. it is often convenient or even necessary to have an ordering on the basis vectors, for example, when discussing orientation, or when one considers the scalar coefficients of a vector with respect to a basis without referring explicitly to the basis elements. in this case, the ordering is necessary for associating each coefficient to the corresponding basis element. this ordering can be done by numbering the basis elements. in order to emphasize that an order has been chosen, one speaks of an ordered basis, which is therefore not simply an unstructured set, but a sequence, an indexed family, or similar; see § ordered bases and coordinates below. the set r  of the ordered pairs of real numbers is a vector space under the operations of component-wise addition more generally, if f is a field, the set f n {\displaystyle f^{n}} of n-tuples of elements of f is a vector space for similarly defined addition and scalar multiplication. let "
15,15,Bayes Theorem,2,https://en.wikipedia.org/wiki/Bayes%27_theorem,"in probability theory and statistics, bayes' theorem (alternatively bayes' law or bayes' rule; recently bayes–price theorem :   ,   ,    and    ), named after thomas bayes, describes the probability of an event, based on prior knowledge of conditions that might be related to the event. for example, if the risk of developing health problems is known to increase with age, bayes' theorem allows the risk to an individual of a known age to be assessed more accurately (by conditioning it on their age) than simply assuming that the individual is typical of the population as a whole. one of the many applications of bayes' theorem is bayesian inference, a particular approach to statistical inference. when applied, the probabilities involved in the theorem may have different probability interpretations. with bayesian probability interpretation, the theorem expresses how a degree of belief, expressed as a probability, should rationally change to account for the availability of related evidence. bayesian inference is fundamental to bayesian statistics. bayes' theorem is stated mathematically as the following equation: p ( a ∣ b ) = p ( b ∣ a ) p ( a ) p ( b ) {\displaystyle p(a\mid b)={\frac {p(b\mid a)p(a)}{p(b)}}} where a {\displaystyle a} and b {\displaystyle b} are events and p ( b ) ≠   {\displaystyle p(b)\neq  } . bayes' theorem may be derived from the definition of conditional probability: where p ( a ∩ b ) {\displaystyle p(a\cap b)} is the probability of both a and b being true. similarly, solving for p ( a ∩ b ) {\displaystyle p(a\cap b)} and substituting into the above expression for p ( a ∣ b ) {\displaystyle p(a\mid b)} yields bayes' theorem: for two continuous random variables x and y, bayes' theorem may be analogously derived from the definition of conditional density: therefore, "
16,16,Bayesian Probability,3,https://en.wikipedia.org/wiki/Bayesian_probability,"bayesian probability is an interpretation of the concept of probability, in which, instead of frequency or propensity of some phenomenon, probability is interpreted as reasonable expectation representing a state of knowledge or as quantification of a personal belief. the bayesian interpretation of probability can be seen as an extension of propositional logic that enables reasoning with hypotheses; that is, with propositions whose truth or falsity is unknown. in the bayesian view, a probability is assigned to a hypothesis, whereas under frequentist inference, a hypothesis is typically tested without being assigned a probability. bayesian probability belongs to the category of evidential probabilities; to evaluate the probability of a hypothesis, the bayesian probabilist specifies a prior probability. this, in turn, is then updated to a posterior probability in the light of new, relevant data (evidence). the bayesian interpretation provides a standard set of procedures and formulae to perform this calculation. the term bayesian derives from the   th-century mathematician and theologian thomas bayes, who provided the first mathematical treatment of a non-trivial problem of statistical data analysis using what is now known as bayesian inference. :     mathematician pierre-simon laplace pioneered and popularized what is now called bayesian probability. :   –   bayesian methods are characterized by concepts and procedures as follows: broadly speaking, there are two interpretations of bayesian probability. for objectivists, who interpret probability as an extension of logic, probability quantifies the reasonable expectation that everyone (even a ""robot"") who shares the same knowledge should share in accordance with the rules of bayesian statistics, which can be justified by cox's theorem. for subjectivists, probability corresponds to a personal belief. rationality and coherence allow for substantial variation within the constraints they pose; the constraints are justified by the dutch book argument or by decision theory and de finetti's theorem. the objective and subjective variants of bayesian probability differ mainly in their interpretation and construction of the prior probability. the term bayesian derives from thomas bayes (    –    ), who proved a special case of what is now called bayes' theorem in a paper titled ""an essay towards solving a problem in the doctrine of chances"". in that special case, the prior and posterior distributions were beta distributions and the data came from bernoulli trials. it was pierre-simon laplace (    –    ) who introduced a general version of the theorem and used it to approach problems in celestial mechanics, medical statistics, reliability, and jurisprudence. early bayesian inference, which used uniform priors following laplace's principle of insufficient reason, was called ""inverse probability"" (because it infers backwards from observations to parameters, or from effects to causes). after the     s, ""inverse probability"" was largely supplanted by a collection of methods that came to be called frequentist statistics. in the   th century, the ideas of laplace developed in two directions, giving rise to objective and subjective currents in bayesian practice. harold jeffreys' theory of probability (first published in     ) played an important role in the revival of the bayesian view of probability, followed by works by abraham wald (    ) and leonard j. savage (    ). the adjective bayesian itself dates to the     s; the derived bayesianism, neo-bayesianism is of     s coinage. in the objectivist stream, the statistical analysis depends on only the model assumed and the data analysed. no subjective decisions need to be involved. in contrast, ""subjectivist"" statisticians deny the possibility of fully objective analysis for the general case. in the     s, there was a dramatic growth in research and applications of bayesian methods, mostly attributed to the discovery of markov chain monte carlo methods and the consequent removal of many of the computational problems, and to an increasing interest in nonstandard, complex applications. while frequentist statistics remains strong (as demonstrated by the fact that much of undergraduate teaching is based on it ), bayesian methods are widely accepted and used, e.g., in the field of machine learning. the use of bayesian probabilities as the basis of bayesian inference has been supported by several arguments, such as cox axioms, the dutch book argument, arguments based on decision theory and de finetti's theorem. "
17,17,Bennett's inequality,1,https://en.wikipedia.org/wiki/Bennett%27s_inequality,"in probability theory, bennett's inequality provides an upper bound on the probability that the sum of independent random variables deviates from its expected value by more than any specified amount. bennett's inequality was proved by george bennett of the university of new south wales in     . let x , … xn be independent random variables with finite variance and assume (for simplicity but without loss of generality) they all have zero expected value. further assume xi ≤ a almost surely for all i, and define s n = ∑ i =   n x i − e ⁡ ( x i ) {\displaystyle s_{n}=\sum _{i= }^{n}x_{i}-\operatorname {e} (x_{i})} and σ   = ∑ i =   n e ⁡ ( x i   ) . {\displaystyle \sigma ^{ }=\sum _{i= }^{n}\operatorname {e} (x_{i}^{ }).} then for any t ≥  , where h(u) = (  + u)log(  + u) – u and log denotes the natural logarithm. for generalizations see freedman (    ) and fan, grama and liu (    ) for a martingale version of bennett's inequality and its improvement, respectively. hoeffding's inequality only assumes the summands are bounded almost surely, while bennett's inequality offers some improvement when the variances of the summands are small compared to their almost sure bounds. however hoeffding's inequality entails sub-gaussian tails, whereas in general bennett's inequality has poissonian tails.[citation needed] bennett's inequality is most similar to the bernstein inequalities, the first of which also gives concentration in terms of the variance and almost sure bound on the individual terms. bennett's inequality is stronger than this bound, but more complicated to compute. in both inequalities, unlike some other inequalities or limit theorems, there is no requirement that the component variables have identical or similar distributions.[citation needed] suppose that each xi is an independent binary random variable with probability p. then bennett's inequality says that: for t ≥    n p {\displaystyle t\geq   np} , h ( t n p ) ≥ t   n p log ⁡ t n p , {\displaystyle h({\frac {t}{np}})\geq {\frac {t}{ np}}\log {\frac {t}{np}},} so for t ≥    n p {\displaystyle t\geq   np} . "
18,18,Transformation to a linear differential equation,3,https://en.wikipedia.org/wiki/Bernoulli_differential_equation,"in mathematics, an ordinary differential equation is called a bernoulli differential equation if it is of the form where n {\displaystyle n} is a real number. some authors allow any real n {\displaystyle n} , whereas others require that n {\displaystyle n} not be   or  . the equation was first discussed in a work of      by jacob bernoulli, after whom it is named. the earliest solution, however, was offered by gottfried leibniz, who published his result in the same year and whose method is the one still used today. bernoulli equations are special because they are nonlinear differential equations with known exact solutions. a notable special case of the bernoulli equation is the logistic differential equation. when n =   {\displaystyle n= } , the differential equation is linear. when n =   {\displaystyle n= } , it is separable. in these cases, standard techniques for solving equations of those forms can be applied. for n ≠   {\displaystyle n\neq  } and n ≠   {\displaystyle n\neq  } , the substitution u = y   − n {\displaystyle u=y^{ -n}} reduces any bernoulli equation to a linear differential equation for example, in the case n =   {\displaystyle n= } , making the substitution u = y −   {\displaystyle u=y^{- }} in the differential equation d y d x +   x y = x y   {\displaystyle {\frac {dy}{dx}}+{\frac { }{x}}y=xy^{ }} produces the equation d u d x −   x u = − x {\displaystyle {\frac {du}{dx}}-{\frac { }{x}}u=-x} , which is a linear differential equation. let x   ∈ ( a , b ) {\displaystyle x_{ }\in (a,b)} and be a solution of the linear differential equation then we have that y ( x ) := [ z ( x ) ]     − α {\displaystyle y(x):=[z(x)]^{\frac { }{ -\alpha }}} is a solution of and for every such differential equation, for all α >   {\displaystyle \alpha > } we have y ≡   {\displaystyle y\equiv  } as solution for y   =   {\displaystyle y_{ }= } . consider the bernoulli equation "
19,19,Bernoulli trials,1,https://en.wikipedia.org/wiki/Bernoulli_trial,"in the theory of probability and statistics, a bernoulli trial (or binomial trial) is a random experiment with exactly two possible outcomes, ""success"" and ""failure"", in which the probability of success is the same every time the experiment is conducted. it is named after jacob bernoulli, a   th-century swiss mathematician, who analyzed them in his ars conjectandi (    ). the mathematical formalisation of the bernoulli trial is known as the bernoulli process. this article offers an elementary introduction to the concept, whereas the article on the bernoulli process offers a more advanced treatment. since a bernoulli trial has only two possible outcomes, it can be framed as some ""yes or no"" question. for example: therefore, success and failure are merely labels for the two outcomes, and should not be construed literally. the term ""success"" in this sense consists in the result meeting specified conditions, not in any moral judgement. more generally, given any probability space, for any event (set of outcomes), one can define a bernoulli trial, corresponding to whether the event occurred or not (event or complementary event). examples of bernoulli trials include: independent repeated trials of an experiment with exactly two possible outcomes are called bernoulli trials. call one of the outcomes ""success"" and the other outcome ""failure"". let p {\displaystyle p} be the probability of success in a bernoulli trial, and q {\displaystyle q} be the probability of failure. then the probability of success and the probability of failure sum to one, since these are complementary events: ""success"" and ""failure"" are mutually exclusive and exhaustive. thus one has the following relations: alternatively, these can be stated in terms of odds: given probability p of success and q of failure, the odds for are p : q {\displaystyle p:q} and the odds against are q : p . {\displaystyle q:p.} these can also be expressed as numbers, by dividing, yielding the odds for, o f {\displaystyle o_{f}} , and the odds against, o a : {\displaystyle o_{a}:} , these are multiplicative inverses, so they multiply to  , with the following relations: in the case that a bernoulli trial is representing an event from finitely many equally likely outcomes, where s of the outcomes are success and f of the outcomes are failure, the odds for are s : f {\displaystyle s:f} and the odds against are f : s . {\displaystyle f:s.} this yields the following formulas for probability and odds: note that here the odds are computed by dividing the number of outcomes, not the probabilities, but the proportion is the same, since these ratios only differ by multiplying both terms by the same constant factor. random variables describing bernoulli trials are often encoded using the convention that   = ""success"",   = ""failure"". "
20,20,Bertrand's postulate,1,https://en.wikipedia.org/wiki/Bertrand%27s_postulate,"in number theory, bertrand's postulate is a theorem stating that for any integer n >   {\displaystyle n> } , there always exists at least one prime number p {\displaystyle p} with a less restrictive formulation is: for every n >   {\displaystyle n> } there is always at least one prime p {\displaystyle p} such that another formulation, where p n {\displaystyle p_{n}} is the n {\displaystyle n} -th prime, is for n ≥   {\displaystyle n\geq  } this statement was first conjectured in      by joseph bertrand (    –    ). bertrand himself verified his statement for all integers   ≤ n ≤           {\displaystyle  \leq n\leq  \,   \,   } . his conjecture was completely proved by chebyshev (    –    ) in      and so the postulate is also called the bertrand–chebyshev theorem or chebyshev's theorem. chebyshev's theorem can also be stated as a relationship with π ( x ) {\displaystyle \pi (x)} , where π ( x ) {\displaystyle \pi (x)} is the prime-counting function (number of primes less than or equal to x {\displaystyle x\,} ): the prime number theorem (pnt) implies that the number of primes up to x is roughly x/ln(x), so if we replace x with  x then we see the number of primes up to  x is asymptotically twice the number of primes up to x (the terms ln( x) and ln(x) are asymptotically equivalent). therefore, the number of primes between n and  n is roughly n/ln(n) when n is large, and so in particular there are many more primes in this interval than are guaranteed by bertrand's postulate. so bertrand's postulate is comparatively weaker than the pnt. but pnt is a deep theorem, while bertrand's postulate can be stated more memorably and proved more easily, and also makes precise claims about what happens for small values of n. (in addition, chebyshev's theorem was proved before the pnt and so has historical interest.) the similar and still unsolved legendre's conjecture asks whether for every n ≥  , there is a prime p, such that n  < p < (n +  ) . again we expect that there will be not just one but many primes between n  and (n +  ) , but in this case the pnt doesn't help: the number of primes up to x  is asymptotic to x /ln(x ) while the number of primes up to (x +  )  is asymptotic to (x +  ) /ln((x +  ) ), which is asymptotic to the estimate on primes up to x . so unlike the previous case of x and  x we don't get a proof of legendre's conjecture even for all large n. error estimates on the pnt are not (indeed, cannot be) sufficient to prove the existence of even one prime in this interval. in     , ramanujan (    –    ) used properties of the gamma function to give a simpler proof. the short paper included a generalization of the postulate, from which would later arise the concept of ramanujan primes. further generalizations of ramanujan primes have also been discovered; for instance, there is a proof that with pk the kth prime and rn the nth ramanujan prime. other generalizations of bertrand's postulate have been obtained using elementary methods. (in the following, n runs through the set of positive integers.) in     , m. el bachraoui proved that there exists a prime between  n and  n. in     , denis hanson proved that there exists a prime between  n and  n. furthermore, in     , andy loo proved that as n tends to infinity, the number of primes between  n and  n also goes to infinity, thereby generalizing erdős' and ramanujan's results (see the section on erdős' theorems below). the first result is obtained with elementary methods. the second one is based on analytic bounds for the factorial function. "
21,21,Beta Function,3,https://en.wikipedia.org/wiki/Beta_function,"in mathematics, the beta function, also called the euler integral of the first kind, is a special function that is closely related to the gamma function and to binomial coefficients. it is defined by the integral for complex number inputs x, y such that re x >  , re y >  . the beta function was studied by euler and legendre and was given its name by jacques binet; its symbol β is a greek capital beta. the beta function is symmetric, meaning that for all inputs x and y. a key property of the beta function is its close relationship to the gamma function: one has that (a proof is given below in § relationship to the gamma function.) the beta function is also closely related to binomial coefficients. when x (or y, by symmetry) is a positive integer, it follows from the definition of the gamma function γ that a simple derivation of the relation b ( x , y ) = γ ( x ) γ ( y ) γ ( x + y ) {\displaystyle \mathrm {b} (x,y)={\frac {\gamma (x)\,\gamma (y)}{\gamma (x+y)}}} can be found in emil artin's book the gamma function, page   –  . to derive this relation, write the product of two factorials as changing variables by u = zt and v = z(  − t) produces "
22,22,Bijective function,2,https://en.wikipedia.org/wiki/Bijection,"in mathematics, a bijection, also known as a bijective function, one-to-one correspondence, or invertible function, is a function between the elements of two sets, where each element of one set is paired with exactly one element of the other set, and each element of the other set is paired with exactly one element of the first set. there are no unpaired elements. in mathematical terms, a bijective function f: x → y is a one-to-one (injective) and onto (surjective) mapping of a set x to a set y. the term one-to-one correspondence must not be confused with one-to-one function (an injective function; see figures). an injective non-surjective function (injection, not a bijection) an injective surjective function (bijection) a non-injective surjective function (surjection, not a bijection) a non-injective non-surjective function (also not a bijection) a bijection from the set x to the set y has an inverse function from y to x. if x and y are finite sets, then the existence of a bijection means they have the same number of elements. for infinite sets, the picture is more complicated, leading to the concept of cardinal number—a way to distinguish the various sizes of infinite sets. a bijective function from a set to itself is also called a permutation, and the set of all permutations of a set forms the symmetric group. bijective functions are essential to many areas of mathematics including the definitions of isomorphism, homeomorphism, diffeomorphism, permutation group, and projective map. for a pairing between x and y (where y need not be different from x) to be a bijection, four properties must hold: satisfying properties ( ) and ( ) means that a pairing is a function with domain x. it is more common to see properties ( ) and ( ) written as a single statement: every element of x is paired with exactly one element of y. functions which satisfy property ( ) are said to be ""onto y "" and are called surjections (or surjective functions). functions which satisfy property ( ) are said to be ""one-to-one functions"" and are called injections (or injective functions). with this terminology, a bijection is a function which is both a surjection and an injection, or using other words, a bijection is a function which is both ""one-to-one"" and ""onto"". "
23,23,"Bijection, injection and surjection",1,"https://en.wikipedia.org/wiki/Bijection,_injection_and_surjection","bijective injective-only injective surjective-only general in mathematics, injections, surjections, and bijections are classes of functions distinguished by the manner in which arguments (input expressions from the domain) and images (output expressions from the codomain) are related or mapped to each other. a function maps elements from its domain to elements in its codomain. given a function f : x → y {\displaystyle f\colon x\to y} : an injective function need not be surjective (not all elements of the codomain may be associated with arguments), and a surjective function need not be injective (some images may be associated with more than one argument). the four possible combinations of injective and surjective features are illustrated in the adjacent diagrams. a function is injective (one-to-one) if each possible element of the codomain is mapped to by at most one argument. equivalently, a function is injective if it maps distinct arguments to distinct images. an injective function is an injection. the formal definition is the following. the following are some facts related to injections: "
24,24,Bijective numeration,0,https://en.wikipedia.org/wiki/Bijective_numeration,"bijective numeration is any numeral system in which every non-negative integer can be represented in exactly one way using a finite string of digits. the name refers to the bijection (i.e. one-to-one correspondence) that exists in this case between the set of non-negative integers and the set of finite strings using a finite set of symbols (the ""digits""). most ordinary numeral systems, such as the common decimal system, are not bijective because more than one string of digits can represent the same positive integer. in particular, adding leading zeroes does not change the value represented, so "" "", ""  "" and ""   "" all represent the number one. even though only the first is usual, the fact that the others are possible means that the decimal system is not bijective. however, unary numeral system, with only one digit, is bijective. a bijective base-k numeration is a bijective positional notation. it uses a string of digits from the set { ,  , ..., k} (where k ≥  ) to encode each positive integer; a digit's position in the string defines its value as a multiple of a power of k. smullyan (    ) calls this notation k-adic, but it should not be confused with the p-adic numbers: bijective numerals are a system for representing ordinary integers by finite strings of nonzero digits, whereas the p-adic numbers are a system of mathematical values that contain the integers as a subset and may need infinite sequences of digits in any numerical representation. the base-k bijective numeration system uses the digit-set { ,  , ..., k} (k ≥  ) to uniquely represent every non-negative integer, as follows: in contrast, standard positional notation can be defined with a similar recursive algorithm where for base k >   {\displaystyle k> } , the bijective base- k {\displaystyle k} numeration system could be extended to negative integers in the same way as the standard base- b {\displaystyle b} numeral system by use of an infinite number of the digit d k −   {\displaystyle d_{k- }} , where f ( d k −   ) = k −   {\displaystyle f(d_{k- })=k- } , represented as a left-infinite sequence of digits … d k −   d k −   d k −   = d k −   ¯ {\displaystyle \ldots d_{k- }d_{k- }d_{k- }={\overline {d_{k- }}}} . this is because the euler summation meaning that and for every positive number n {\displaystyle n} with bijective numeration digit representation d {\displaystyle d} is represented by d k −   ¯ d k d {\displaystyle {\overline {d_{k- }}}d_{k}d} . for base k >   {\displaystyle k> } , negative numbers n < −   {\displaystyle n<- } are represented by d k −   ¯ d i d {\displaystyle {\overline {d_{k- }}}d_{i}d} with i < k −   {\displaystyle i<k- } , while for base k =   {\displaystyle k= } , negative numbers n < −   {\displaystyle n<- } are represented by d k ¯ d {\displaystyle {\overline {d_{k}}}d} . this is similar to how in signed-digit representations, all integers n {\displaystyle n} with digit representations d {\displaystyle d} are represented as d   ¯ d {\displaystyle {\overline {d_{ }}}d} where f ( d   ) =   {\displaystyle f(d_{ })= } . this representation is no longer bijective, as the entire set of left-infinite sequences of digits is used to represent the k {\displaystyle k} -adic integers, of which the integers are only a subset. for a given base k ≥   {\displaystyle k\geq  } , for a given base k ≥   {\displaystyle k\geq  } , "
25,25,Binary operation,1,https://en.wikipedia.org/wiki/Binary_operation,"in mathematics, a binary operation or dyadic operation is a rule for combining two elements (called operands) to produce another element. more formally, a binary operation is an operation of arity two. more specifically, a binary operation on a set is an operation whose two domains and the codomain are the same set. examples include the familiar arithmetic operations of addition, subtraction, and multiplication. other examples are readily found in different areas of mathematics, such as vector addition, matrix multiplication, and conjugation in groups. an operation of arity two that involves several sets is sometimes also called a binary operation. for example, scalar multiplication of vector spaces takes a scalar and a vector to produce a vector, and scalar product takes two vectors to produce a scalar. such binary operations may be called simply binary functions. binary operations are the keystone of most algebraic structures that are studied in algebra, in particular in semigroups, monoids, groups, rings, fields, and vector spaces. more precisely, a binary operation on a set s is a mapping of the elements of the cartesian product s × s to s: because the result of performing the operation on a pair of elements of s is again an element of s, the operation is called a closed (or internal) binary operation on s (or sometimes expressed as having the property of closure). if f is not a function, but a partial function, then f is called a partial binary operation. for instance, division of real numbers is a partial binary operation, because one can't divide by zero: a/  is undefined for every real number a. in both universal algebra and model theory, binary operations are required to be defined on all elements of s × s. sometimes, especially in computer science, the term binary operation is used for any binary function. typical examples of binary operations are the addition (+) and multiplication (×) of numbers and matrices as well as composition of functions on a single set. for instance, many binary operations of interest in both algebra and formal logic are commutative, satisfying f(a, b) = f(b, a) for all elements a and b in s, or associative, satisfying f(f(a, b), c) = f(a, f(b, c)) for all a, b, and c in s. many also have identity elements and inverse elements. "
26,26,Binary Relation,2,https://en.wikipedia.org/wiki/Binary_relation,"in mathematics, a binary relation associates elements of one set, called the domain, with elements of another set, called the codomain. a binary relation over sets x and y is a new set of ordered pairs (x, y) consisting of elements x in x and y in y. it is a generalization of the more widely understood idea of a mathematical function, but with fewer restrictions. it encodes the common concept of relation: an element x is related to an element y, if and only if the pair (x, y) belongs to the set of ordered pairs that defines the binary relation. a binary relation is the most studied special case n =   of an n-ary relation over sets x , ..., xn, which is a subset of the cartesian product x   × ⋯ × x n . {\displaystyle x_{ }\times \cdots \times x_{n}.} an example of a binary relation is the ""divides"" relation over the set of prime numbers p {\displaystyle \mathbb {p} } and the set of integers z {\displaystyle \mathbb {z} } , in which each prime p is related to each integer z that is a multiple of p, but not to an integer that is not a multiple of p. in this relation, for instance, the prime number   is related to numbers such as − ,  ,  ,   , but not to   or  , just as the prime number   is related to  ,  , and  , but not to   or   . binary relations are used in many branches of mathematics to model a wide variety of concepts. these include, among others: a function may be defined as a special kind of binary relation. binary relations are also heavily used in computer science. a binary relation over sets x and y is an element of the power set of x × y . {\displaystyle x\times y.} since the latter set is ordered by inclusion (⊆), each relation has a place in the lattice of subsets of x × y . {\displaystyle x\times y.} a binary relation is either a homogeneous relation or a heterogeneous relation depending on whether x = y or not. since relations are sets, they can be manipulated using set operations, including union, intersection, and complementation, and satisfying the laws of an algebra of sets. beyond that, operations like the converse of a relation and the composition of relations are available, satisfying the laws of a calculus of relations, for which there are textbooks by ernst schröder, clarence lewis, and gunther schmidt. a deeper analysis of relations involves decomposing them into subsets called concepts, and placing them in a complete lattice. in some systems of axiomatic set theory, relations are extended to classes, which are generalizations of sets. this extension is needed for, among other things, modeling the concepts of ""is an element of"" or ""is a subset of"" in set theory, without running into logical inconsistencies such as russell's paradox. the terms correspondence, dyadic relation and two-place relation are synonyms for binary relation, though some authors use the term ""binary relation"" for any subset of a cartesian product x × y {\displaystyle x\times y} without reference to x and y, and reserve the term ""correspondence"" for a binary relation with reference to x and y.[citation needed] given sets x and y, the cartesian product x × y {\displaystyle x\times y} is defined as { ( x , y ) : x ∈ x and y ∈ y } , {\displaystyle \{(x,y):x\in x{\text{ and }}y\in y\},} and its elements are called ordered pairs. a binary relation r over sets x and y is a subset of x × y . {\displaystyle x\times y.} the set x is called the domain or set of departure of r, and the set y the codomain or set of destination of r. in order to specify the choices of the sets x and y, some authors define a binary relation or correspondence as an ordered triple (x, y, g), where g is a subset of x × y {\displaystyle x\times y} called the graph of the binary relation. the statement ( x , y ) ∈ r {\displaystyle (x,y)\in r} reads ""x is r-related to y"" and is denoted by xry. [note  ] the domain of definition or active domain of r is the set of all x such that xry for at least one y. the codomain of definition, active codomain, image or range of r is the set of all y such that xry for at least one x. the field of r is the union of its domain of definition and its codomain of definition. "
27,27,properties of binomial coefficients,1,https://en.wikipedia.org/wiki/Binomial_coefficient,"in mathematics, the binomial coefficients are the positive integers that occur as coefficients in the binomial theorem. commonly, a binomial coefficient is indexed by a pair of integers n ≥ k ≥   and is written ( n k ) . {\displaystyle {\tbinom {n}{k}}.} it is the coefficient of the xk term in the polynomial expansion of the binomial power (  + x)n, and is given by the formula for example, the fourth power of   + x is and the binomial coefficient (     ) =   !   !   ! =   {\displaystyle {\tbinom { }{ }}={\tfrac { !}{ ! !}}= } is the coefficient of the x  term. arranging the numbers ( n   ) , ( n   ) , … , ( n n ) {\displaystyle {\tbinom {n}{ }},{\tbinom {n}{ }},\ldots ,{\tbinom {n}{n}}} in successive rows for n =   ,   ,   , … {\displaystyle n= , , ,\ldots } gives a triangular array called pascal's triangle, satisfying the recurrence relation the binomial coefficients occur in many areas of mathematics, and especially in combinatorics. the symbol ( n k ) {\displaystyle {\tbinom {n}{k}}} is usually read as ""n choose k"" because there are ( n k ) {\displaystyle {\tbinom {n}{k}}} ways to choose an (unordered) subset of k elements from a fixed set of n elements. for example, there are (     ) =   {\displaystyle {\tbinom { }{ }}= } ways to choose   elements from {   ,   ,   ,   } , {\displaystyle \{ , , , \},} namely {   ,   } , {   ,   } , {   ,   } , {   ,   } , {   ,   } , {\displaystyle \{ , \},\,\{ , \},\,\{ , \},\,\{ , \},\,\{ , \},} and {   ,   } . {\displaystyle \{ , \}.} the binomial coefficients can be generalized to ( z k ) {\displaystyle {\tbinom {z}{k}}} for any complex number z and integer k ≥  , and many of their properties continue to hold in this more general form. andreas von ettingshausen introduced the notation ( n k ) {\displaystyle {\tbinom {n}{k}}} in     , although the numbers were known centuries earlier (see pascal's triangle). the earliest known detailed discussion of binomial coefficients is in a tenth-century commentary, by halayudha, on an ancient sanskrit text, pingala's chandaḥśāstra. in about     , the indian mathematician bhaskaracharya gave an exposition of binomial coefficients in his book līlāvatī. alternative notations include c(n, k), nck, nck, ckn, cnk, and cn,k in all of which the c stands for combinations or choices. many calculators use variants of the c notation because they can represent it on a single-line display. in this form the binomial coefficients are easily compared to k-permutations of n, written as p(n, k), etc. for natural numbers (taken to include  ) n and k, the binomial coefficient ( n k ) {\displaystyle {\tbinom {n}{k}}} can be defined as the coefficient of the monomial xk in the expansion of (  + x)n. the same coefficient also occurs (if k ≤ n) in the binomial formula "
28,28,Binomial Distribution,2,https://en.wikipedia.org/wiki/Binomial_distribution,"in probability theory and statistics, the binomial distribution with parameters n and p is the discrete probability distribution of the number of successes in a sequence of n independent experiments, each asking a yes–no question, and each with its own boolean-valued outcome: success (with probability p) or failure (with probability q =   − p). a single success/failure experiment is also called a bernoulli trial or bernoulli experiment, and a sequence of outcomes is called a bernoulli process; for a single trial, i.e., n =  , the binomial distribution is a bernoulli distribution. the binomial distribution is the basis for the popular binomial test of statistical significance. the binomial distribution is frequently used to model the number of successes in a sample of size n drawn with replacement from a population of size n. if the sampling is carried out without replacement, the draws are not independent and so the resulting distribution is a hypergeometric distribution, not a binomial one. however, for n much larger than n, the binomial distribution remains a good approximation, and is widely used. in general, if the random variable x follows the binomial distribution with parameters n ∈ n {\displaystyle \mathbb {n} } and p ∈ [ , ], we write x ~ b(n, p). the probability of getting exactly k successes in n independent bernoulli trials is given by the probability mass function: for k =  ,  ,  , ..., n, where is the binomial coefficient, hence the name of the distribution. the formula can be understood as follows: k successes occur with probability pk and n − k failures occur with probability (  − p)n − k. however, the k successes can occur anywhere among the n trials, and there are ( n k ) {\displaystyle {\tbinom {n}{k}}} different ways of distributing k successes in a sequence of n trials. in creating reference tables for binomial distribution probability, usually the table is filled in up to n/  values. this is because for k > n/ , the probability can be calculated by its complement as looking at the expression f(k, n, p) as a function of k, there is a k value that maximizes it. this k value can be found by calculating and comparing it to  . there is always an integer m that satisfies f(k, n, p) is monotone increasing for k < m and monotone decreasing for k > m, with the exception of the case where (n +  )p is an integer. in this case, there are two values for which f is maximal: (n +  )p and (n +  )p −  . m is the most probable outcome (that is, the most likely, although this can still be unlikely overall) of the bernoulli trials and is called the mode. suppose a biased coin comes up heads with probability  .  when tossed. the probability of seeing exactly   heads in   tosses is "
29,29,Binomial Series,2,https://en.wikipedia.org/wiki/Binomial_series,"in mathematics, the binomial series is the taylor series for the function f {\displaystyle f} given by f ( x ) = (   + x ) α , {\displaystyle f(x)=( +x)^{\alpha },} where α ∈ c {\displaystyle \alpha \in \mathbb {c} } is an arbitrary complex number and |x| <  . explicitly, ( )and the binomial series is the power series on the right-hand side of ( ), expressed in terms of the (generalized) binomial coefficients if α is a nonnegative integer n, then the (n +  )th term and all later terms in the series are  , since each contains a factor (n − n); thus in this case the series is finite and gives the algebraic binomial formula. the following variant holds for arbitrary complex β, but is especially useful for handling negative integer exponents in ( ): in terms of the multiset coefficient or binomial coefficient. "
30,30,Tangent properties of parabola,3,https://en.wikipedia.org/wiki/Binomial_theorem,"in elementary algebra, the binomial theorem (or binomial expansion) describes the algebraic expansion of powers of a binomial. according to the theorem, it is possible to expand the polynomial (x + y)n into a sum involving terms of the form axbyc, where the exponents b and c are nonnegative integers with b + c = n, and the coefficient a of each term is a specific positive integer depending on n and b. for example, for n =  , the coefficient a in the term of axbyc is known as the binomial coefficient ( n b ) {\displaystyle {\tbinom {n}{b}}} or ( n c ) {\displaystyle {\tbinom {n}{c}}} (the two have the same value). these coefficients for varying n and b can be arranged to form pascal's triangle. these numbers also occur in combinatorics, where ( n b ) {\displaystyle {\tbinom {n}{b}}} gives the number of different combinations of b elements that can be chosen from an n-element set. therefore ( n b ) {\displaystyle {\tbinom {n}{b}}} is often pronounced as ""n choose b"". special cases of the binomial theorem were known since at least the  th century bc when greek mathematician euclid mentioned the special case of the binomial theorem for exponent  . there is evidence that the binomial theorem for cubes was known by the  th century ad in india. binomial coefficients, as combinatorial quantities expressing the number of ways of selecting k objects out of n without replacement, were of interest to ancient indian mathematicians. the earliest known reference to this combinatorial problem is the chandaḥśāstra by the indian lyricist pingala (c.     bc), which contains a method for its solution. :     the commentator halayudha from the   th century ad explains this method using what is now known as pascal's triangle. by the  th century ad, the indian mathematicians probably knew how to express this as a quotient n ! ( n − k ) ! k ! {\textstyle {\frac {n!}{(n-k)!k!}}} , and a clear statement of this rule can be found in the   th century text lilavati by bhaskara. the first formulation of the binomial theorem and the table of binomial coefficients, to our knowledge, can be found in a work by al-karaji, quoted by al-samaw'al in his ""al-bahir"". al-karaji described the triangular pattern of the binomial coefficients and also provided a mathematical proof of both the binomial theorem and pascal's triangle, using an early form of mathematical induction. the persian poet and mathematician omar khayyam was probably familiar with the formula to higher orders, although many of his mathematical works are lost. the binomial expansions of small degrees were known in the   th century mathematical works of yang hui and also chu shih-chieh. yang hui attributes the method to a much earlier   th century text of jia xian, although those writings are now also lost. :     in     , michael stifel introduced the term ""binomial coefficient"" and showed how to use them to express (   + a ) n {\displaystyle ( +a)^{n}} in terms of (   + a ) n −   {\displaystyle ( +a)^{n- }} , via ""pascal's triangle"". blaise pascal studied the eponymous triangle comprehensively in his traité du triangle arithmétique. however, the pattern of numbers was already known to the european mathematicians of the late renaissance, including stifel, niccolò fontana tartaglia, and simon stevin. isaac newton is generally credited with the generalized binomial theorem, valid for any rational exponent. according to the theorem, it is possible to expand any nonnegative power of x + y into a sum of the form here are the first few cases of the binomial theorem: an example illustrating the last two points: "
31,31,Borel Cantelli Lemma,2,https://en.wikipedia.org/wiki/Borel%E2%80%93Cantelli_lemma,"in probability theory, the borel–cantelli lemma is a theorem about sequences of events. in general, it is a result in measure theory. it is named after émile borel and francesco paolo cantelli, who gave statement to the lemma in the first decades of the   th century. a related result, sometimes called the second borel–cantelli lemma, is a partial converse of the first borel–cantelli lemma. the lemma states that, under certain conditions, an event will have probability of either zero or one. accordingly, it is the best-known of a class of similar theorems, known as zero-one laws. other examples include kolmogorov's zero–one law and the hewitt–savage zero–one law. let e ,e ,... be a sequence of events in some probability space. the borel–cantelli lemma states: borel–cantelli lemma — if the sum of the probabilities of the events {en} is finite here, ""lim sup"" denotes limit supremum of the sequence of events, and each event is a set of outcomes. that is, lim sup en is the set of outcomes that occur infinitely many times within the infinite sequence of events (en). explicitly, the set lim sup en is sometimes denoted {en i.o. }, where ""i.o."" stands for ""infinitely often"". the theorem therefore asserts that if the sum of the probabilities of the events en is finite, then the set of all outcomes that are ""repeated"" infinitely many times must occur with probability zero. note that no assumption of independence is required. suppose (xn) is a sequence of random variables with pr(xn =  ) =  /n  for each n. the probability that xn =   occurs for infinitely many n is equivalent to the probability of the intersection of infinitely many [xn =  ] events. the intersection of infinitely many such events is a set of outcomes common to all of them. however, the sum σpr(xn =  ) converges to π /  ≈  .    < ∞, and so the borel–cantelli lemma states that the set of outcomes that are common to infinitely many such events occurs with probability zero. hence, the probability of xn =   occurring for infinitely many n is  . almost surely (i.e., with probability  ), xn is nonzero for all but finitely many n. let (en) be a sequence of events in some probability space. the sequence of events { ⋃ n = n ∞ e n } n =   ∞ {\textstyle \left\{\bigcup _{n=n}^{\infty }e_{n}\right\}_{n= }^{\infty }} is non-increasing: by continuity from above, by subadditivity, "
32,32,Calculus,1,https://en.wikipedia.org/wiki/Calculus," calculus, originally called infinitesimal calculus or ""the calculus of infinitesimals"", is the mathematical study of continuous change, in the same way that geometry is the study of shape, and algebra is the study of generalizations of arithmetic operations. it has two major branches, differential calculus and integral calculus; differential calculus concerns instantaneous rates of change, and the slopes of curves, while integral calculus concerns accumulation of quantities, and areas under or between curves. these two branches are related to each other by the fundamental theorem of calculus, and they make use of the fundamental notions of convergence of infinite sequences and infinite series to a well-defined limit. infinitesimal calculus was developed independently in the late   th century by isaac newton and gottfried wilhelm leibniz. later work, including codifying the idea of limits, put these developments on a more solid conceptual footing. today, calculus has widespread uses in science, engineering, and economics. in mathematics education, calculus denotes courses of elementary mathematical analysis, which are mainly devoted to the study of functions and limits. the word calculus is latin for ""small pebble"" (the diminutive of calx, meaning ""stone""). because such pebbles were used for counting out distances, tallying votes, and doing abacus arithmetic, the word came to mean a method of computation. in this sense, it was used in english at least as early as     , several years prior to the publications of leibniz and newton. (the older meaning still persists in medicine.) in addition to the differential calculus and integral calculus, the term is also used for naming specific methods of calculation and related theories, such as propositional calculus, ricci calculus, calculus of variations, lambda calculus, and process calculus. modern calculus was developed in   th-century europe by isaac newton and gottfried wilhelm leibniz (independently of each other, first publishing around the same time) but elements of it appeared in ancient greece, then in china and the middle east, and still later again in medieval europe and in india. calculations of volume and area, one goal of integral calculus, can be found in the egyptian moscow papyrus (c.      bc), but the formulae are simple instructions, with no indication as to how they were obtained. laying the foundations for integral calculus and foreshadowing the concept of the limit, ancient greek mathematician eudoxus of cnidus (c.     –     bce) developed the method of exhaustion to prove the formulas for cone and pyramid volumes. during the hellenistic period, this method was further developed by archimedes, who combined it with a concept of the indivisibles—a precursor to infinitesimals—allowing him to solve several problems now treated by integral calculus. these problems include, for example, calculating the center of gravity of a solid hemisphere, the center of gravity of a frustum of a circular paraboloid, and the area of a region bounded by a parabola and one of its secant lines. the method of exhaustion was later discovered independently in china by liu hui in the  rd century ad in order to find the area of a circle. in the  th century ad, zu gengzhi, son of zu chongzhi, established a method that would later be called cavalieri's principle to find the volume of a sphere. "
33,33,Cantor's theorem,3,https://en.wikipedia.org/wiki/Cantor%27s_theorem,"in mathematical set theory, cantor's theorem is a fundamental result which states that, for any set a {\displaystyle a} , the set of all subsets of a , {\displaystyle a,} the power set of a , {\displaystyle a,} has a strictly greater cardinality than a {\displaystyle a} itself. for finite sets, cantor's theorem can be seen to be true by simple enumeration of the number of subsets. counting the empty set as a subset, a set with n {\displaystyle n} elements has a total of   n {\displaystyle  ^{n}} subsets, and the theorem holds because   n > n {\displaystyle  ^{n}>n} for all non-negative integers. much more significant is cantor's discovery of an argument that is applicable to any set, and shows that the theorem holds for infinite sets also. as a consequence, the cardinality of the real numbers, which is the same as that of the power set of the integers, is strictly larger than the cardinality of the integers; see cardinality of the continuum for details. the theorem is named for german mathematician georg cantor, who first stated and proved it at the end of the   th century. cantor's theorem had immediate and important consequences for the philosophy of mathematics. for instance, by iteratively taking the power set of an infinite set and applying cantor's theorem, we obtain an endless hierarchy of infinite cardinals, each strictly larger than the one before it. consequently, the theorem implies that there is no largest cardinal number (colloquially, ""there's no largest infinity""). cantor's argument is elegant and remarkably simple. the complete proof is presented below, with detailed explanations to follow. theorem (cantor) — let f {\displaystyle f} be a map from set a {\displaystyle a} to its power set p ( a ) {\displaystyle {\mathcal {p}}(a)} . then f : a → p ( a ) {\displaystyle f:a\to {\mathcal {p}}(a)} is not surjective. as a consequence, card ⁡ ( a ) < card ⁡ ( p ( a ) ) {\displaystyle \operatorname {card} (a)<\operatorname {card} ({\mathcal {p}}(a))} holds for any set a {\displaystyle a} . consider the set b = { x ∈ a ∣ x ∉ f ( x ) } {\displaystyle b=\{x\in a\mid x\notin f(x)\}} . suppose to the contrary that f {\displaystyle f} is surjective. then there exists ξ ∈ a {\displaystyle \xi \in a} such that f ( ξ ) = b {\displaystyle f(\xi )=b} . but by construction, ξ ∈ b ⟺ ξ ∉ f ( ξ ) = b {\displaystyle \xi \in b\iff \xi \notin f(\xi )=b} . this is a contradiction. thus, f {\displaystyle f} cannot be surjective. on the other hand, g : a → p ( a ) {\displaystyle g:a\to {\mathcal {p}}(a)} defined by x ↦ { x } {\displaystyle x\mapsto \{x\}} is an injective map. consequently, we must have card ⁡ ( a ) < card ⁡ ( p ( a ) ) {\displaystyle \operatorname {card} (a)<\operatorname {card} ({\mathcal {p}}(a))} . q.e.d. by definition of cardinality, we have card ⁡ ( x ) < card ⁡ ( y ) {\displaystyle \operatorname {card} (x)<\operatorname {card} (y)} for any two sets x {\displaystyle x} and y {\displaystyle y} if and only if there is an injective function but no bijective function from x {\displaystyle x} to y {\displaystyle y} . it suffices to show that there is no surjection from x {\displaystyle x} to y {\displaystyle y} . this is the heart of cantor's theorem: there is no surjective function from any set a {\displaystyle a} to its power set. to establish this, it is enough to show that no function f that maps elements in a {\displaystyle a} to subsets of a {\displaystyle a} can reach every possible subset, i.e., we just need to demonstrate the existence of a subset of a {\displaystyle a} that is not equal to f ( x ) {\displaystyle f(x)} for any x {\displaystyle x} ∈ a {\displaystyle a} . (recall that each f ( x ) {\displaystyle f(x)} is a subset of a {\displaystyle a} .) such a subset is given by the following construction, sometimes called the cantor diagonal set of f {\displaystyle f} : this means, by definition, that for all x ∈ a, x ∈ b if and only if x ∉ f(x). for all x the sets b and f(x) cannot be the same because b was constructed from elements of a whose images (under f) did not include themselves. more specifically, consider any x ∈ a, then either x ∈ f(x) or x ∉ f(x). in the former case, f(x) cannot equal b because x ∈ f(x) by assumption and x ∉ b by the construction of b. in the latter case, f(x) cannot equal b because x ∉ f(x) by assumption and x ∈ b by the construction of b. equivalently, and slightly more formally, we just proved that the existence of ξ ∈ a such that f(ξ) = b implies the following contradiction: "
34,34,Cartesian Coordinates,1,https://en.wikipedia.org/wiki/Cartesian_coordinate_system," a cartesian coordinate system (uk: /kɑːˈtiːzjən/, us: /kɑːrˈtiʒən/) in a plane is a coordinate system that specifies each point uniquely by a pair of numerical coordinates, which are the signed distances to the point from two fixed perpendicular oriented lines, measured in the same unit of length. each reference coordinate line is called a coordinate axis or just axis (plural axes) of the system, and the point where they meet is its origin, at ordered pair ( ,  ). the coordinates can also be defined as the positions of the perpendicular projections of the point onto the two axes, expressed as signed distances from the origin. one can use the same principle to specify the position of any point in three-dimensional space by three cartesian coordinates, its signed distances to three mutually perpendicular planes (or, equivalently, by its perpendicular projection onto three mutually perpendicular lines). in general, n cartesian coordinates (an element of real n-space) specify the point in an n-dimensional euclidean space for any dimension n. these coordinates are equal, up to sign, to distances from the point to n mutually perpendicular hyperplanes. the invention of cartesian coordinates in the   th century by rené descartes (latinized name: cartesius) revolutionized mathematics by providing the first systematic link between euclidean geometry and algebra. using the cartesian coordinate system, geometric shapes (such as curves) can be described by cartesian equations: algebraic equations involving the coordinates of the points lying on the shape. for example, a circle of radius  , centered at the origin of the plane, may be described as the set of all points whose coordinates x and y satisfy the equation x  + y  =  . cartesian coordinates are the foundation of analytic geometry, and provide enlightening geometric interpretations for many other branches of mathematics, such as linear algebra, complex analysis, differential geometry, multivariate calculus, group theory and more. a familiar example is the concept of the graph of a function. cartesian coordinates are also essential tools for most applied disciplines that deal with geometry, including astronomy, physics, engineering and many more. they are the most common coordinate system used in computer graphics, computer-aided geometric design and other geometry-related data processing. the adjective cartesian refers to the french mathematician and philosopher rené descartes, who published this idea in     . it was independently discovered by pierre de fermat, who also worked in three dimensions, although fermat did not publish the discovery. the french cleric nicole oresme used constructions similar to cartesian coordinates well before the time of descartes and fermat. both descartes and fermat used a single axis in their treatments and have a variable length measured in reference to this axis. the concept of using a pair of axes was introduced later, after descartes' la géométrie was translated into latin in      by frans van schooten and his students. these commentators introduced several concepts while trying to clarify the ideas contained in descartes' work. the development of the cartesian coordinate system would play a fundamental role in the development of the calculus by isaac newton and gottfried wilhelm leibniz. the two-coordinate description of the plane was later generalized into the concept of vector spaces. many other coordinate systems have been developed since descartes, such as the polar coordinates for the plane, and the spherical and cylindrical coordinates for three-dimensional space. choosing a cartesian coordinate system for a one-dimensional space—that is, for a straight line—involves choosing a point o of the line (the origin), a unit of length, and an orientation for the line. an orientation chooses which of the two half-lines determined by o is the positive and which is negative; we then say that the line ""is oriented"" (or ""points"") from the negative half towards the positive half. then each point p of the line can be specified by its distance from o, taken with a + or − sign depending on which half-line contains p. "
35,35,Cartesian Product,1,https://en.wikipedia.org/wiki/Cartesian_product," in mathematics, specifically set theory, the cartesian product of two sets a and b, denoted a × b, is the set of all ordered pairs (a, b) where a is in a and b is in b. in terms of set-builder notation, that is a table can be created by taking the cartesian product of a set of rows and a set of columns. if the cartesian product rows × columns is taken, the cells of the table contain ordered pairs of the form (row value, column value). one can similarly define the cartesian product of n sets, also known as an n-fold cartesian product, which can be represented by an n-dimensional array, where each element is an n-tuple. an ordered pair is a  -tuple or couple. more generally still, one can define the cartesian product of an indexed family of sets. the cartesian product is named after rené descartes, whose formulation of analytic geometry gave rise to the concept, which is further generalized in terms of direct product. an illustrative example is the standard   -card deck. the standard playing card ranks {a, k, q, j,   ,  ,  ,  ,  ,  ,  ,  ,  } form a   -element set. the card suits {♠, ♥, ♦, ♣} form a four-element set. the cartesian product of these sets returns a   -element set consisting of    ordered pairs, which correspond to all    possible playing cards. ranks × suits returns a set of the form {(a, ♠), (a, ♥), (a, ♦), (a, ♣), (k, ♠), …, ( , ♣), ( , ♠), ( , ♥), ( , ♦), ( , ♣)}. suits × ranks returns a set of the form {(♠, a), (♠, k), (♠, q), (♠, j), (♠,   ), …, (♣,  ), (♣,  ), (♣,  ), (♣,  ), (♣,  )}. these two sets are distinct, even disjoint. the main historical example is the cartesian plane in analytic geometry. in order to represent geometrical shapes in a numerical way, and extract numerical information from shapes' numerical representations, rené descartes assigned to each point in the plane a pair of real numbers, called its coordinates. usually, such a pair's first and second components are called its x and y coordinates, respectively (see picture). the set of all such pairs (i.e., the cartesian product ℝ×ℝ, with ℝ denoting the real numbers) is thus assigned to the set of all points in the plane.[citation needed] "
36,36,Categorical distribution,0,https://en.wikipedia.org/wiki/Categorical_distribution,"( ) p ( x = i ) = p i {\displaystyle p(x=i)=p_{i}} ( ) p ( x ) = p   [ x =   ] ⋯ p k [ x = k ] {\displaystyle p(x)=p_{ }^{[x= ]}\cdots p_{k}^{[x=k]}} ( ) p ( x ) = [ x =   ] ⋅ p   + ⋯ + [ x = k ] ⋅ p k {\displaystyle p(x)=[x= ]\cdot p_{ }\,+\cdots +\,[x=k]\cdot p_{k}} in probability theory and statistics, a categorical distribution (also called a generalized bernoulli distribution, multinoulli distribution ) is a discrete probability distribution that describes the possible results of a random variable that can take on one of k possible categories, with the probability of each category separately specified. there is no innate underlying ordering of these outcomes, but numerical labels are often attached for convenience in describing the distribution, (e.g.   to k). the k-dimensional categorical distribution is the most general distribution over a k-way event; any other discrete distribution over a size-k sample space is a special case. the parameters specifying the probabilities of each possible outcome are constrained only by the fact that each must be in the range   to  , and all must sum to  . the categorical distribution is the generalization of the bernoulli distribution for a categorical random variable, i.e. for a discrete variable with more than two possible outcomes, such as the roll of a dice. on the other hand, the categorical distribution is a special case of the multinomial distribution, in that it gives the probabilities of potential outcomes of a single drawing rather than multiple drawings. occasionally, the categorical distribution is termed the ""discrete distribution"". however, this properly refers not to one particular family of distributions but to a general class of distributions. in some fields, such as machine learning and natural language processing, the categorical and multinomial distributions are conflated, and it is common to speak of a ""multinomial distribution"" when a ""categorical distribution"" would be more precise. this imprecise usage stems from the fact that it is sometimes convenient to express the outcome of a categorical distribution as a "" -of-k"" vector (a vector with one element containing a   and all other elements containing a  ) rather than as an integer in the range   to k; in this form, a categorical distribution is equivalent to a multinomial distribution for a single observation (see below). however, conflating the categorical and multinomial distributions can lead to problems. for example, in a dirichlet-multinomial distribution, which arises commonly in natural language processing models (although not usually with this name) as a result of collapsed gibbs sampling where dirichlet distributions are collapsed out of a hierarchical bayesian model, it is very important to distinguish categorical from multinomial. the joint distribution of the same variables with the same dirichlet-multinomial distribution has two different forms depending on whether it is characterized as a distribution whose domain is over individual categorical nodes or over multinomial-style counts of nodes in each particular category (similar to the distinction between a set of bernoulli-distributed nodes and a single binomial-distributed node). both forms have very similar-looking probability mass functions (pmfs), which both make reference to multinomial-style counts of nodes in a category. however, the multinomial-style pmf has an extra factor, a multinomial coefficient, that is a constant equal to   in the categorical-style pmf. confusing the two can easily lead to incorrect results in settings where this extra factor is not constant with respect to the distributions of interest. the factor is frequently constant in the complete conditionals used in gibbs sampling and the optimal distributions in variational methods. a categorical distribution is a discrete probability distribution whose sample space is the set of k individually identified items. it is the generalization of the bernoulli distribution for a categorical random variable. in one formulation of the distribution, the sample space is taken to be a finite sequence of integers. the exact integers used as labels are unimportant; they might be { ,  , ..., k −  } or { ,  , ..., k} or any other arbitrary set of values. in the following descriptions, we use { ,  , ..., k} for convenience, although this disagrees with the convention for the bernoulli distribution, which uses { ,  }. in this case, the probability mass function f is: where p = ( p   , … , p k ) {\displaystyle {\boldsymbol {p}}=(p_{ },\ldots ,p_{k})} , p i {\displaystyle p_{i}} represents the probability of seeing element i and ∑ i =   k p i =   {\displaystyle \textstyle {\sum _{i= }^{k}p_{i}= }} . another formulation that appears more complex but facilitates mathematical manipulations is as follows, using the iverson bracket: "
37,37,Discrete Mathematics,1,https://en.wikipedia.org/wiki/Category:Discrete_mathematics,"discrete mathematics, also called finite mathematics, is the study of mathematical structures that are fundamentally discrete, in the sense of not supporting or requiring the notion of continuity. most, if not all, of the objects studied in finite mathematics are countable sets, such as integers, finite graphs, and formal languages. this category has the following   subcategories, out of   total. the following    pages are in this category, out of    total. this list may not reflect recent changes (learn more). "
38,38,cauchy schwarz inequality,2,https://en.wikipedia.org/wiki/Cauchy%E2%80%93Schwarz_inequality,"the cauchy–schwarz inequality (also called cauchy–bunyakovsky–schwarz inequality) is considered one of the most important and widely used inequalities in mathematics. the inequality for sums was published by augustin-louis cauchy (    ). the corresponding inequality for integrals was published by viktor bunyakovsky (    ) and hermann schwarz (    ). schwarz gave the modern proof of the integral version. the cauchy–schwarz inequality states that for all vectors u {\displaystyle \mathbf {u} } and v {\displaystyle \mathbf {v} } of an inner product space it is true that (cauchy-schwarz inequality [written using only the inner product])where ⟨ ⋅ , ⋅ ⟩ {\displaystyle \langle \cdot ,\cdot \rangle } is the inner product. examples of inner products include the real and complex dot product; see the examples in inner product. every inner product gives rise to a norm, called the canonical or induced norm, where the norm of a vector u {\displaystyle \mathbf {u} } is denoted and defined by: "
39,39,Cauchy Distribution,2,https://en.wikipedia.org/wiki/Cauchy_distribution,"the cauchy distribution, named after augustin cauchy, is a continuous probability distribution. it is also known, especially among physicists, as the lorentz distribution (after hendrik lorentz), cauchy–lorentz distribution, lorentz(ian) function, or breit–wigner distribution. the cauchy distribution f ( x ; x   , γ ) {\displaystyle f(x;x_{ },\gamma )} is the distribution of the x-intercept of a ray issuing from ( x   , γ ) {\displaystyle (x_{ },\gamma )} with a uniformly distributed angle. it is also the distribution of the ratio of two independent normally distributed random variables with mean zero. the cauchy distribution is often used in statistics as the canonical example of a ""pathological"" distribution since both its expected value and its variance are undefined (but see § explanation of undefined moments below). the cauchy distribution does not have finite moments of order greater than or equal to one; only fractional absolute moments exist. the cauchy distribution has no moment generating function. in mathematics, it is closely related to the poisson kernel, which is the fundamental solution for the laplace equation in the upper half-plane. it is one of the few distributions that is stable and has a probability density function that can be expressed analytically, the others being the normal distribution and the lévy distribution. functions with the form of the density function of the cauchy distribution were studied by mathematicians in the   th century, but in a different context and under the title of the witch of agnesi. despite its name, the first explicit analysis of the properties of the cauchy distribution was published by the french mathematician poisson in     , with cauchy only becoming associated with it during an academic controversy in     . as such, the name of the distribution is a case of stigler's law of eponymy. poisson noted that if the mean of observations following such a distribution were taken, the mean error did not converge to any finite number. as such, laplace's use of the central limit theorem with such distribution was inappropriate, as it assumed a finite mean and variance. despite this, poisson did not regard the issue as important, in contrast to bienaymé, who was to engage cauchy in a long dispute over the matter. the cauchy distribution has the probability density function (pdf) where x   {\displaystyle x_{ }} is the location parameter, specifying the location of the peak of the distribution, and γ {\displaystyle \gamma } is the scale parameter which specifies the half-width at half-maximum (hwhm), alternatively   γ {\displaystyle  \gamma } is full width at half maximum (fwhm). γ {\displaystyle \gamma } is also equal to half the interquartile range and is sometimes called the probable error. augustin-louis cauchy exploited such a density function in      with an infinitesimal scale parameter, defining what would now be called a dirac delta function. the maximum value or amplitude of the cauchy pdf is   π γ {\displaystyle {\frac { }{\pi \gamma }}} , located at x = x   {\displaystyle x=x_{ }} . it is sometimes convenient to express the pdf in terms of the complex parameter ψ = x   + i γ {\displaystyle \psi =x_{ }+i\gamma } the special case when x   =   {\displaystyle x_{ }= } and γ =   {\displaystyle \gamma = } is called the standard cauchy distribution with the probability density function "
40,40,Cauchy Sequence,0,https://en.wikipedia.org/wiki/Cauchy_sequence,"in mathematics, a cauchy sequence (french pronunciation: ​[koʃi]; english: /ˈkoʊʃiː/ koh-shee), named after augustin-louis cauchy, is a sequence whose elements become arbitrarily close to each other as the sequence progresses. more precisely, given any small positive distance, all but a finite number of elements of the sequence are less than that given distance from each other. it is not sufficient for each term to become arbitrarily close to the preceding term. for instance, in the sequence of square roots of natural numbers: the utility of cauchy sequences lies in the fact that in a complete metric space (one where all such sequences are known to converge to a limit), the criterion for convergence depends only on the terms of the sequence itself, as opposed to the definition of convergence, which uses the limit value as well as the terms. this is often exploited in algorithms, both theoretical and applied, where an iterative process can be shown relatively easily to produce a cauchy sequence, consisting of the iterates, thus fulfilling a logical condition, such as termination. generalizations of cauchy sequences in more abstract uniform spaces exist in the form of cauchy filters and cauchy nets. a sequence for any real number r, the sequence of truncated decimal expansions of r forms a cauchy sequence. for example, when r = π , {\displaystyle r=\pi ,} this sequence is ( ,  . ,  .  ,  .   , ...). the mth and nth terms differ by at most      − m {\displaystyle   ^{ -m}} when m < n, and as m grows this becomes smaller than any fixed positive number ε . {\displaystyle \varepsilon .} if ( x   , x   , x   , . . . ) {\displaystyle (x_{ },x_{ },x_{ },...)} is a sequence in the set x , {\displaystyle x,} then a modulus of cauchy convergence for the sequence is a function α {\displaystyle \alpha } from the set of natural numbers to itself, such that for all natural numbers k {\displaystyle k} and natural numbers m , n > α ( k ) , {\displaystyle m,n>\alpha (k),} | x m − x n | <   / k . {\displaystyle |x_{m}-x_{n}|< /k.} any sequence with a modulus of cauchy convergence is a cauchy sequence. the existence of a modulus for a cauchy sequence follows from the well-ordering property of the natural numbers (let α ( k ) {\displaystyle \alpha (k)} be the smallest possible n {\displaystyle n} in the definition of cauchy sequence, taking r {\displaystyle r} to be   / k {\displaystyle  /k} ). the existence of a modulus also follows from the principle of dependent choice, which is a weak form of the axiom of choice, and it also follows from an even weaker condition called ac  . regular cauchy sequences are sequences with a given modulus of cauchy convergence (usually α ( k ) = k {\displaystyle \alpha (k)=k} or α ( k ) =   k {\displaystyle \alpha (k)= ^{k}} ). any cauchy sequence with a modulus of cauchy convergence is equivalent to a regular cauchy sequence; this can be proven without using any form of the axiom of choice. moduli of cauchy convergence are used by constructive mathematicians who do not wish to use any form of choice. using a modulus of cauchy convergence can simplify both definitions and theorems in constructive analysis. regular cauchy sequences were used by errett bishop in his foundations of constructive analysis, and by douglas bridges in a non-constructive textbook (isbn    - -   -     - ). since the definition of a cauchy sequence only involves metric concepts, it is straightforward to generalize it to any metric space x. to do so, the absolute value | x m − x n | {\displaystyle \left|x_{m}-x_{n}\right|} is replaced by the distance d ( x m , x n ) {\displaystyle d\left(x_{m},x_{n}\right)} (where d denotes a metric) between x m {\displaystyle x_{m}} and x n . {\displaystyle x_{n}.} "
41,41,Cauchy–Schwarz inequality,2,https://en.wikipedia.org/wiki/Cauchy–Schwarz_inequality,
42,42,Cayley–Hamilton theorem,2,https://en.wikipedia.org/wiki/Cayley%E2%80%93Hamilton_theorem,"in linear algebra, the cayley–hamilton theorem (named after the mathematicians arthur cayley and william rowan hamilton) states that every square matrix over a commutative ring (such as the real or complex numbers or the integers) satisfies its own characteristic equation. if a is a given n × n matrix and in is the n × n identity matrix, then the characteristic polynomial of a is defined as p a ( λ ) = det ( λ i n − a ) {\displaystyle p_{a}(\lambda )=\det(\lambda i_{n}-a)} , where det is the determinant operation and λ is a variable for a scalar element of the base ring. since the entries of the matrix ( λ i n − a ) {\displaystyle (\lambda i_{n}-a)} are (linear or constant) polynomials in λ, the determinant is also a degree-n monic polynomial in λ, p a ( λ ) = λ n + c n −   λ n −   + ⋯ + c   λ + c   . {\displaystyle p_{a}(\lambda )=\lambda ^{n}+c_{n- }\lambda ^{n- }+\cdots +c_{ }\lambda +c_{ }~.} one can create an analogous polynomial p a ( a ) {\displaystyle p_{a}(a)} in the matrix a instead of the scalar variable λ, defined as p a ( a ) = a n + c n −   a n −   + ⋯ + c   a + c   i n . {\displaystyle p_{a}(a)=a^{n}+c_{n- }a^{n- }+\cdots +c_{ }a+c_{ }i_{n}~.} the cayley–hamilton theorem states that this polynomial expression is equal to the zero matrix, which is to say that p a ( a ) =   {\displaystyle p_{a}(a)=\mathbf { } } . the theorem allows an to be expressed as a linear combination of the lower matrix powers of a. when the ring is a field, the cayley–hamilton theorem is equivalent to the statement that the minimal polynomial of a square matrix divides its characteristic polynomial. the theorem was first proved in      in terms of inverses of linear functions of quaternions, a non-commutative ring, by hamilton. this corresponds to the special case of certain   ×   real or   ×   complex matrices. the theorem holds for general quaternionic matrices. [nb  ] cayley in      stated it for   ×   and smaller matrices, but only published a proof for the   ×   case. the general case was first proved by ferdinand frobenius in     . for a   ×   matrix a = (a), the characteristic polynomial is given by p(λ) = λ − a, and so p(a) = (a) − a( ) =   is trivial. as a concrete example, let its characteristic polynomial is given by the cayley–hamilton theorem claims that, if we define then we can verify by computation that indeed, for a generic   ×   matrix, the characteristic polynomial is given by p(λ) = λ  − (a + d)λ + (ad − bc), so the cayley–hamilton theorem states that "
43,43,central limit theorm,0,https://en.wikipedia.org/wiki/Central_limit_theorem,"in probability theory, the central limit theorem (clt) establishes that, in many situations, when independent random variables are summed up, their properly normalized sum tends toward a normal distribution (informally a bell curve) even if the original variables themselves are not normally distributed. the theorem is a key concept in probability theory because it implies that probabilistic and statistical methods that work for normal distributions can be applicable to many problems involving other types of distributions. this theorem has seen many changes during the formal development of probability theory. previous versions of the theorem date back to     , but in its modern general form, this fundamental result in probability theory was precisely stated as late as     , thereby serving as a bridge between classical and modern probability theory. if x   , x   , … , x n , … {\textstyle x_{ },x_{ },\dots ,x_{n},\dots } are random samples drawn from a population with overall mean μ {\textstyle \mu } and finite variance σ   {\textstyle \sigma ^{ }} , and if x ¯ n {\textstyle {\bar {x}}_{n}} is the sample mean of the first n {\textstyle n} samples, then the limiting form of the distribution, z = lim n → ∞ ( x ¯ n − μ σ n ) {\textstyle z=\lim _{n\to \infty }{\left({\frac {{\bar {x}}_{n}-\mu }{\frac {\sigma }{\sqrt {n}}}}\right)}} , is a standard normal distribution. for example, suppose that a sample is obtained containing many observations, each observation being randomly generated in a way that does not depend on the values of the other observations, and that the arithmetic mean of the observed values is computed. if this procedure is performed many times, the central limit theorem says that the probability distribution of the average will closely approximate a normal distribution. a simple example of this is that if one flips a coin many times, the probability of getting a given number of heads will approach a normal distribution, with the mean equal to half the total number of flips. at the limit of an infinite number of flips, it will equal a normal distribution. the central limit theorem has several variants. in its common form, the random variables must be identically distributed. in variants, convergence of the mean to the normal distribution also occurs for non-identical distributions or for non-independent observations, if they comply with certain conditions. the earliest version of this theorem, that the normal distribution may be used as an approximation to the binomial distribution, is the de moivre–laplace theorem. let { x   , … , x n , … } {\textstyle \{x_{ },\ldots ,x_{n},\ldots \}} be a sequence of random samples — that is, a sequence of independent and identically distributed (i.i.d.) random variables drawn from a distribution of expected value given by μ {\textstyle \mu } and finite variance given by σ   {\textstyle \sigma ^{ }} . suppose we are interested in the sample average by the law of large numbers, the sample averages converge almost surely (and therefore also converge in probability) to the expected value μ {\textstyle \mu } as n → ∞ {\textstyle n\to \infty } . the classical central limit theorem describes the size and the distributional form of the stochastic fluctuations around the deterministic number μ {\textstyle \mu } during this convergence. more precisely, it states that as n {\textstyle n} gets larger, the distribution of the difference between the sample average x ¯ n {\textstyle {\bar {x}}_{n}} and its limit μ {\textstyle \mu } , when multiplied by the factor n {\textstyle {\sqrt {n}}} (that is n ( x ¯ n − μ ) {\textstyle {\sqrt {n}}({\bar {x}}_{n}-\mu )} ) approximates the normal distribution with mean   and variance σ   {\textstyle \sigma ^{ }} . for large enough n, the distribution of x ¯ n {\textstyle {\bar {x}}_{n}} is close to the normal distribution with mean μ {\textstyle \mu } and variance σ   / n {\textstyle \sigma ^{ }/n} . "
44,44,Central Tendency,0,https://en.wikipedia.org/wiki/Central_tendency,"in statistics, a central tendency (or measure of central tendency) is a central or typical value for a probability distribution. colloquially, measures of central tendency are often called averages. the term central tendency dates from the late     s. the most common measures of central tendency are the arithmetic mean, the median, and the mode. a middle tendency can be calculated for either a finite set of values or for a theoretical distribution, such as the normal distribution. occasionally authors use central tendency to denote ""the tendency of quantitative data to cluster around some central value."" the central tendency of a distribution is typically contrasted with its dispersion or variability; dispersion and central tendency are the often characterized properties of distributions. analysis may judge whether data has a strong or a weak central tendency based on its dispersion. the following may be applied to one-dimensional data. depending on the circumstances, it may be appropriate to transform the data before calculating a central tendency. examples are squaring the values or taking logarithms. whether a transformation is appropriate and what it should be, depend heavily on the data being analyzed. any of the above may be applied to each dimension of multi-dimensional data, but the results may not be invariant to rotations of the multi-dimensional space. in addition, there are the several measures of central tendency can be characterized as solving a variational problem, in the sense of the calculus of variations, namely minimizing variation from the center. that is, given a measure of statistical dispersion, one asks for a measure of central tendency that minimizes variation: such that variation from the center is minimal among all choices of center. in a quip, ""dispersion precedes location"". these measures are initially defined in one dimension, but can be generalized to multiple dimensions. this center may or may not be unique. in the sense of lp spaces, the correspondence is: the associated functions are called p-norms: respectively  -""norm"",  -norm,  -norm, and ∞-norm. the function corresponding to the l  space is not a norm, and is thus often referred to in quotes:  -""norm"". in equations, for a given (finite) data set x, thought of as a vector x = (x ,…,xn), the dispersion about a point c is the ""distance"" from x to the constant vector c = (c,…,c) in the p-norm (normalized by the number of points n): for p =   and p = ∞ these functions are defined by taking limits, respectively as p →   and p → ∞. for p =   the limiting values are    =   and a  =   or a ≠  , so the difference becomes simply equality, so the  -norm counts the number of unequal points. for p = ∞ the largest number dominates, and thus the ∞-norm is the maximum difference. "
45,45,Centroid,1,https://en.wikipedia.org/wiki/Centroid,"in mathematics and physics, the centroid or geometric center of a plane figure is the arithmetic mean position of all the points in the figure. informally, it is the point at which a cutout of the shape (with uniformly distributed mass) could be perfectly balanced on the tip of a pin. the same definition extends to any object in n-dimensional space. while in geometry, the word barycenter is a synonym for centroid, in astrophysics and astronomy, the barycenter is the center of mass of two or more bodies that orbit one another. in physics, the center of mass is the arithmetic mean of all points weighted by the local density or specific weight. if a physical object has uniform density, then its center of mass is the same as the centroid of its shape. in geography, the centroid of a radial projection of a region of the earth's surface to sea level is the region's geographical center. the term ""centroid"" is of recent coinage (    ).[citation needed] it is used as a substitute for the older terms ""center of gravity,"" and ""center of mass"", when the purely geometrical aspects of that point are to be emphasized. the term is peculiar to the english language. the french use ""centre de gravité"" on most occasions, and others use terms of similar meaning. the center of gravity, as the name indicates, is a notion that arose in mechanics, most likely in connection with building activities. when, where, and by whom it was invented is not known, as it is a concept that likely occurred to many people individually with minor differences. while archimedes does not state that proposition explicitly, he makes indirect references to it, suggesting he was familiar with it. however, jean-étienne montucla (    –    ), the author of the first history of mathematics (    ), declares categorically (vol. i, p.    ) that the center of gravity of solids is a subject archimedes did not touch. in      charles bossut (    –    ) published a two-volume essai sur l'histoire générale des mathématiques. this book was highly esteemed by his contemporaries, judging from the fact that within two years after its publication it was already available in translation in italian (    –  ), english (    ), and german (    ). bossut credits archimedes with having found the centroid of plane figures, but has nothing to say about solids. while it is possible euclid was still active in alexandria during the childhood of archimedes (   –    bce), it is certain that when archimedes visited alexandria, euclid was no longer there. thus archimedes could not have learned the theorem that the medians of a triangle meet in a point—the center of gravity of the triangle—directly from euclid, as this proposition is not in euclid's elements. the first explicit statement of this proposition is due to heron of alexandria (perhaps the first century ce) and occurs in his mechanics. it may be added, in passing, that the proposition did not become common in the textbooks on plane geometry until the nineteenth century. the geometric centroid of a convex object always lies in the object. a non-convex object might have a centroid that is outside the figure itself. the centroid of a ring or a bowl, for example, lies in the object's central void. if the centroid is defined, it is a fixed point of all isometries in its symmetry group. in particular, the geometric centroid of an object lies in the intersection of all its hyperplanes of symmetry. the centroid of many figures (regular polygon, regular polyhedron, cylinder, rectangle, rhombus, circle, sphere, ellipse, ellipsoid, superellipse, superellipsoid, etc.) can be determined by this principle alone. "
46,46,Chain Rule,1,https://en.wikipedia.org/wiki/Chain_rule,"in calculus, the chain rule is a formula that expresses the derivative of the composition of two differentiable functions f and g in terms of the derivatives of f and g. more precisely, if h = f ∘ g {\displaystyle h=f\circ g} is the function such that h ( x ) = f ( g ( x ) ) {\displaystyle h(x)=f(g(x))} for every x, then the chain rule is, in lagrange's notation, or, equivalently, the chain rule may also be expressed in leibniz's notation. if a variable z depends on the variable y, which itself depends on the variable x (that is, y and z are dependent variables), then z depends on x as well, via the intermediate variable y. in this case, the chain rule is expressed as and for indicating at which points the derivatives have to be evaluated. in integration, the counterpart to the chain rule is the substitution rule. intuitively, the chain rule states that knowing the instantaneous rate of change of z relative to y and that of y relative to x allows one to calculate the instantaneous rate of change of z relative to x as the product of the two rates of change. as put by george f. simmons: ""if a car travels twice as fast as a bicycle and the bicycle is four times as fast as a walking man, then the car travels   ×   =   times as fast as the man."" the relationship between this example and the chain rule is as follows. let z, y and x be the (variable) positions of the car, the bicycle, and the walking man, respectively. the rate of change of relative positions of the car and the bicycle is d z d y =  . {\textstyle {\frac {dz}{dy}}= .} similarly, d y d x =  . {\textstyle {\frac {dy}{dx}}= .} so, the rate of change of the relative positions of the car and the walking man is the rate of change of positions is the ratio of the speeds, and the speed is the derivative of the position with respect to the time; that is, "
47,47,Chebyshev's inequality,2,https://en.wikipedia.org/wiki/Chebyshev%27s_inequality,"in probability theory, chebyshev's inequality (also called the bienaymé–chebyshev inequality) guarantees that, for a wide class of probability distributions, no more than a certain fraction of values can be more than a certain distance from the mean. specifically, no more than  /k  of the distribution's values can be k or more standard deviations away from the mean (or equivalently, over   −  /k  of the distribution's values are less than k standard deviations away from the mean). the rule is often called chebyshev's theorem, about the range of standard deviations around the mean, in statistics. the inequality has great utility because it can be applied to any probability distribution in which the mean and variance are defined. for example, it can be used to prove the weak law of large numbers. its practical usage is similar to the   –  –  .  rule, which applies only to normal distributions. chebyshev's inequality is more general, stating that a minimum of just   % of values must lie within two standard deviations of the mean and   .  % within three standard deviations for a broad range of different probability distributions. the term chebyshev's inequality may also refer to markov's inequality, especially in the context of analysis. they are closely related, and some authors refer to markov's inequality as ""chebyshev's first inequality,"" and the similar one referred to on this page as ""chebyshev's second inequality."" the theorem is named after russian mathematician pafnuty chebyshev, although it was first formulated by his friend and colleague irénée-jules bienaymé. :    the theorem was first stated without proof by bienaymé in      and later proved by chebyshev in     . his student andrey markov provided another proof in his      ph.d. thesis. chebyshev's inequality is usually stated for random variables, but can be generalized to a statement about measure spaces. let x (integrable) be a random variable with finite expected value μ and finite non-zero variance σ . then for any real number k >  , only the case k >   {\displaystyle k> } is useful. when k ≤   {\displaystyle k\leq  } the right-hand side   k   ≥   {\displaystyle {\frac { }{k^{ }}}\geq  } and the inequality is trivial as all probabilities are ≤  . as an example, using k =   {\displaystyle k={\sqrt { }}} shows that the probability that values lie outside the interval ( μ −   σ , μ +   σ ) {\displaystyle (\mu -{\sqrt { }}\sigma ,\mu +{\sqrt { }}\sigma )} does not exceed     {\displaystyle {\frac { }{ }}} . because it can be applied to completely arbitrary distributions provided they have a known finite mean and variance, the inequality generally gives a poor bound compared to what might be deduced if more aspects are known about the distribution involved. let (x, σ, μ) be a measure space, and let f be an extended real-valued measurable function defined on x. then for any real number t >   and   < p < ∞, "
48,48,Chernoff bound,2,https://en.wikipedia.org/wiki/Chernoff_bound,"in probability theory, the chernoff bound gives exponentially decreasing bounds on tail distributions of sums of independent random variables. despite being named after herman chernoff, the author of the paper it first appeared in, the result is due to herman rubin. it is a sharper bound than the known first- or second-moment-based tail bounds such as markov's inequality or chebyshev's inequality, which only yield power-law bounds on tail decay. however, the chernoff bound requires that the variates be independent – a condition that neither markov's inequality nor chebyshev's inequality require, although chebyshev's inequality does require the variates to be pairwise independent. it is related to the (historically prior) bernstein inequalities and to hoeffding's inequality. the generic chernoff bound for a random variable x is attained by applying markov's inequality to etx. this gives a bound in terms of the moment-generating function of x. for every t ≥   {\displaystyle t\geq  } : since this bound is true for every t {\displaystyle t} , we have: the chernoff bound sometimes refers to the above inequality, which was first applied by sergei bernstein to prove the related bernstein inequalities.[citation needed] it is also used to prove hoeffding's inequality, bennett's inequality, and mcdiarmid's inequality. this inequality can be applied generally to various classes of distributions, including sub-gaussian distributions, sub-gamma distributions, and sums of independent random variables. chernoff bounds commonly refer to the case where x {\displaystyle x} is the sum of independent bernoulli random variables. when x is the sum of n independent random variables x , ..., xn, the moment generating function of x is the product of the individual moment generating functions, giving that "
49,49,Chi-squared_Distribution,0,https://en.wikipedia.org/wiki/Chi-squared_distribution,"in probability theory and statistics, the chi-squared distribution (also chi-square or χ -distribution) with k degrees of freedom is the distribution of a sum of the squares of k independent standard normal random variables. the chi-squared distribution is a special case of the gamma distribution and is one of the most widely used probability distributions in inferential statistics, notably in hypothesis testing and in construction of confidence intervals. this distribution is sometimes called the central chi-squared distribution, a special case of the more general noncentral chi-squared distribution. the chi-squared distribution is used in the common chi-squared tests for goodness of fit of an observed distribution to a theoretical one, the independence of two criteria of classification of qualitative data, and in confidence interval estimation for a population standard deviation of a normal distribution from a sample standard deviation. many other statistical tests also use this distribution, such as friedman's analysis of variance by ranks. if z , ..., zk are independent, standard normal random variables, then the sum of their squares, is distributed according to the chi-squared distribution with k degrees of freedom. this is usually denoted as the chi-squared distribution has one parameter: a positive integer k that specifies the number of degrees of freedom (the number of random variables being summed, zi s). the chi-squared distribution is used primarily in hypothesis testing, and to a lesser extent for confidence intervals for population variance when the underlying distribution is normal. unlike more widely known distributions such as the normal distribution and the exponential distribution, the chi-squared distribution is not as often applied in the direct modeling of natural phenomena. it arises in the following hypothesis tests, among others: it is also a component of the definition of the t-distribution and the f-distribution used in t-tests, analysis of variance, and regression analysis. the primary reason for which the chi-squared distribution is extensively used in hypothesis testing is its relationship to the normal distribution. many hypothesis tests use a test statistic, such as the t-statistic in a t-test. for these hypothesis tests, as the sample size, n, increases, the sampling distribution of the test statistic approaches the normal distribution (central limit theorem). because the test statistic (such as t) is asymptotically normally distributed, provided the sample size is sufficiently large, the distribution used for hypothesis testing may be approximated by a normal distribution. testing hypotheses using a normal distribution is well understood and relatively easy. the simplest chi-squared distribution is the square of a standard normal distribution. so wherever a normal distribution could be used for a hypothesis test, a chi-squared distribution could be used. suppose that z {\displaystyle z} is a random variable sampled from the standard normal distribution, where the mean is   {\displaystyle  } and the variance is   {\displaystyle  } : z ∼ n (   ,   ) {\displaystyle z\sim n( , )} . now, consider the random variable q = z   {\displaystyle q=z^{ }} . the distribution of the random variable q {\displaystyle q} is an example of a chi-squared distribution: q ∼ χ     . {\displaystyle \ q\ \sim \ \chi _{ }^{ }.} the subscript   indicates that this particular chi-squared distribution is constructed from only   standard normal distribution. a chi-squared distribution constructed by squaring a single standard normal distribution is said to have   degree of freedom. thus, as the sample size for a hypothesis test increases, the distribution of the test statistic approaches a normal distribution. just as extreme values of the normal distribution have low probability (and give small p-values), extreme values of the chi-squared distribution have low probability. an additional reason that the chi-squared distribution is widely used is that it turns up as the large sample distribution of generalized likelihood ratio tests (lrt). lrt's have several desirable properties; in particular, simple lrt's commonly provide the highest power to reject the null hypothesis (neyman–pearson lemma) and this leads also to optimality properties of generalised lrts. however, the normal and chi-squared approximations are only valid asymptotically. for this reason, it is preferable to use the t distribution rather than the normal approximation or the chi-squared approximation for a small sample size. similarly, in analyses of contingency tables, the chi-squared approximation will be poor for a small sample size, and it is preferable to use fisher's exact test. ramsey shows that the exact binomial test is always more powerful than the normal approximation. "
50,50,Chi-squared_test,0,https://en.wikipedia.org/wiki/Chi-squared_test,"a chi-squared test (also chi-square or χ  test) is a statistical hypothesis test that is valid to perform when the test statistic is chi-squared distributed under the null hypothesis, specifically pearson's chi-squared test and variants thereof. pearson's chi-squared test is used to determine whether there is a statistically significant difference between the expected frequencies and the observed frequencies in one or more categories of a contingency table. in the standard applications of this test, the observations are classified into mutually exclusive classes. if the null hypothesis that there are no differences between the classes in the population is true, the test statistic computed from the observations follows a χ  frequency distribution. the purpose of the test is to evaluate how likely the observed frequencies would be assuming the null hypothesis is true. test statistics that follow a χ  distribution occur when the observations are independent. there are also χ  tests for testing the null hypothesis of independence of a pair of random variables based on observations of the pairs. chi-squared tests often refers to tests for which the distribution of the test statistic approaches the χ  distribution asymptotically, meaning that the sampling distribution (if the null hypothesis is true) of the test statistic approximates a chi-squared distribution more and more closely as sample sizes increase. in the   th century, statistical analytical methods were mainly applied in biological data analysis and it was customary for researchers to assume that observations followed a normal distribution, such as sir george airy and mansfield merriman, whose works were criticized by karl pearson in his      paper. at the end of the   th century, pearson noticed the existence of significant skewness within some biological observations. in order to model the observations regardless of being normal or skewed, pearson, in a series of articles published from      to     , devised the pearson distribution, a family of continuous probability distributions, which includes the normal distribution and many skewed distributions, and proposed a method of statistical analysis consisting of using the pearson distribution to model the observation and performing a test of goodness of fit to determine how well the model really fits to the observations. in     , pearson published a paper on the χ  test which is considered to be one of the foundations of modern statistics. in this paper, pearson investigated a test of goodness of fit. suppose that n observations in a random sample from a population are classified into k mutually exclusive classes with respective observed numbers xi (for i =  , ,…,k), and a null hypothesis gives the probability pi that an observation falls into the ith class. so we have the expected numbers mi = npi for all i, where pearson proposed that, under the circumstance of the null hypothesis being correct, as n → ∞ the limiting distribution of the quantity given below is the χ  distribution. pearson dealt first with the case in which the expected numbers mi are large enough known numbers in all cells assuming every xi may be taken as normally distributed, and reached the result that, in the limit as n becomes large, x  follows the χ  distribution with k −   degrees of freedom. "
51,51,Chinese Remainder Theorem,2,https://en.wikipedia.org/wiki/Chinese_remainder_theorem,"in mathematics, the chinese remainder theorem states that if one knows the remainders of the euclidean division of an integer n by several integers, then one can determine uniquely the remainder of the division of n by the product of these integers, under the condition that the divisors are pairwise coprime (no two divisors share a common factor other than  ). for example, if we know that the remainder of n divided by   is  , the remainder of n divided by   is  , and the remainder of n divided by   is  , then without knowing the value of n, we can determine that the remainder of n divided by     (the product of  ,  , and  ) is   . importantly, this tells us that if n is a natural number less than    , then    is the only possible value of n. the earliest known statement of the theorem is by the chinese mathematician sun-tzu in the sun-tzu suan-ching in the  rd century ce. the chinese remainder theorem is widely used for computing with large integers, as it allows replacing a computation for which one knows a bound on the size of the result by several similar computations on small integers. the chinese remainder theorem (expressed in terms of congruences) is true over every principal ideal domain. it has been generalized to any ring, with a formulation involving two-sided ideals. the earliest known statement of the theorem, as a problem with specific numbers, appears in the  rd-century book sun-tzu suan-ching by the chinese mathematician sun-tzu: there are certain things whose number is unknown. if we count them by threes, we have two left over; by fives, we have three left over; and by sevens, two are left over. how many things are there? sun-tzu's work contains neither a proof nor a full algorithm. what amounts to an algorithm for solving this problem was described by aryabhata ( th century). special cases of the chinese remainder theorem were also known to brahmagupta ( th century), and appear in fibonacci's liber abaci (    ). the result was later generalized with a complete solution called da-yan-shu (大衍術) in ch'in chiu-shao's      mathematical treatise in nine sections (數書九章, shu-shu chiu-chang) which was translated into english in early   th century by british missionary alexander wylie. the notion of congruences was first introduced and used by carl friedrich gauss in his disquisitiones arithmeticae of     . gauss illustrates the chinese remainder theorem on a problem involving calendars, namely, ""to find the years that have a certain period number with respect to the solar and lunar cycle and the roman indiction."" gauss introduces a procedure for solving the problem that had already been used by leonhard euler but was in fact an ancient method that had appeared several times. let n , ..., nk be integers greater than  , which are often called moduli or divisors. let us denote by n the product of the ni. "
52,52,Chord,1,https://en.wikipedia.org/wiki/Chord_(geometry),"a chord of a circle is a straight line segment whose endpoints both lie on a circular arc. the infinite line extension of a chord is a secant line, or just secant. more generally, a chord is a line segment joining two points on any curve, for instance, an ellipse. a chord that passes through a circle's center point is the circle's diameter. the word chord is from the latin chorda meaning bowstring. among properties of chords of a circle are the following: the midpoints of a set of parallel chords of an ellipse are collinear. chords were used extensively in the early development of trigonometry. the first known trigonometric table, compiled by hipparchus, tabulated the value of the chord function for every  + /  degrees. in the second century ad, ptolemy of alexandria compiled a more extensive table of chords in his book on astronomy, giving the value of the chord for angles ranging from  /  to     degrees by increments of  /  degree. the circle was of diameter    , and the chord lengths are accurate to two base-   digits after the integer part. the chord function is defined geometrically as shown in the picture. the chord of an angel is the length of the chord between two points on a unit circle separated by that central angle. the angle θ is taken in the positive sense and must lie in the interval   < θ ≤ π (radian measure). the chord function can be related to the modern sine function, by taking one of the points to be ( , ), and the other point to be (cos θ, sin θ), and then using the pythagorean theorem to calculate the chord length: the last step uses the half-angle formula. much as modern trigonometry is built on the sine function, ancient trigonometry was built on the chord function. hipparchus is purported to have written a twelve-volume work on chords, all now lost, so presumably, a great deal was known about them. in the table below (where c is the chord length, and d the diameter of the circle) the chord function can be shown to satisfy many identities analogous to well-known modern ones: the inverse function exists as well: hawking, s.w., ed. (    ). on the shoulders of giants: the great works of physics and astronomy. philadelphia, pa: running press. isbn  -    -    -x. lccn           . retrieved     -  -  .{{cite book}}: cs  maint: url-status (link) "
53,53,Circle construction with non linear points,1,https://en.wikipedia.org/wiki/Circle," a circle is a shape consisting of all points in a plane that are at a given distance from a given point, the centre; equivalently it is the curve traced out by a point that moves in a plane so that its distance from a given point is constant. the distance between any point of the circle and the centre is called the radius. usually, the radius is required to be a positive number. a circle with r =   {\displaystyle r= } is a degenerate case. this article is about circles in euclidean geometry, and, in particular, the euclidean plane, except where otherwise noted. specifically, a circle is a simple closed curve that divides the plane into two regions: an interior and an exterior. in everyday use, the term ""circle"" may be used interchangeably to refer to either the boundary of the figure, or to the whole figure including its interior; in strict technical usage, the circle is only the boundary and the whole figure is called a disc. a circle may also be defined as a special kind of ellipse in which the two foci are coincident, the eccentricity is  , and the semi-major and semi-minor axes are equal; or the two-dimensional shape enclosing the most area per unit perimeter squared, using calculus of variations. a circle is a plane figure bounded by one curved line, and such that all straight lines drawn from a certain point within it to the bounding line, are equal. the bounding line is called its circumference and the point, its centre.in the field of topology, a circle isn't limited to the geometric concept, but to all of its homeomorphisms. two topological circles are equivalent if one can be transformed into the other via a deformation of r  upon itself (known as an ambient isotopy). all of the specified regions may be considered as open, that is, not containing their boundaries, or as closed, including their respective boundaries. the word circle derives from the greek κίρκος/κύκλος (kirkos/kuklos), itself a metathesis of the homeric greek κρίκος (krikos), meaning ""hoop"" or ""ring"". the origins of the words circus and circuit are closely related. the circle has been known since before the beginning of recorded history. natural circles would have been observed, such as the moon, sun, and a short plant stalk blowing in the wind on sand, which forms a circle shape in the sand. the circle is the basis for the wheel, which, with related inventions such as gears, makes much of modern machinery possible. in mathematics, the study of the circle has helped inspire the development of geometry, astronomy and calculus. early science, particularly geometry and astrology and astronomy, was connected to the divine for most medieval scholars, and many believed that there was something intrinsically ""divine"" or ""perfect"" that could be found in circles. "
54,54,circumcentre of a triangle,2,https://en.wikipedia.org/wiki/Circumscribed_circle,"in geometry, the circumscribed circle or circumcircle of a polygon is a circle that passes through all the vertices of the polygon. the center of this circle is called the circumcenter and its radius is called the circumradius. not every polygon has a circumscribed circle. a polygon that does have one is called a cyclic polygon, or sometimes a concyclic polygon because its vertices are concyclic. all triangles, all regular simple polygons, all rectangles, all isosceles trapezoids, and all right kites are cyclic. a related notion is the one of a minimum bounding circle, which is the smallest circle that completely contains the polygon within it, if the circle's center is within the polygon. every polygon has a unique minimum bounding circle, which may be constructed by a linear time algorithm. even if a polygon has a circumscribed circle, it may be different from its minimum bounding circle. for example, for an obtuse triangle, the minimum bounding circle has the longest side as diameter and does not pass through the opposite vertex. all triangles are cyclic; that is, every triangle has a circumscribed circle. the circumcenter of a triangle can be constructed by drawing any two of the three perpendicular bisectors. for three non-collinear points, these two lines cannot be parallel, and the circumcenter is the point where they cross. any point on the bisector is equidistant from the two points that it bisects, from which it follows that this point, on both bisectors, is equidistant from all three triangle vertices. the circumradius is the distance from it to any of the three vertices. an alternative method to determine the circumcenter is to draw any two lines each one departing from one of the vertices at an angle with the common side, the common angle of departure being   ° minus the angle of the opposite vertex. (in the case of the opposite angle being obtuse, drawing a line at a negative angle means going outside the triangle.) in coastal navigation, a triangle's circumcircle is sometimes used as a way of obtaining a position line using a sextant when no compass is available. the horizontal angle between two landmarks defines the circumcircle upon which the observer lies. in the euclidean plane, it is possible to give explicitly an equation of the circumcircle in terms of the cartesian coordinates of the vertices of the inscribed triangle. suppose that are the coordinates of points a, b, and c. the circumcircle is then the locus of points v = (vx, vy) in the cartesian plane satisfying the equations guaranteeing that the points a, b, c, and v are all the same distance r from the common center u of the circle. using the polarization identity, these equations reduce to the condition that the matrix "
55,55,Classification of discontinuities,2,https://en.wikipedia.org/wiki/Classification_of_discontinuities,"continuous functions are of utmost importance in mathematics, functions and applications. however, not all functions are continuous. if a function is not continuous at a point in its domain, one says that it has a discontinuity there. the set of all points of discontinuity of a function may be a discrete set, a dense set, or even the entire domain of the function. this article describes the classification of discontinuities in the simplest case of functions of a single real variable taking real values. the oscillation of a function at a point quantifies these discontinuities as follows: a special case is if the function diverges to infinity or minus infinity, in which case the oscillation is not defined (in the extended real numbers, this is a removable discontinuity). for each of the following, consider a real valued function f {\displaystyle f} of a real variable x , {\displaystyle x,} defined in a neighborhood of the point x   {\displaystyle x_{ }} at which f {\displaystyle f} is discontinuous. consider the piecewise function the point x   =   {\displaystyle x_{ }= } is a removable discontinuity. for this kind of discontinuity: the one-sided limit from the negative direction: the term removable discontinuity is sometimes broadened to include a removable singularity, in which the limits in both directions exist and are equal, while the function is undefined at the point x   . {\displaystyle x_{ }.} [a] this use is an abuse of terminology because continuity and discontinuity of a function are concepts defined only for points in the function's domain. consider the function then, the point x   =   {\displaystyle x_{ }= } is a jump discontinuity. "
56,56,clockwise,1,https://en.wikipedia.org/wiki/Clockwise,"two-dimensional rotation can occur in two possible directions. clockwise motion (abbreviated cw) proceeds in the same direction as a clock's hands: from the top to the right, then down and then to the left, and back up to the top. the opposite sense of rotation or revolution is (in commonwealth english) anticlockwise (acw) or (in north american english) counterclockwise (ccw). before clocks were commonplace, the terms ""sunwise"" and ""deasil"", ""deiseil"" and even ""deocil"" from the scottish gaelic language and from the same root as the latin ""dexter"" (""right"") were used for clockwise. ""widdershins"" or ""withershins"" (from middle low german ""weddersinnes"", ""opposite course"") was used for counterclockwise. the terms clockwise and counterclockwise can only be applied to a rotational motion once a side of the rotational plane is specified, from which the rotation is observed. for example, the daily rotation of the earth is clockwise when viewed from above the south pole, and counterclockwise when viewed from above the north pole (considering ""above a point"" to be defined as ""farther away from the center of earth and on the same ray""). clocks traditionally follow this sense of rotation because of the clock's predecessor: the sundial. clocks with hands were first built in the northern hemisphere (see clock), and they were made to work like horizontal sundials. in order for such a sundial to work north of the equator during spring and summer, and north of the tropic of cancer the whole year, the noon-mark of the dial must be placed northward of the pole casting the shadow. then, when the sun moves in the sky (from east to south to west), the shadow, which is cast on the sundial in the opposite direction, moves with the same sense of rotation (from west to north to east). this is why hours must be drawn in horizontal sundials in that manner, and why modern clocks have their numbers set in the same way, and their hands moving accordingly. for a vertical sundial (such as those placed on the walls of buildings, the dial being below the post), the movement of the sun is from right to top to left, and, accordingly, the shadow moves from left to down to right, i.e., counterclockwise. this effect is caused by the plane of the dial having been rotated through the plane of the motion of the sun and thus the shadow is observed from the other side of the dial's plane and is observed as moving in the opposite direction. some clocks were constructed to mimic this. the best-known surviving example is the münster astronomical clock, whose hands move counterclockwise. occasionally, clocks whose hands revolve counterclockwise are sold as a novelty. one historic jewish clock was built that way in the jewish town hall in prague in the   th century, using right-to-left reading in the hebrew language. in      under bolivian president evo morales, the clock outside the legislative assembly in plaza murillo, la paz, was shifted to counterclockwise motion to promote indigenous values. typical nuts, screws, bolts, bottle caps, and jar lids are tightened (moved away from the observer) clockwise and loosened (moved towards the observer) counterclockwise in accordance with the right-hand rule. to apply the right-hand rule, place one's loosely clenched right hand above the object with the thumb pointing in the direction one wants the screw, nut, bolt, or cap ultimately to move, and the curl of the fingers, from the palm to the tips, will indicate in which way one needs to turn the screw, nut, bolt or cap to achieve the desired result. almost all threaded objects obey this rule except for a few left-handed exceptions described below. the reason for the clockwise standard for most screws and bolts is that supination of the arm, which is used by a right-handed person to tighten a screw clockwise, is generally stronger than pronation used to loosen. sometimes the opposite (left-handed, counterclockwise, reverse) sense of threading is used for a special reason. a thread might need to be left-handed to prevent operational stresses from loosening it. for example, some older cars and trucks had right-handed lug nuts on the right wheels and left-handed lug nuts on the left wheels, so that, as the vehicle moved forward, the lug nuts tended to tighten rather than loosen. for bicycle pedals, the one on the left must be reverse-threaded to prevent it unscrewing during use. similarly, the flyer whorl of a spinning wheel uses a left-hand thread to keep it from loosening. a turnbuckle has right-handed threads on one end and left-handed threads on the other. some gas fittings are left-handed to prevent disastrous misconnections: oxygen fittings are right-handed, but acetylene, propane, and other flammable gases are unmistakably distinguished by left-handed fittings. in trigonometry and mathematics in general, plane angles are conventionally measured counterclockwise, starting with  ° or   radians pointing directly to the right (or east), and   ° pointing straight up (or north). however, in navigation, compass headings increase clockwise around the compass face, starting with  ° at the top of the compass (the northerly direction), with   ° to the right (east). "
57,57,Codomain of a Function,1,https://en.wikipedia.org/wiki/Codomain,"in mathematics, the codomain or set of destination of a function is the set into which all of the output of the function is constrained to fall. it is the set y in the notation f: x → y. the term range is sometimes ambiguously used to refer to either the codomain or image of a function. a codomain is part of a function f if f is defined as a triple (x, y, g) where x is called the domain of f, y its codomain, and g its graph. the set of all elements of the form f(x), where x ranges over the elements of the domain x, is called the image of f. the image of a function is a subset of its codomain so it might not coincide with it. namely, a function that is not surjective has elements y in its codomain for which the equation f(x) = y does not have a solution. a codomain is not part of a function f if f is defined as just a graph. for example in set theory it is desirable to permit the domain of a function to be a proper class x, in which case there is formally no such thing as a triple (x, y, g). with such a definition functions do not have a codomain, although some authors still use it informally after introducing a function in the form f: x → y. for a function defined by the codomain of f is r {\displaystyle \textstyle \mathbb {r} } , but f does not map to any negative number. thus the image of f is the set r   + {\displaystyle \textstyle \mathbb {r} _{ }^{+}} ; i.e., the interval [ , ∞). an alternative function g is defined thus: while f and g map a given x to the same number, they are not, in this view, the same function because they have different codomains. a third function h can be defined to demonstrate why: the domain of h cannot be r {\displaystyle \textstyle \mathbb {r} } but can be defined to be r   + {\displaystyle \textstyle \mathbb {r} _{ }^{+}} : the compositions are denoted "
58,58,Coefficient matrix,2,https://en.wikipedia.org/wiki/Coefficient_matrix,"in linear algebra, a coefficient matrix is a matrix consisting of the coefficients of the variables in a set of linear equations. the matrix is used in solving systems of linear equations. in general, a system with m linear equations and n unknowns can be written as where x   , x   , … , x n {\displaystyle x_{ },x_{ },\ldots ,x_{n}} are the unknowns and the numbers a    , a    , … , a m n {\displaystyle a_{  },a_{  },\ldots ,a_{mn}} are the coefficients of the system. the coefficient matrix is the m × n matrix with the coefficient a i j {\displaystyle a_{ij}} as the (i, j )th entry: then the above set of equations can be expressed more succinctly as where a is the coefficient matrix and b is the column vector of constant terms. by the rouché–capelli theorem, the system of equations is inconsistent, meaning it has no solutions, if the rank of the augmented matrix (the coefficient matrix augmented with an additional column consisting of the vector b) is greater than the rank of the coefficient matrix. if, on the other hand, the ranks of these two matrices are equal, the system must have at least one solution. the solution is unique if and only if the rank r equals the number n of variables. otherwise the general solution has n – r free parameters; hence in such a case there are an infinitude of solutions, which can be found by imposing arbitrary values on n – r of the variables and solving the resulting system for its unique solution; different choices of which variables to fix, and different fixed values of them, give different system solutions. a first-order matrix difference equation with constant term can be written as where a is n × n and y and c are n ×  . this system converges to its steady-state level of y if and only if the absolute values of all n eigenvalues of a are less than  . a first-order matrix differential equation with constant term can be written as this system is stable if and only if all n eigenvalues of a have negative real parts. "
59,59,Coefficient of variation,2,https://en.wikipedia.org/wiki/Coefficient_of_variation," in probability theory and statistics, the coefficient of variation (cv), also known as relative standard deviation (rsd),[citation needed] is a standardized measure of dispersion of a probability distribution or frequency distribution. it is often expressed as a percentage, and is defined as the ratio of the standard deviation σ {\displaystyle \ \sigma } to the mean μ {\displaystyle \ \mu } (or its absolute value, | μ | {\displaystyle |\mu |} ). the cv or rsd is widely used in analytical chemistry to express the precision and repeatability of an assay. it is also commonly used in fields such as engineering or physics when doing quality assurance studies and anova gauge r&r.[citation needed] in addition, cv is utilized by economists and investors in economic models. the coefficient of variation (cv) is defined as the ratio of the standard deviation σ {\displaystyle \ \sigma } to the mean μ {\displaystyle \ \mu } , c v = σ μ . {\displaystyle c_{\rm {v}}={\frac {\sigma }{\mu }}.} it shows the extent of variability in relation to the mean of the population. the coefficient of variation should be computed only for data measured on scales that have a meaningful zero (ratio scale) and hence allow relative comparison of two measurements (i.e., division of one measurement by the other). the coefficient of variation may not have any meaning for data on an interval scale. for example, most temperature scales (e.g., celsius, fahrenheit etc.) are interval scales with arbitrary zeros, so the computed coefficient of variation would be different depending on which scale you used. on the other hand, kelvin temperature has a meaningful zero, the complete absence of thermal energy, and thus is a ratio scale. in plain language, it is meaningful to say that    kelvin is twice as hot as    kelvin, but only in this scale with a true absolute zero. while a standard deviation (sd) can be measured in kelvin, celsius, or fahrenheit, the value computed is only applicable to that scale. only the kelvin scale can be used to compute a valid coefficient of variability. measurements that are log-normally distributed exhibit stationary cv; in contrast, sd varies depending upon the expected value of measurements. a more robust possibility is the quartile coefficient of dispersion, half the interquartile range ( q   − q   ) /   {\displaystyle {(q_{ }-q_{ })/ }} divided by the average of the quartiles (the midhinge), ( q   + q   ) /   {\displaystyle {(q_{ }+q_{ })/ }} . in most cases, a cv is computed for a single independent variable (e.g., a single factory product) with numerous, repeated measures of a dependent variable (e.g., error in the production process). however, data that are linear or even logarithmically non-linear and include a continuous range for the independent variable with sparse measurements across each value (e.g., scatter-plot) may be amenable to single cv calculation using a maximum-likelihood estimation approach. a data set of [   ,    ,    ] has constant values. its standard deviation is   and average is    , giving the coefficient of variation as a data set of [  ,    ,    ] has more variability. its population standard deviation is  .    and its average is    , giving the coefficient of variation as "
60,60,Combination,2,https://en.wikipedia.org/wiki/Combination,"in mathematics, a combination is a selection of items from a set that has distinct members, such that the order of selection does not matter (unlike permutations). for example, given three fruits, say an apple, an orange and a pear, there are three combinations of two that can be drawn from this set: an apple and a pear; an apple and an orange; or a pear and an orange. more formally, a k-combination of a set s is a subset of k distinct elements of s. so, two combinations are identical if and only if each combination has the same members. (the arrangement of the members in each set does not matter.) if the set has n elements, the number of k-combinations, denoted as c k n {\displaystyle c_{k}^{n}} , is equal to the binomial coefficient which can be written using factorials as n ! k ! ( n − k ) ! {\displaystyle \textstyle {\frac {n!}{k!(n-k)!}}} whenever k ≤ n {\displaystyle k\leq n} , and which is zero when k > n {\displaystyle k>n} . this formula can be derived from the fact that each k-combination of a set s of n members has k ! {\displaystyle k!} permutations so p k n = c k n × k ! {\displaystyle p_{k}^{n}=c_{k}^{n}\times k!} or c k n = p k n / k ! {\displaystyle c_{k}^{n}=p_{k}^{n}/k!} . the set of all k-combinations of a set s is often denoted by ( s k ) {\displaystyle \textstyle {\binom {s}{k}}} . a combination is a combination of n things taken k at a time without repetition. to refer to combinations in which repetition is allowed, the terms k-selection, k-multiset, or k-combination with repetition are often used. if, in the above example, it were possible to have two of any one kind of fruit there would be   more  -selections: one with two apples, one with two oranges, and one with two pears. although the set of three fruits was small enough to write a complete list of combinations, this becomes impractical as the size of the set increases. for example, a poker hand can be described as a  -combination (k =  ) of cards from a    card deck (n =   ). the   cards of the hand are all distinct, and the order of cards in the hand does not matter. there are  ,   ,    such combinations, and the chance of drawing any one hand at random is   /  ,   ,   . the number of k-combinations from a given set s of n elements is often denoted in elementary combinatorics texts by c ( n , k ) {\displaystyle c(n,k)} , or by a variation such as c k n {\displaystyle c_{k}^{n}} , n c k {\displaystyle {}_{n}c_{k}} , n c k {\displaystyle {}^{n}c_{k}} , c n , k {\displaystyle c_{n,k}} or even c n k {\displaystyle c_{n}^{k}} (the latter form is standard in french, romanian, russian, chinese and polish texts[citation needed]). the same number however occurs in many other mathematical contexts, where it is denoted by ( n k ) {\displaystyle {\tbinom {n}{k}}} (often read as ""n choose k""); notably it occurs as a coefficient in the binomial formula, hence its name binomial coefficient. one can define ( n k ) {\displaystyle {\tbinom {n}{k}}} for all natural numbers k at once by the relation from which it is clear that and further, to see that these coefficients count k-combinations from s, one can first consider a collection of n distinct variables xs labeled by the elements s of s, and expand the product over all elements of s: it has  n distinct terms corresponding to all the subsets of s, each subset giving the product of the corresponding variables xs. now setting all of the xs equal to the unlabeled variable x, so that the product becomes (  + x)n, the term for each k-combination from s becomes xk, so that the coefficient of that power in the result equals the number of such k-combinations. binomial coefficients can be computed explicitly in various ways. to get all of them for the expansions up to (  + x)n, one can use (in addition to the basic cases already given) the recursion relation "
61,61,permutations and combinations,1,https://en.wikipedia.org/wiki/Combinations_and_permutations,"combinations and permutations in the mathematical sense are described in several articles. described together, in-depth: explained separately in a more accessible way: for meanings outside of mathematics, please see both words’ disambiguation pages: "
62,62,Sets complement,1,https://en.wikipedia.org/wiki/Complement_(set_theory),"in set theory, the complement of a set a, often denoted by ac (or a′), are the elements not in a. when all sets under consideration are considered to be subsets of a given set u, the absolute complement of a is the set of elements in u that are not in a. the relative complement of a with respect to a set b, also termed the set difference of b and a, written b ∖ a , {\displaystyle b\setminus a,} is the set of elements in b that are not in a. if a is a set, then the absolute complement of a (or simply the complement of a) is the set of elements not in a (within a larger set that is implicitly defined). in other words, let u be a set that contains all the elements under study; if there is no need to mention u, either because it has been previously specified, or it is obvious and unique, then the absolute complement of a is the relative complement of a in u: or formally: the absolute complement of a is usually denoted by ac. other notations include a ¯ , a ′ , {\displaystyle {\overline {a}},a',} ∁ u a , and ∁ a . {\displaystyle \complement _{u}a,{\text{ and }}\complement a.} let a and b be two sets in a universe u. the following identities capture important properties of absolute complements: de morgan's laws: complement laws: involution or double complement law: "
63,63,Complementary Event,1,https://en.wikipedia.org/wiki/Complementary_event,"in probability theory, the complement of any event a is the event [not a], i.e. the event that a does not occur. the event a and its complement [not a] are mutually exclusive and exhaustive. generally, there is only one event b such that a and b are both mutually exclusive and exhaustive; that event is the complement of a. the complement of an event a is usually denoted as a′, ac, ¬ {\displaystyle \neg } a or a. given an event, the event and its complementary event define a bernoulli trial: did the event occur or not? for example, if a typical coin is tossed and one assumes that it cannot land on its edge, then it can either land showing ""heads"" or ""tails."" because these two outcomes are mutually exclusive (i.e. the coin cannot simultaneously show both heads and tails) and collectively exhaustive (i.e. there are no other possible outcomes not represented between these two), they are therefore each other's complements. this means that [heads] is logically equivalent to [not tails], and [tails] is equivalent to [not heads]. in a random experiment, the probabilities of all possible events (the sample space) must total to  — that is, some outcome must occur on every trial. for two events to be complements, they must be collectively exhaustive, together filling the entire sample space. therefore, the probability of an event's complement must be unity minus the probability of the event. that is, for an event a, equivalently, the probabilities of an event and its complement must always total to  . this does not, however, mean that any two events whose probabilities total to   are each other's complements; complementary events must also fulfill the condition of mutual exclusivity. suppose one throws an ordinary six-sided die eight times. what is the probability that one sees a "" "" at least once? it may be tempting to say that this result cannot be right because a probability cannot be more than  . the technique is wrong because the eight events whose probabilities got added are not mutually exclusive. one may resolve this overlap by the principle of inclusion-exclusion, or, in this case, by simply finding the probability of the complementary event and subtracting it from  , thus: "
64,64,Completing The square method,1,https://en.wikipedia.org/wiki/Completing_the_square,"in elementary algebra, completing the square is a technique for converting a quadratic polynomial of the form to the form for some values of h and k. completing the square is used in in mathematics, completing the square is often applied in any computation involving quadratic polynomials. the formula in elementary algebra for computing the square of a binomial is: for example: in any perfect square, the coefficient of x is twice the number p, and the constant term is equal to p . consider the following quadratic polynomial: this quadratic is not a perfect square, since    is not the square of  : "
65,65,Complex analysis,2,https://en.wikipedia.org/wiki/Complex_analysis,"complex analysis, traditionally known as the theory of functions of a complex variable, is the branch of mathematical analysis that investigates functions of complex numbers. it is helpful in many branches of mathematics, including algebraic geometry, number theory, analytic combinatorics, applied mathematics; as well as in physics, including the branches of hydrodynamics, thermodynamics, and particularly quantum mechanics. by extension, use of complex analysis also has applications in engineering fields such as nuclear, aerospace, mechanical and electrical engineering.[citation needed] as a differentiable function of a complex variable is equal to its taylor series (that is, it is analytic), complex analysis is particularly concerned with analytic functions of a complex variable (that is, holomorphic functions). complex analysis is one of the classical branches in mathematics, with roots in the   th century and just prior. important mathematicians associated with complex numbers include euler, gauss, riemann, cauchy, weierstrass, and many more in the   th century. complex analysis, in particular the theory of conformal mappings, has many physical applications and is also used throughout analytic number theory. in modern times, it has become very popular through a new boost from complex dynamics and the pictures of fractals produced by iterating holomorphic functions. another important application of complex analysis is in string theory which examines conformal invariants in quantum field theory. a complex function is a function from complex numbers to complex numbers. in other words, it is a function that has a subset of the complex numbers as a domain and the complex numbers as a codomain. complex functions are generally supposed to have a domain that contains a nonempty open subset of the complex plane. for any complex function, the values z {\displaystyle z} from the domain and their images f ( z ) {\displaystyle f(z)} in the range may be separated into real and imaginary parts: where x , y , u ( x , y ) , v ( x , y ) {\displaystyle x,y,u(x,y),v(x,y)} are all real-valued. in other words, a complex function f : c → c {\displaystyle f:\mathbb {c} \to \mathbb {c} } may be decomposed into i.e., into two real-valued functions ( u {\displaystyle u} , v {\displaystyle v} ) of two real variables ( x {\displaystyle x} , y {\displaystyle y} ). similarly, any complex-valued function f on an arbitrary set x can be considered as an ordered pair of two real-valued functions: (re f, im f) or, alternatively, as a vector-valued function from x into r   . {\displaystyle \mathbb {r} ^{ }.} some properties of complex-valued functions (such as continuity) are nothing more than the corresponding properties of vector valued functions of two real variables. other concepts of complex analysis, such as differentiability are direct generalizations of the similar concepts for real functions, but may have very different properties. in particular, every differentiable complex function is analytic (see next section), and two differentiable functions that are equal in a neighborhood of a point are equal on the intersection of their domain (if the domains are connected). the latter property is the basis of the principle of analytic continuation which allows extending every real analytic function in a unique way for getting a complex analytic function whose domain is the whole complex plane with a finite number of curve arcs removed. many basic and special complex functions are defined in this way, including the complex exponential function, complex logarithm functions, and trigonometric functions. "
66,66,complex conjugate number,1,https://en.wikipedia.org/wiki/Complex_conjugate,"in mathematics, the complex conjugate of a complex number is the number with an equal real part and an imaginary part equal in magnitude but opposite in sign. that is, (if a {\displaystyle a} and b {\displaystyle b} are real, then) the complex conjugate of a + b i {\displaystyle a+bi} is equal to a − b i . {\displaystyle a-bi.} the complex conjugate of z {\displaystyle z} is often denoted as z ¯ . {\displaystyle {\overline {z}}.} in polar form, the conjugate of r e i φ {\displaystyle re^{i\varphi }} is r e − i φ . {\displaystyle re^{-i\varphi }.} this can be shown using euler's formula. the product of a complex number and its conjugate is a real number: a   + b   {\displaystyle a^{ }+b^{ }} (or r   {\displaystyle r^{ }} in polar coordinates). if a root of a univariate polynomial with real coefficients is complex, then its complex conjugate is also a root. the complex conjugate of a complex number z {\displaystyle z} is written as z ¯ {\displaystyle {\overline {z}}} or z ∗ . {\displaystyle z^{*}.} the first notation, a vinculum, avoids confusion with the notation for the conjugate transpose of a matrix, which can be thought of as a generalization of the complex conjugate. the second is preferred in physics, where dagger (†) is used for the conjugate transpose, as well as electrical engineering and computer engineering, where bar notation can be confused for the logical negation (""not"") boolean algebra symbol, while the bar notation is more common in pure mathematics. if a complex number is represented as a   ×   {\displaystyle  \times  } matrix, the notations are identical.[clarification needed] the following properties apply for all complex numbers z {\displaystyle z} and w , {\displaystyle w,} unless stated otherwise, and can be proved by writing z {\displaystyle z} and w {\displaystyle w} in the form a + b i . {\displaystyle a+bi.} for any two complex numbers, conjugation is distributive over addition, subtraction, multiplication and division: a complex number is equal to its complex conjugate if its imaginary part is zero, or equivalently, if the number is real. in other words, real numbers are the only fixed points of conjugation. conjugation does not change the modulus of a complex number: | z ¯ | = | z | . {\displaystyle \left|{\overline {z}}\right|=|z|.} conjugation is an involution, that is, the conjugate of the conjugate of a complex number z {\displaystyle z} is z . {\displaystyle z.} in symbols, z ¯ ¯ = z . {\displaystyle {\overline {\overline {z}}}=z.} "
67,67,Complex Numbers,2,https://en.wikipedia.org/wiki/Complex_number," in mathematics, a complex number is an element of a number system that contains the real numbers and a specific element denoted i, called the imaginary unit, and satisfying the equation i  = − . moreover, every complex number can be expressed in the form a + bi, where a and b are real numbers. because no real number satisfies the above equation, i was called an imaginary number by rené descartes. for the complex number a + bi, a is called the real part and b is called the imaginary part. the set of complex numbers is denoted by either of the symbols c {\displaystyle \mathbb {c} } or c. despite the historical nomenclature ""imaginary"", complex numbers are regarded in the mathematical sciences as just as ""real"" as the real numbers and are fundamental in many aspects of the scientific description of the natural world. [a] complex numbers allow solutions to all polynomial equations, even those that have no solutions in real numbers. more precisely, the fundamental theorem of algebra asserts that every non-constant polynomial equation with real or complex coefficients has a solution which is a complex number. for example, the equation ( x +   )   = −   {\displaystyle (x+ )^{ }=- } has no real solution, since the square of a real number cannot be negative, but has the two nonreal complex solutions −  +  i and −  −  i. addition, subtraction and multiplication of complex numbers can be naturally defined by using the rule i  = −  combined with the associative, commutative and distributive laws. every nonzero complex number has a multiplicative inverse. this makes the complex numbers a field that has the real numbers as a subfield. the complex numbers also form a real vector space of dimension two, with { , i} as a standard basis. this standard basis makes the complex numbers a cartesian plane, called the complex plane. this allows a geometric interpretation of the complex numbers and their operations, and conversely expressing in terms of complex numbers some geometric properties and constructions. for example, the real numbers form the real line which is identified to the horizontal axis of the complex plane. the complex numbers of absolute value one form the unit circle. the addition of a complex number is a translation in the complex plane, and the multiplication by a complex number is a similarity centered at the origin. the complex conjugation is the reflection symmetry with respect to the real axis. the complex absolute value is a euclidean norm. in summary, the complex numbers form a rich structure that is simultaneously an algebraically closed field, a commutative algebra over the reals, and a euclidean vector space of dimension two. a complex number is a number of the form a + bi, where a and b are real numbers, and i is an indeterminate satisfying i  = − . for example,   +  i is a complex number. this way, a complex number is defined as a polynomial with real coefficients in the single indeterminate i, for which the relation i  +   =   is imposed. based on this definition, complex numbers can be added and multiplied, using the addition and multiplication for polynomials. the relation i  +   =   induces the equalities i k =  , i k+  = i, i k+  = − , and i k+  = −i, which hold for all integers k; these allow the reduction of any polynomial that results from the addition and multiplication of complex numbers to a linear polynomial in i, again of the form a + bi with real coefficients a, b. the real number a is called the real part of the complex number a + bi; the real number b is called its imaginary part. to emphasize, the imaginary part does not include a factor i; that is, the imaginary part is b, not bi. formally, the complex numbers are defined as the quotient ring of the polynomial ring in the indeterminate i, by the ideal generated by the polynomial i  +   (see below). "
68,68,Complex Plane,2,https://en.wikipedia.org/wiki/Complex_plane,"in mathematics, the complex plane is the plane formed by the complex numbers, with a cartesian coordinate system such that the x-axis, called real axis, is formed by the real numbers, and the y-axis, called imaginary axis, is formed by the imaginary numbers. the complex plane allows a geometric interpretation of complex numbers. under addition, they add like vectors. the multiplication of two complex numbers can be expressed more easily in polar coordinates—the magnitude or modulus of the product is the product of the two absolute values, or moduli, and the angle or argument of the product is the sum of the two angles, or arguments. in particular, multiplication by a complex number of modulus   acts as a rotation. the complex plane is sometimes known as the argand plane or gauss plane. in complex analysis, the complex numbers are customarily represented by the symbol z, which can be separated into its real (x) and imaginary (y) parts: in the cartesian plane the point (x, y) can also be represented in polar coordinates as in the cartesian plane it may be assumed that the arctangent takes values from −π/  to π/  (in radians), and some care must be taken to define the more complete arctangent function for points (x, y) when x ≤  .[note  ] in the complex plane these polar coordinates take the form here |z| is the absolute value or modulus of the complex number z; θ, the argument of z, is usually taken on the interval   ≤ θ <  π; and the last equality (to |z|eiθ) is taken from euler's formula. without the constraint on the range of θ, the argument of z is multi-valued, because the complex exponential function is periodic, with period  π i. thus, if θ is one value of arg(z), the other values are given by arg(z) = θ +  nπ, where n is any non-zero integer. while seldom used explicitly, the geometric view of the complex numbers is implicitly based on its structure of a euclidean vector space of dimension  , where the inner product of complex numbers w and z is given by ℜ ( w z ¯ ) {\displaystyle \re (w{\overline {z}})} ; then for a complex number z its absolute value |z| coincides with its euclidean norm, and its argument arg(z) with the angle turning from   to z. the theory of contour integration comprises a major part of complex analysis. in this context, the direction of travel around a closed curve is important – reversing the direction in which the curve is traversed multiplies the value of the integral by − . by convention the positive direction is counterclockwise. for example, the unit circle is traversed in the positive direction when we start at the point z =  , then travel up and to the left through the point z = i, then down and to the left through − , then down and to the right through −i, and finally up and to the right to z =  , where we started. almost all of complex analysis is concerned with complex functions – that is, with functions that map some subset of the complex plane into some other (possibly overlapping, or even identical) subset of the complex plane. here it is customary to speak of the domain of f(z) as lying in the z-plane, while referring to the range of f(z) as a set of points in the w-plane. in symbols we write "
69,69,Composite Number,1,https://en.wikipedia.org/wiki/Composite_number,"a composite number is a positive integer that can be formed by multiplying two smaller positive integers. equivalently, it is a positive integer that has at least one divisor other than   and itself. every positive integer is composite, prime, or the unit  , so the composite numbers are exactly the numbers that are not prime and not a unit. for example, the integer    is a composite number because it is the product of the two smaller integers   ×  . likewise, the integers   and   are not composite numbers because each of them can only be divided by one and itself. the composite numbers up to     are every composite number can be written as the product of two or more (not necessarily distinct) primes. for example, the composite number     can be written as    ×   , and the composite number     can be written as    ×    ×  ; furthermore, this representation is unique up to the order of the factors. this fact is called the fundamental theorem of arithmetic. there are several known primality tests that can determine whether a number is prime or composite, without necessarily revealing the factorization of a composite input. one way to classify composite numbers is by counting the number of prime factors. a composite number with two prime factors is a semiprime or  -almost prime (the factors need not be distinct, hence squares of primes are included). a composite number with three distinct prime factors is a sphenic number. in some applications, it is necessary to differentiate between composite numbers with an odd number of distinct prime factors and those with an even number of distinct prime factors. for the latter (where μ is the möbius function and x is half the total of prime factors), while for the former however, for prime numbers, the function also returns −  and μ (   ) =   {\displaystyle \mu ( )= } . for a number n with one or more repeated prime factors, if all the prime factors of a number are repeated it is called a powerful number (all perfect powers are powerful numbers). if none of its prime factors are repeated, it is called squarefree. (all prime numbers and   are squarefree.) for example,    =    ×   , all the prime factors are repeated, so    is a powerful number.    =   ×   ×  , none of the prime factors are repeated, so    is squarefree. "
70,70,concave function,2,https://en.wikipedia.org/wiki/Concave_function," in mathematics, a concave function is the negative of a convex function. a concave function is also synonymously called concave downwards, concave down, convex upwards, convex cap, or upper convex. a real-valued function f {\displaystyle f} on an interval (or, more generally, a convex set in vector space) is said to be concave if, for any x {\displaystyle x} and y {\displaystyle y} in the interval and for any α ∈ [   ,   ] {\displaystyle \alpha \in [ , ]} , a function is called strictly concave if for any α ∈ (   ,   ) {\displaystyle \alpha \in ( , )} and x ≠ y {\displaystyle x\neq y} . for a function f : r → r {\displaystyle f:\mathbb {r} \to \mathbb {r} } , this second definition merely states that for every z {\displaystyle z} strictly between x {\displaystyle x} and y {\displaystyle y} , the point ( z , f ( z ) ) {\displaystyle (z,f(z))} on the graph of f {\displaystyle f} is above the straight line joining the points ( x , f ( x ) ) {\displaystyle (x,f(x))} and ( y , f ( y ) ) {\displaystyle (y,f(y))} . a function f {\displaystyle f} is quasiconcave if the upper contour sets of the function s ( a ) = { x : f ( x ) ≥ a } {\displaystyle s(a)=\{x:f(x)\geq a\}} are convex sets. "
71,71,concurrency of lines,1,https://en.wikipedia.org/wiki/Concurrent_lines,"lines in a plane or higher-dimensional space are said to be concurrent if they intersect at a single point. they are in contrast to parallel lines. in a triangle, four basic types of sets of concurrent lines are altitudes, angle bisectors, medians, and perpendicular bisectors: other sets of lines associated with a triangle are concurrent as well. for example: according to the rouché–capelli theorem, a system of equations is consistent if and only if the rank of the coefficient matrix is equal to the rank of the augmented matrix (the coefficient matrix augmented with a column of intercept terms), and the system has a unique solution if and only if that common rank equals the number of variables. thus with two variables the k lines in the plane, associated with a set of k equations, are concurrent if and only if the rank of the k ×   coefficient matrix and the rank of the k ×   augmented matrix are both  . in that case only two of the k equations are independent, and the point of concurrency can be found by solving any two mutually independent equations simultaneously for the two variables. in projective geometry, in two dimensions concurrency is the dual of collinearity; in three dimensions, concurrency is the dual of coplanarity. "
72,72,conditional dependence,2,https://en.wikipedia.org/wiki/Conditional_dependence,"in probability theory, conditional dependence is a relationship between two or more events that are dependent when a third event occurs. for example, if a {\displaystyle a} and b {\displaystyle b} are two events that individually increase the probability of a third event c , {\displaystyle c,} and do not directly affect each other, then initially (when it has not been observed whether or not the event c {\displaystyle c} occurs) but suppose that now c {\displaystyle c} is observed to occur. if event b {\displaystyle b} occurs then the probability of occurrence of the event a {\displaystyle a} will decrease because its positive relation to c {\displaystyle c} is less necessary as an explanation for the occurrence of c {\displaystyle c} (similarly, event a {\displaystyle a} occurring will decrease the probability of occurrence of b {\displaystyle b} ). hence, now the two events a {\displaystyle a} and b {\displaystyle b} are conditionally negatively dependent on each other because the probability of occurrence of each is negatively dependent on whether the other occurs. we have conditional dependence is different from conditional independence. in conditional independence two events (which may be dependent or not) become independent given the occurrence of a third event. in essence probability is influenced by a person's information about the possible occurrence of an event. for example, let the event a {\displaystyle a} be 'i have a new phone'; event b {\displaystyle b} be 'i have a new watch'; and event c {\displaystyle c} be 'i am happy'; and suppose that having either a new phone or a new watch increases the probability of my being happy. let us assume that the event c {\displaystyle c} has occurred – meaning 'i am happy'. now if another person sees my new watch, he/she will reason that my likelihood of being happy was increased by my new watch, so there is less need to attribute my happiness to a new phone. to make the example more numerically specific, suppose that there are four possible states ω = { s   , s   , s   , s   } , {\displaystyle \omega =\left\{s_{ },s_{ },s_{ },s_{ }\right\},} given in the middle four columns of the following table, in which the occurrence of event a {\displaystyle a} is signified by a   {\displaystyle  } in row a {\displaystyle a} and its non-occurrence is signified by a   , {\displaystyle  ,} and likewise for b {\displaystyle b} and c . {\displaystyle c.} that is, a = { s   , s   } , b = { s   , s   } , {\displaystyle a=\left\{s_{ },s_{ }\right\},b=\left\{s_{ },s_{ }\right\},} and c = { s   , s   , s   } . {\displaystyle c=\left\{s_{ },s_{ },s_{ }\right\}.} the probability of s i {\displaystyle s_{i}} is   /   {\displaystyle  / } for every i . {\displaystyle i.} and so in this example, c {\displaystyle c} occurs if and only if at least one of a , b {\displaystyle a,b} occurs. unconditionally (that is, without reference to c {\displaystyle c} ), a {\displaystyle a} and b {\displaystyle b} are independent of each other because p ⁡ ( a ) {\displaystyle \operatorname {p} (a)} —the sum of the probabilities associated with a   {\displaystyle  } in row a {\displaystyle a} —is     , {\displaystyle {\tfrac { }{ }},} while "
73,73,Conditional Probabaility,1,https://en.wikipedia.org/wiki/Conditional_probability,"in probability theory, conditional probability is a measure of the probability of an event occurring, given that another event (by assumption, presumption, assertion or evidence) has already occurred. this particular method relies on event b occurring with some sort of relationship with another event a. in this event, the event b can be analyzed by a conditionally probability with respect to a. if the event of interest is a and the event b is known or assumed to have occurred, ""the conditional probability of a given b"", or ""the probability of a under the condition b"", is usually written as p(a|b) or occasionally pb(a). this can also be understood as the fraction of probability b that intersects with a: p ( a ∣ b ) = p ( a ∩ b ) p ( b ) {\displaystyle p(a\mid b)={\frac {p(a\cap b)}{p(b)}}} . for example, the probability that any given person has a cough on any given day may be only  %. but if we know or assume that the person is sick, then they are much more likely to be coughing. for example, the conditional probability that someone unwell (sick) is coughing might be   %, in which case we would have that p(cough) =  % and p(cough|sick) =   %. although there is a relationship between a and b in this example, such a relationship or dependence between a and b is not necessary, nor do they have to occur simultaneously. p(a|b) may or may not be equal to p(a) (the unconditional probability of a). if p(a|b) = p(a), then events a and b are said to be independent: in such a case, knowledge about either event does not alter the likelihood of each other. p(a|b) (the conditional probability of a given b) typically differs from p(b|a). for example, if a person has dengue fever, the person might have a   % chance of being tested as positive for the disease. in this case, what is being measured is that if event b (having dengue) has occurred, the probability of a (tested as positive) given that b occurred is   %, simply writing p(a|b) =   %. alternatively, if a person is tested as positive for dengue fever, they may have only a   % chance of actually having this rare disease due to high false positive rates. in this case, the probability of the event b (having dengue) given that the event a (testing positive) has occurred is   % or p(b|a) =   %. it should be apparent now that falsely equating the two probabilities can lead to various errors of reasoning, which is commonly seen through base rate fallacies. while conditional probabilities can provide extremely useful information, limited information is often supplied or at hand. therefore, it can be useful to reverse or convert a condition probability using bayes' theorem: p ( a | b ) = p ( b | a ) ∗ p ( a ) p ( b ) {\displaystyle p(a|b)={{p(b|a)*p(a)} \over {p(b)}}} . another option is to display conditional probabilities in conditional probability table to illuminate the relationship between events. given two events a and b from the sigma-field of a probability space, with the unconditional probability of b being greater than zero (i.e., p(b) >  ), the conditional probability of a given b ( p ( a ∣ b ) {\displaystyle p(a\mid b)} ) is the probability of a occurring if b has or is assumed to have happened. a is assumed to a set of all possible outcomes of an experiment or random trial that has a restricted or reduced sample space. the conditional probability can be found by the quotient of the probability of the joint intersection of events a and b ( p ( a ∩ b ) {\displaystyle p(a\cap b)} ) -- the probability at which a and b occur together, although not necessarily occurring at the same time-- and the probability of b: for a sample space consisting of equal likelihood outcomes, the probability of the event a is understood as the fraction of the number of outcomes in a to the number of all outcomes in the sample space. then, this equation is understood as the fraction of the set a ∩ b {\displaystyle a\cap b} to the set b. note that the above equation is a definition, not just a theoretical result. we denote the quantity p ( a ∩ b ) p ( b ) {\displaystyle {\frac {p(a\cap b)}{p(b)}}} as p ( a ∣ b ) {\displaystyle p(a\mid b)} and call it the ""conditional probability of a given b."" some authors, such as de finetti, prefer to introduce conditional probability as an axiom of probability: this equation for a conditional probability, although mathematically equivalent, may be intuitively easier to understand. it can be interpreted as ""the probability of b occurring multiplied by the probability of a occurring, provided that b has occurred, is equal to the probability of the a and b occurrences together, although not necessarily occurring at the same time"". additionally, this may be preferred philosophically; under major probability interpretations, such as the subjective theory, conditional probability is considered a primitive entity. moreover, this ""multiplication rule"" can be practically useful in computing the probability of a ∩ b {\displaystyle a\cap b} and introduces a symmetry with the summation axiom for mutually exclusive events: conditional probability can be defined as the probability of a conditional event a b {\displaystyle a_{b}} . the goodman–nguyen–van fraassen conditional event can be defined as: it can be shown that "
74,74,conditional Probability distribution,3,https://en.wikipedia.org/wiki/Conditional_probability_distribution,"in probability theory and statistics, given two jointly distributed random variables x {\displaystyle x} and y {\displaystyle y} , the conditional probability distribution of y {\displaystyle y} given x {\displaystyle x} is the probability distribution of y {\displaystyle y} when x {\displaystyle x} is known to be a particular value; in some cases the conditional probabilities may be expressed as functions containing the unspecified value x {\displaystyle x} of x {\displaystyle x} as a parameter. when both x {\displaystyle x} and y {\displaystyle y} are categorical variables, a conditional probability table is typically used to represent the conditional probability. the conditional distribution contrasts with the marginal distribution of a random variable, which is its distribution without reference to the value of the other variable. if the conditional distribution of y {\displaystyle y} given x {\displaystyle x} is a continuous distribution, then its probability density function is known as the conditional density function. the properties of a conditional distribution, such as the moments, are often referred to by corresponding names such as the conditional mean and conditional variance. more generally, one can refer to the conditional distribution of a subset of a set of more than two variables; this conditional distribution is contingent on the values of all the remaining variables, and if more than one variable is included in the subset then this conditional distribution is the conditional joint distribution of the included variables. for discrete random variables, the conditional probability mass function of y {\displaystyle y} given x = x {\displaystyle x=x} can be written according to its definition as: p y | x ( y ∣ x ) ≜ p ( y = y ∣ x = x ) = p ( { x = x } ∩ { y = y } ) p ( x = x ) {\displaystyle p_{y|x}(y\mid x)\triangleq p(y=y\mid x=x)={\frac {p(\{x=x\}\cap \{y=y\})}{p(x=x)}}} due to the occurrence of p ( x = x ) {\displaystyle p(x=x)} in the denominator, this is defined only for non-zero (hence strictly positive) p ( x = x ) . {\displaystyle p(x=x).} the relation with the probability distribution of x {\displaystyle x} given y {\displaystyle y} is: consider the roll of a fair die and let x =   {\displaystyle x= } if the number is even (i.e.,  ,  , or  ) and x =   {\displaystyle x= } otherwise. furthermore, let y =   {\displaystyle y= } if the number is prime (i.e.,  ,  , or  ) and y =   {\displaystyle y= } otherwise. then the unconditional probability that x =   {\displaystyle x= } is  /  =  /  (since there are six possible rolls of the die, of which three are even), whereas the probability that x =   {\displaystyle x= } conditional on y =   {\displaystyle y= } is  /  (since there are three possible prime number rolls— ,  , and  —of which one is even). similarly for continuous random variables, the conditional probability density function of y {\displaystyle y} given the occurrence of the value x {\displaystyle x} of x {\displaystyle x} can be written as : p.    "
75,75,Congruence,1,https://en.wikipedia.org/wiki/Congruence_(geometry)," in geometry, two figures or objects are congruent if they have the same shape and size, or if one has the same shape and size as the mirror image of the other. more formally, two sets of points are called congruent if, and only if, one can be transformed into the other by an isometry, i.e., a combination of rigid motions, namely a translation, a rotation, and a reflection. this means that either object can be repositioned and reflected (but not resized) so as to coincide precisely with the other object. so two distinct plane figures on a piece of paper are congruent if we can cut them out and then match them up completely. turning the paper over is permitted. in elementary geometry the word congruent is often used as follows. the word equal is often used in place of congruent for these objects. in this sense, two plane figures are congruent implies that their corresponding characteristics are ""congruent"" or ""equal"" including not just their corresponding sides and angles, but also their corresponding diagonals, perimeters, and areas. the related concept of similarity applies if the objects have the same shape but do not necessarily have the same size. (most definitions consider congruence to be a form of similarity, although a minority require that the objects have different sizes in order to qualify as similar.) for two polygons to be congruent, they must have an equal number of sides (and hence an equal number—the same number—of vertices). two polygons with n sides are congruent if and only if they each have numerically identical sequences (even if clockwise for one polygon and counterclockwise for the other) side-angle-side-angle-... for n sides and n angles. congruence of polygons can be established graphically as follows: if at any time the step cannot be completed, the polygons are not congruent. two triangles are congruent if their corresponding sides are equal in length, and their corresponding angles are equal in measure. "
76,76,Conic Sections,2,https://en.wikipedia.org/wiki/Conic_section,"in mathematics, a conic section (or simply conic) is a curve obtained as the intersection of the surface of a cone with a plane. the three types of conic section are the hyperbola, the parabola, and the ellipse; the circle is a special case of the ellipse, though historically it was sometimes called a fourth type. the ancient greek mathematicians studied conic sections, culminating around     bc with apollonius of perga's systematic work on their properties. the conic sections in the euclidean plane have various distinguishing properties, many of which can be used as alternative definitions. one such property defines a non-circular conic to be the set of those points whose distances to some particular point, called a focus, and some particular line, called a directrix, are in a fixed ratio, called the eccentricity. the type of conic is determined by the value of the eccentricity. in analytic geometry, a conic may be defined as a plane algebraic curve of degree  ; that is, as the set of points whose coordinates satisfy a quadratic equation in two variables, which may be written in matrix form. this equation allows deducing and expressing algebraically the geometric properties of conic sections. in the euclidean plane, the three types of conic sections appear quite different, but share many properties. by extending the euclidean plane to include a line at infinity, obtaining a projective plane, the apparent difference vanishes: the branches of a hyperbola meet in two points at infinity, making it a single closed curve; and the two ends of a parabola meet to make it a closed curve tangent to the line at infinity. further extension, by expanding the real coordinates to admit complex coordinates, provides the means to see this unification algebraically. the conic sections have been studied for thousands of years and have provided a rich source of interesting and beautiful results in euclidean geometry. a conic is the curve obtained as the intersection of a plane, called the cutting plane, with the surface of a double cone (a cone with two nappes). it is usually assumed that the cone is a right circular cone for the purpose of easy description, but this is not required; any double cone with some circular cross-section will suffice. planes that pass through the vertex of the cone will intersect the cone in a point, a line or a pair of intersecting lines. these are called degenerate conics and some authors do not consider them to be conics at all. unless otherwise stated, ""conic"" in this article will refer to a non-degenerate conic. there are three types of conics: the ellipse, parabola, and hyperbola. the circle is a special kind of ellipse, although historically apollonius considered it a fourth type. ellipses arise when the intersection of the cone and plane is a closed curve. the circle is obtained when the cutting plane is parallel to the plane of the generating circle of the cone; for a right cone, this means the cutting plane is perpendicular to the axis. if the cutting plane is parallel to exactly one generating line of the cone, then the conic is unbounded and is called a parabola. in the remaining case, the figure is a hyperbola: the plane intersects both halves of the cone, producing two separate unbounded curves. alternatively, one can define a conic section purely in terms of plane geometry: it is the locus of all points p whose distance to a fixed point f (called the focus) is a constant multiple (called the eccentricity e) of the distance from p to a fixed line l (called the directrix). for   < e <   we obtain an ellipse, for e =   a parabola, and for e >   a hyperbola. a circle is a limiting case and is not defined by a focus and directrix in the euclidean plane. the eccentricity of a circle is defined to be zero and its focus is the center of the circle, but its directrix can only be taken as the line at infinity in the projective plane. the eccentricity of an ellipse can be seen as a measure of how far the ellipse deviates from being circular. :     if the angle between the surface of the cone and its axis is β {\displaystyle \beta } and the angle between the cutting plane and the axis is α , {\displaystyle \alpha ,} the eccentricity is cos ⁡ α cos ⁡ β . {\displaystyle {\frac {\cos \alpha }{\cos \beta }}.} "
77,77,Connectivity,1,https://en.wikipedia.org/wiki/Connectivity_(graph_theory),"in mathematics and computer science, connectivity is one of the basic concepts of graph theory: it asks for the minimum number of elements (nodes or edges) that need to be removed to separate the remaining nodes into two or more isolated subgraphs. it is closely related to the theory of network flow problems. the connectivity of a graph is an important measure of its resilience as a network. in an undirected graph g, two vertices u and v are called connected if g contains a path from u to v. otherwise, they are called disconnected. if the two vertices are additionally connected by a path of length  , i.e. by a single edge, the vertices are called adjacent. a graph is said to be connected if every pair of vertices in the graph is connected. this means that there is a path between every pair of vertices. an undirected graph that is not connected is called disconnected. an undirected graph g is therefore disconnected if there exist two vertices in g such that no path in g has these vertices as endpoints. a graph with just one vertex is connected. an edgeless graph with two or more vertices is disconnected. a directed graph is called weakly connected if replacing all of its directed edges with undirected edges produces a connected (undirected) graph. it is unilaterally connected or unilateral (also called semiconnected) if it contains a directed path from u to v or a directed path from v to u for every pair of vertices u, v. it is strongly connected, or simply strong, if it contains a directed path from u to v and a directed path from v to u for every pair of vertices u, v. a connected component is a maximal connected subgraph of an undirected graph. each vertex belongs to exactly one connected component, as does each edge. a graph is connected if and only if it has exactly one connected component. the strong components are the maximal strongly connected subgraphs of a directed graph. a vertex cut or separating set of a connected graph g is a set of vertices whose removal renders g disconnected. the vertex connectivity κ(g) (where g is not a complete graph) is the size of a minimal vertex cut. a graph is called k-vertex-connected or k-connected if its vertex connectivity is k or greater. more precisely, any graph g (complete or not) is said to be k-vertex-connected if it contains at least k+  vertices, but does not contain a set of k −   vertices whose removal disconnects the graph; and κ(g) is defined as the largest k such that g is k-connected. in particular, a complete graph with n vertices, denoted kn, has no vertex cuts at all, but κ(kn) = n −  . a vertex cut for two vertices u and v is a set of vertices whose removal from the graph disconnects u and v. the local connectivity κ(u, v) is the size of a smallest vertex cut separating u and v. local connectivity is symmetric for undirected graphs; that is, κ(u, v) = κ(v, u). moreover, except for complete graphs, κ(g) equals the minimum of κ(u, v) over all nonadjacent pairs of vertices u, v.  -connectivity is also called biconnectivity and  -connectivity is also called triconnectivity. a graph g which is connected but not  -connected is sometimes called separable. "
78,78,Constant Function,1,https://en.wikipedia.org/wiki/Constant_function,"in mathematics, a constant function is a function whose (output) value is the same for every input value. for example, the function y(x) =   is a constant function because the value of y(x) is   regardless of the input value x (see image). as a real-valued function of a real-valued argument, a constant function has the general form y(x) = c or just y = c. the graph of the constant function y = c is a horizontal line in the plane that passes through the point ( , c). in the context of a polynomial in one variable x, the non-zero constant function is a polynomial of degree   and its general form is f(x) = c where c is nonzero. this function has no intersection point with the x-axis, that is, it has no root (zero). on the other hand, the polynomial f(x) =   is the identically zero function. it is the (trivial) constant function and every x is a root. its graph is the x-axis in the plane. a constant function is an even function, i.e. the graph of a constant function is symmetric with respect to the y-axis. in the context where it is defined, the derivative of a function is a measure of the rate of change of function values with respect to change in input values. because a constant function does not change, its derivative is  . this is often written: ( x ↦ c ) ′ =   {\displaystyle (x\mapsto c)'= } . the converse is also true. namely, if y′(x) =   for all real numbers x, then y is a constant function. for functions between preordered sets, constant functions are both order-preserving and order-reversing; conversely, if f is both order-preserving and order-reversing, and if the domain of f is a lattice, then f must be constant. a function on a connected set is locally constant if and only if it is constant. "
79,79,Constant of integration,1,https://en.wikipedia.org/wiki/Constant_of_integration,"in calculus, the constant of integration, often denoted by c {\displaystyle c} , is a constant term added to an antiderivative of a function f ( x ) {\displaystyle f(x)} to indicate that the indefinite integral of f ( x ) {\displaystyle f(x)} (i.e., the set of all antiderivatives of f ( x ) {\displaystyle f(x)} ), on a connected domain, is only defined up to an additive constant. this constant expresses an ambiguity inherent in the construction of antiderivatives. more specifically, if a function f ( x ) {\displaystyle f(x)} is defined on an interval, and f ( x ) {\displaystyle f(x)} is an antiderivative of f ( x ) {\displaystyle f(x)} , then the set of all antiderivatives of f ( x ) {\displaystyle f(x)} is given by the functions f ( x ) + c {\displaystyle f(x)+c} , where c {\displaystyle c} is an arbitrary constant (meaning that any value of c {\displaystyle c} would make f ( x ) + c {\displaystyle f(x)+c} a valid antiderivative). for that reason, the indefinite integral is often written as ∫ f ( x ) d x = f ( x ) + c {\textstyle \int f(x)\,dx=f(x)+c} , although the constant of integration might be sometimes omitted in lists of integrals for simplicity. the derivative of any constant function is zero. once one has found one antiderivative f ( x ) {\displaystyle f(x)} for a function f ( x ) {\displaystyle f(x)} , adding or subtracting any constant c {\displaystyle c} will give us another antiderivative, because d d x ( f ( x ) + c ) = d d x f ( x ) + d d x c = f ′ ( x ) = f ( x ) {\textstyle {\frac {d}{dx}}(f(x)+c)={\frac {d}{dx}}f(x)+{\frac {d}{dx}}c=f'(x)=f(x)} . the constant is a way of expressing that every function with at least one antiderivative will have an infinite number of them. let f : r → r {\displaystyle f:\mathbb {r} \to \mathbb {r} } and g : r → r {\displaystyle g:\mathbb {r} \to \mathbb {r} } be two everywhere differentiable functions. suppose that f ′ ( x ) = g ′ ( x ) {\displaystyle f\,'(x)=g\,'(x)} for every real number x. then there exists a real number c {\displaystyle c} such that f ( x ) − g ( x ) = c {\displaystyle f(x)-g(x)=c} for every real number x. to prove this, notice that [ f ( x ) − g ( x ) ] ′ =   {\displaystyle [f(x)-g(x)]'= } . so f {\displaystyle f} can be replaced by f − g {\displaystyle f-g} , and g {\displaystyle g} by the constant function   {\displaystyle  } , making the goal to prove that an everywhere differentiable function whose derivative is always zero must be constant: choose a real number a {\displaystyle a} , and let c = f ( a ) {\displaystyle c=f(a)} . for any x, the fundamental theorem of calculus, together with the assumption that the derivative of f {\displaystyle f} vanishes, implies that thereby showing that f {\displaystyle f} is a constant function. two facts are crucial in this proof. first, the real line is connected. if the real line were not connected, we would not always be able to integrate from our fixed a to any given x. for example, if we were to ask for functions defined on the union of intervals [ , ] and [ , ], and if a were  , then it would not be possible to integrate from   to  , because the function is not defined between   and  . here, there will be two constants, one for each connected component of the domain. in general, by replacing constants with locally constant functions, we can extend this theorem to disconnected domains. for example, there are two constants of integration for ∫ d x / x {\textstyle \int dx/x} , and infinitely many for ∫ tan ⁡ x d x {\textstyle \int \tan x\,dx} , so for example, the general form for the integral of  /x is: second, f {\displaystyle f} and g {\displaystyle g} were assumed to be everywhere differentiable. if f {\displaystyle f} and g {\displaystyle g} are not differentiable at even one point, then the theorem might fail. as an example, let f ( x ) {\displaystyle f(x)} be the heaviside step function, which is zero for negative values of x and one for non-negative values of x, and let g ( x ) =   {\displaystyle g(x)= } . then the derivative of f {\displaystyle f} is zero where it is defined, and the derivative of g {\displaystyle g} is always zero. yet it's clear that f {\displaystyle f} and g {\displaystyle g} do not differ by a constant, even if it is assumed that f {\displaystyle f} and g {\displaystyle g} are everywhere continuous and almost everywhere differentiable the theorem still fails. as an example, take f {\displaystyle f} to be the cantor function and again let g =   {\displaystyle g= } . for example, suppose one wants to find antiderivatives of cos ⁡ ( x ) {\displaystyle \cos(x)} . one such antiderivative is sin ⁡ ( x ) {\displaystyle \sin(x)} . another one is sin ⁡ ( x ) +   {\displaystyle \sin(x)+ } . a third is sin ⁡ ( x ) − π {\displaystyle \sin(x)-\pi } . each of these has derivative cos ⁡ ( x ) {\displaystyle \cos(x)} , so they are all antiderivatives of cos ⁡ ( x ) {\displaystyle \cos(x)} . "
80,80,Continued fraction,2,https://en.wikipedia.org/wiki/Continued_fraction,"in mathematics, a continued fraction is an expression obtained through an iterative process of representing a number as the sum of its integer part and the reciprocal of another number, then writing this other number as the sum of its integer part and another reciprocal, and so on. in a finite continued fraction (or terminated continued fraction), the iteration/recursion is terminated after finitely many steps by using an integer in lieu of another continued fraction. in contrast, an infinite continued fraction is an infinite expression. in either case, all integers in the sequence, other than the first, must be positive. the integers a i {\displaystyle a_{i}} are called the coefficients or terms of the continued fraction. it is generally assumed that the numerator of all of the fractions is  . if arbitrary values and/or functions are used in place of one or more of the numerators or the integers in the denominators, the resulting expression is a generalized continued fraction. when it is necessary to distinguish the first form from generalized continued fractions, the former may be called a simple or regular continued fraction, or said to be in canonical form. continued fractions have a number of remarkable properties related to the euclidean algorithm for integers or real numbers. every rational number p {\displaystyle p} / q {\displaystyle q} has two closely related expressions as a finite continued fraction, whose coefficients ai can be determined by applying the euclidean algorithm to ( p , q ) {\displaystyle (p,q)} . the numerical value of an infinite continued fraction is irrational; it is defined from its infinite sequence of integers as the limit of a sequence of values for finite continued fractions. each finite continued fraction of the sequence is obtained by using a finite prefix of the infinite continued fraction's defining sequence of integers. moreover, every irrational number α {\displaystyle \alpha } is the value of a unique infinite regular continued fraction, whose coefficients can be found using the non-terminating version of the euclidean algorithm applied to the incommensurable values α {\displaystyle \alpha } and  . this way of expressing real numbers (rational and irrational) is called their continued fraction representation. the term continued fraction may also refer to representations of rational functions, arising in their analytic theory. for this use of the term, see padé approximation and chebyshev rational functions. consider, for example, the rational number    /  , which is around  .    . as a first approximation, start with  , which is the integer part;    /   =   +   /  . the fractional part is the reciprocal of   /   which is about  .    . use the integer part,  , as an approximation for the reciprocal to obtain a second approximation of   +  /  =  . ;   /   =   +  /  . the remaining fractional part,  /  , is the reciprocal of   / , and   /  is around  .    . use   as an approximation for this to obtain   +  /  as an approximation for   /   and   +  /  +  / , about  .    , as the third approximation;   /  =   +  / . finally, the fractional part,  / , is the reciprocal of  , so its approximation in this scheme,  , is exact ( /  =   +  / ) and produces the exact expression   +  /  +  /  +  /  for    /  . the expression   +  /  +  /  +  /  is called the continued fraction representation of    /  . this can be represented by the abbreviated notation    /   = [ ;  ,  ,  ]. (it is customary to replace only the first comma by a semicolon.) some older textbooks use all commas in the (n +  )-tuple, for example, [ ,  ,  ,  ]. if the starting number is rational, then this process exactly parallels the euclidean algorithm applied to the numerator and denominator of the number. in particular, it must terminate and produce a finite continued fraction representation of the number. the sequence of integers that occur in this representation is the sequence of successive quotients computed by the euclidean algorithm. if the starting number is irrational, then the process continues indefinitely. this produces a sequence of approximations, all of which are rational numbers, and these converge to the starting number as a limit. this is the (infinite) continued fraction representation of the number. examples of continued fraction representations of irrational numbers are: continued fractions are, in some ways, more ""mathematically natural"" representations of a real number than other representations such as decimal representations, and they have several desirable properties: a continued fraction is an expression of the form where ai and bi can be any complex numbers. usually they are required to be integers. if bi =   for all i the expression is called a simple continued fraction. if the expression contains finitely many terms, it is called a finite continued fraction. if the expression contains infinitely many terms, it is called an infinite continued fraction. "
81,81,Continuous function,2,https://en.wikipedia.org/wiki/Continuous_function,"in mathematics, a continuous function is a function such that a continuous variation (that is a change without jump) of the argument induces a continuous variation of the value of the function. this means that there are no abrupt changes in value, known as discontinuities. more precisely, a function is continuous if arbitrarily small changes in its value can be assured by restricting to sufficiently small changes of its argument. a discontinuous function is a function that is not continuous. up until the   th century, mathematicians largely relied on intuitive notions of continuity, and considered only continuous functions. the epsilon–delta definition of a limit was introduced to formalize the definition of continuity. continuity is one of the core concepts of calculus and mathematical analysis, where arguments and values of functions are real and complex numbers. the concept has been generalized to functions between metric spaces and between topological spaces. the latter are the most general continuous functions, and their definition is the basis of topology. a stronger form of continuity is uniform continuity. in order theory, especially in domain theory, a related concept of continuity is scott continuity. as an example, the function h(t) denoting the height of a growing flower at time t would be considered continuous. in contrast, the function m(t) denoting the amount of money in a bank account at time t would be considered discontinuous, since it ""jumps"" at each point in time when money is deposited or withdrawn. a form of the epsilon–delta definition of continuity was first given by bernard bolzano in     . augustin-louis cauchy defined continuity of y = f ( x ) {\displaystyle y=f(x)} as follows: an infinitely small increment α {\displaystyle \alpha } of the independent variable x always produces an infinitely small change f ( x + α ) − f ( x ) {\displaystyle f(x+\alpha )-f(x)} of the dependent variable y (see e.g. cours d'analyse, p.   ). cauchy defined infinitely small quantities in terms of variable quantities, and his definition of continuity closely parallels the infinitesimal definition used today (see microcontinuity). the formal definition and the distinction between pointwise continuity and uniform continuity were first given by bolzano in the     s but the work wasn't published until the     s. like bolzano, karl weierstrass denied continuity of a function at a point c unless it was defined at and on both sides of c, but édouard goursat allowed the function to be defined only at and on one side of c, and camille jordan allowed it even if the function was defined only at c. all three of those nonequivalent definitions of pointwise continuity are still in use. eduard heine provided the first published definition of uniform continuity in     , but based these ideas on lectures given by peter gustav lejeune dirichlet in     . a real function, that is a function from real numbers to real numbers, can be represented by a graph in the cartesian plane; such a function is continuous if, roughly speaking, the graph is a single unbroken curve whose domain is the entire real line. a more mathematically rigorous definition is given below. continuity of real functions is usually defined in terms of limits. a function f with variable x is continuous at the real number c, if the limit of f ( x ) , {\displaystyle f(x),} as x tends to c, is equal to f ( c ) . {\displaystyle f(c).} there are several different definitions of (global) continuity of a function, which depend on the nature of its domain. a function is continuous on an open interval if the interval is contained in the domain of the function, and the function is continuous at every point of the interval. a function that is continuous on the interval ( − ∞ , + ∞ ) {\displaystyle (-\infty ,+\infty )} (the whole real line) is often called simply a continuous function; one says also that such a function is continuous everywhere. for example, all polynomial functions are continuous everywhere. a function is continuous on a semi-open or a closed interval, if the interval is contained in the domain of the function, the function is continuous at every interior point of the interval, and the value of the function at each endpoint that belongs to the interval is the limit of the values of the function when the variable tends to the endpoint from the interior of the interval. for example, the function f ( x ) = x {\displaystyle f(x)={\sqrt {x}}} is continuous on its whole domain, which is the closed interval [   , + ∞ ) . {\displaystyle [ ,+\infty ).} "
82,82,Convergent series,3,https://en.wikipedia.org/wiki/Convergent_series,"in mathematics, a series is the sum of the terms of an infinite sequence of numbers. more precisely, an infinite sequence ( a   , a   , a   , … ) {\displaystyle (a_{ },a_{ },a_{ },\ldots )} defines a series s that is denoted the nth partial sum sn is the sum of the first n terms of the sequence; that is, a series is convergent (or converges) if the sequence ( s   , s   , s   , … ) {\displaystyle (s_{ },s_{ },s_{ },\dots )} of its partial sums tends to a limit; that means that, when adding one a k {\displaystyle a_{k}} after the other in the order given by the indices, one gets partial sums that become closer and closer to a given number. more precisely, a series converges, if there exists a number ℓ {\displaystyle \ell } such that for every arbitrarily small positive number ε {\displaystyle \varepsilon } , there is a (sufficiently large) integer n {\displaystyle n} such that for all n ≥ n {\displaystyle n\geq n} , if the series is convergent, the (necessarily unique) number ℓ {\displaystyle \ell } is called the sum of the series. the same notation is used for the series, and, if it is convergent, to its sum. this convention is similar to that which is used for addition: a + b denotes the operation of adding a and b as well as the result of this addition, which is called the sum of a and b. any series that is not convergent is said to be divergent or to diverge. there are a number of methods of determining whether a series converges or diverges. comparison test. the terms of the sequence { a n } {\displaystyle \left\{a_{n}\right\}} are compared to those of another sequence { b n } {\displaystyle \left\{b_{n}\right\}} . if, for all n,   ≤ a n ≤ b n {\displaystyle  \leq \ a_{n}\leq \ b_{n}} , and ∑ n =   ∞ b n {\textstyle \sum _{n= }^{\infty }b_{n}} converges, then so does ∑ n =   ∞ a n . {\textstyle \sum _{n= }^{\infty }a_{n}.} however, if, for all n,   ≤ b n ≤ a n {\displaystyle  \leq \ b_{n}\leq \ a_{n}} , and ∑ n =   ∞ b n {\textstyle \sum _{n= }^{\infty }b_{n}} diverges, then so does ∑ n =   ∞ a n . {\textstyle \sum _{n= }^{\infty }a_{n}.} "
83,83,convex function,2,https://en.wikipedia.org/wiki/Convex_function,"in mathematics, a real-valued function is called convex if the line segment between any two points on the graph of the function does not lie below the graph between the two points. equivalently, a function is convex if its epigraph (the set of points on or above the graph of the function) is a convex set. a twice-differentiable function of a single variable is convex if and only if its second derivative is nonnegative on its entire domain. well-known examples of convex functions of a single variable include the quadratic function x   {\displaystyle x^{ }} and the exponential function e x {\displaystyle e^{x}} . in simple terms, a convex function refers to a function whose graph is shaped like a cup ∪ {\displaystyle \cup } , while a concave function's graph is shaped like a cap ∩ {\displaystyle \cap } . convex functions play an important role in many areas of mathematics. they are especially important in the study of optimization problems where they are distinguished by a number of convenient properties. for instance, a strictly convex function on an open set has no more than one minimum. even in infinite-dimensional spaces, under suitable additional hypotheses, convex functions continue to satisfy such properties and as a result, they are the most well-understood functionals in the calculus of variations. in probability theory, a convex function applied to the expected value of a random variable is always bounded above by the expected value of the convex function of the random variable. this result, known as jensen's inequality, can be used to deduce inequalities such as the arithmetic–geometric mean inequality and hölder's inequality. let x {\displaystyle x} be a convex subset of a real vector space and let f : x → r {\displaystyle f:x\to \mathbb {r} } be a function. then f {\displaystyle f} is called convex if and only if any of the following equivalent conditions hold: the second statement characterizing convex functions that are valued in the real line r {\displaystyle \mathbb {r} } is also the statement used to define convex functions that are valued in the extended real number line [ − ∞ , ∞ ] = r ∪ { ± ∞ } , {\displaystyle [-\infty ,\infty ]=\mathbb {r} \cup \{\pm \infty \},} where such a function f {\displaystyle f} is allowed to (but is not required to) take ± ∞ {\displaystyle \pm \infty } as a value. the first statement is not used because it permits t {\displaystyle t} to take   {\displaystyle  } or   {\displaystyle  } as a value, in which case, if f ( x   ) = ± ∞ {\displaystyle f\left(x_{ }\right)=\pm \infty } or f ( x   ) = ± ∞ , {\displaystyle f\left(x_{ }\right)=\pm \infty ,} respectively, then t f ( x   ) + (   − t ) f ( x   ) {\displaystyle tf\left(x_{ }\right)+( -t)f\left(x_{ }\right)} would be undefined (because the multiplications   ⋅ ∞ {\displaystyle  \cdot \infty } and   ⋅ ( − ∞ ) {\displaystyle  \cdot (-\infty )} are undefined). the sum − ∞ + ∞ {\displaystyle -\infty +\infty } is also undefined so a convex extended real-valued function is typically only allowed to take exactly one of − ∞ {\displaystyle -\infty } and + ∞ {\displaystyle +\infty } as a value. the second statement can also be modified to get the definition of strict convexity, where the latter is obtained by replacing ≤ {\displaystyle \,\leq \,} with the strict inequality < . {\displaystyle \,<.} explicitly, the map f {\displaystyle f} is called strictly convex if and only if for all real   < t <   {\displaystyle  <t< } and all x   , x   ∈ x {\displaystyle x_{ },x_{ }\in x} such that x   ≠ x   {\displaystyle x_{ }\neq x_{ }} : a strictly convex function f {\displaystyle f} is a function that the straight line between any pair of points on the curve f {\displaystyle f} is above the curve f {\displaystyle f} except for the intersection points between the straight line and the curve. the function f {\displaystyle f} is said to be concave (resp. strictly concave) if − f {\displaystyle -f} ( f {\displaystyle f} multiplied by - ) is convex (resp. strictly convex ). the term convex is often referred to as convex down or concave upward, and the term concave is often referred as concave down or convex upward. if the term ""convex"" is used without an ""up"" or ""down"" keyword, then it refers strictly to a cup shaped graph ∪ {\displaystyle \cup } . as an example, jensen's inequality refers to an inequality involving a convex or convex-(up), function. many properties of convex functions have the same simple formulation for functions of many variable as for functions of one variable. see below the properties for the case of many variables, as some of them are not listed for functions of one variable. "
84,84,Convex_optimization,0,https://en.wikipedia.org/wiki/Convex_optimization,"convex optimization is a subfield of mathematical optimization that studies the problem of minimizing convex functions over convex sets. many classes of convex optimization problems admit polynomial-time algorithms, whereas mathematical optimization is in general np-hard. convex optimization has applications in a wide range of disciplines, such as automatic control systems, estimation and signal processing, communications and networks, electronic circuit design, data analysis and modeling, finance, statistics (optimal experimental design), and structural optimization, where the approximation concept has proven to be efficient. with recent advancements in computing and optimization algorithms, convex programming is nearly as straightforward as linear programming. a convex optimization problem is an optimization problem in which the objective function is a convex function and the feasible set is a convex set. a function f {\displaystyle f} mapping some subset of r n {\displaystyle \mathbb {r} ^{n}} into r ∪ { ± ∞ } {\displaystyle \mathbb {r} \cup \{\pm \infty \}} is convex if its domain is convex and for all θ ∈ [   ,   ] {\displaystyle \theta \in [ , ]} and all x , y {\displaystyle x,y} in its domain, the following condition holds: f ( θ x + (   − θ ) y ) ≤ θ f ( x ) + (   − θ ) f ( y ) {\displaystyle f(\theta x+( -\theta )y)\leq \theta f(x)+( -\theta )f(y)} . a set s is convex if for all members x , y ∈ s {\displaystyle x,y\in s} and all θ ∈ [   ,   ] {\displaystyle \theta \in [ , ]} , we have that θ x + (   − θ ) y ∈ s {\displaystyle \theta x+( -\theta )y\in s} . concretely, a convex optimization problem is the problem of finding some x ∗ ∈ c {\displaystyle \mathbf {x^{\ast }} \in c} attaining where the objective function f : d ⊆ r n → r {\displaystyle f:{\mathcal {d}}\subseteq \mathbb {r} ^{n}\to \mathbb {r} } is convex, as is the feasible set c {\displaystyle c} . if such a point exists, it is referred to as an optimal point or solution; the set of all optimal points is called the optimal set. if f {\displaystyle f} is unbounded below over c {\displaystyle c} or the infimum is not attained, then the optimization problem is said to be unbounded. otherwise, if c {\displaystyle c} is the empty set, then the problem is said to be infeasible. a convex optimization problem is in standard form if it is written as where x ∈ r n {\displaystyle \mathbf {x} \in \mathbb {r} ^{n}} is the optimization variable, the function f : d ⊆ r n → r {\displaystyle f:{\mathcal {d}}\subseteq \mathbb {r} ^{n}\to \mathbb {r} } is convex, g i : r n → r {\displaystyle g_{i}:\mathbb {r} ^{n}\to \mathbb {r} } , i =   , … , m {\displaystyle i= ,\ldots ,m} , are convex, and h i : r n → r {\displaystyle h_{i}:\mathbb {r} ^{n}\to \mathbb {r} } , i =   , … , p {\displaystyle i= ,\ldots ,p} , are affine. this notation describes the problem of finding x ∈ r n {\displaystyle \mathbf {x} \in \mathbb {r} ^{n}} that minimizes f ( x ) {\displaystyle f(\mathbf {x} )} among all x {\displaystyle \mathbf {x} } satisfying g i ( x ) ≤   {\displaystyle g_{i}(\mathbf {x} )\leq  } , i =   , … , m {\displaystyle i= ,\ldots ,m} and h i ( x ) =   {\displaystyle h_{i}(\mathbf {x} )= } , i =   , … , p {\displaystyle i= ,\ldots ,p} . the function f {\displaystyle f} is the objective function of the problem, and the functions g i {\displaystyle g_{i}} and h i {\displaystyle h_{i}} are the constraint functions. the feasible set c {\displaystyle c} of the optimization problem consists of all points x ∈ d {\displaystyle \mathbf {x} \in {\mathcal {d}}} satisfying the constraints. this set is convex because d {\displaystyle {\mathcal {d}}} is convex, the sublevel sets of convex functions are convex, affine sets are convex, and the intersection of convex sets is convex. a solution to a convex optimization problem is any point x ∈ c {\displaystyle \mathbf {x} \in c} attaining inf { f ( x ) : x ∈ c } {\displaystyle \inf\{f(\mathbf {x} ):\mathbf {x} \in c\}} . in general, a convex optimization problem may have zero, one, or many solutions.[citation needed] many optimization problems can be equivalently formulated in this standard form. for example, the problem of maximizing a concave function f {\displaystyle f} can be re-formulated equivalently as the problem of minimizing the convex function − f {\displaystyle -f} . the problem of maximizing a concave function over a convex set is commonly called a convex optimization problem.[citation needed] "
85,85,Convolutions,2,https://en.wikipedia.org/wiki/Convolution,"in mathematics (in particular, functional analysis), convolution is a mathematical operation on two functions (f and g) that produces a third function ( f ∗ g {\displaystyle f*g} ) that expresses how the shape of one is modified by the other. the term convolution refers to both the result function and to the process of computing it. it is defined as the integral of the product of the two functions after one is reversed and shifted. the integral is evaluated for all values of shift, producing the convolution function. some features of convolution are similar to cross-correlation: for real-valued functions, of a continuous or discrete variable, it differs from cross-correlation ( f ⋆ g {\displaystyle f\star g} ) only in that either f(x) or g(x) is reflected about the y-axis; thus it is a cross-correlation of f(x) and g(−x), or f(−x) and g(x).[a] for complex-valued functions, the cross-correlation operator is the adjoint of the convolution operator. convolution has applications that include probability, statistics, acoustics, spectroscopy, signal processing and image processing, geophysics, engineering, physics, computer vision and differential equations. the convolution can be defined for functions on euclidean space and other groups.[citation needed] for example, periodic functions, such as the discrete-time fourier transform, can be defined on a circle and convolved by periodic convolution. (see row    at dtft § properties.) a discrete convolution can be defined for functions on the set of integers. generalizations of convolution have applications in the field of numerical analysis and numerical linear algebra, and in the design and implementation of finite impulse response filters in signal processing.[citation needed] computing the inverse of the convolution operation is known as deconvolution. the convolution of f and g is written f∗g, denoting the operator with the symbol ∗.[b] it is defined as the integral of the product of the two functions after one is reversed and shifted. as such, it is a particular kind of integral transform: an equivalent definition is (see commutativity): while the symbol t is used above, it need not represent the time domain. but in that context, the convolution formula can be described as the area under the function f(τ) weighted by the function g(−τ) shifted by amount t. as t changes, the weighting function g(t − τ) emphasizes different parts of the input function f(τ). for functions f, g supported on only [ , ∞) (i.e., zero for negative arguments), the integration limits can be truncated, resulting in: "
86,86,Coordinate Geometry,1,https://en.wikipedia.org/wiki/Coordinate_system,"in geometry, a coordinate system is a system that uses one or more numbers, or coordinates, to uniquely determine the position of the points or other geometric elements on a manifold such as euclidean space. the order of the coordinates is significant, and they are sometimes identified by their position in an ordered tuple and sometimes by a letter, as in ""the x-coordinate"". the coordinates are taken to be real numbers in elementary mathematics, but may be complex numbers or elements of a more abstract system such as a commutative ring. the use of a coordinate system allows problems in geometry to be translated into problems about numbers and vice versa; this is the basis of analytic geometry. the simplest example of a coordinate system is the identification of points on a line with real numbers using the number line. in this system, an arbitrary point o (the origin) is chosen on a given line. the coordinate of a point p is defined as the signed distance from o to p, where the signed distance is the distance taken as positive or negative depending on which side of the line p lies. each point is given a unique coordinate and each real number is the coordinate of a unique point. the prototypical example of a coordinate system is the cartesian coordinate system. in the plane, two perpendicular lines are chosen and the coordinates of a point are taken to be the signed distances to the lines. in three dimensions, three mutually orthogonal planes are chosen and the three coordinates of a point are the signed distances to each of the planes. this can be generalized to create n coordinates for any point in n-dimensional euclidean space. depending on the direction and order of the coordinate axes, the three-dimensional system may be a right-handed or a left-handed system. this is one of many coordinate systems. another common coordinate system for the plane is the polar coordinate system. a point is chosen as the pole and a ray from this point is taken as the polar axis. for a given angle θ, there is a single line through the pole whose angle with the polar axis is θ (measured counterclockwise from the axis to the line). then there is a unique point on this line whose signed distance from the origin is r for given number r. for a given pair of coordinates (r, θ) there is a single point, but any point is represented by many pairs of coordinates. for example, (r, θ), (r, θ+ π) and (−r, θ+π) are all polar coordinates for the same point. the pole is represented by ( , θ) for any value of θ. there are two common methods for extending the polar coordinate system to three dimensions. in the cylindrical coordinate system, a z-coordinate with the same meaning as in cartesian coordinates is added to the r and θ polar coordinates giving a triple (r, θ, z). spherical coordinates take this a step further by converting the pair of cylindrical coordinates (r, z) to polar coordinates (ρ, φ) giving a triple (ρ, θ, φ). a point in the plane may be represented in homogeneous coordinates by a triple (x, y, z) where x/z and y/z are the cartesian coordinates of the point. this introduces an ""extra"" coordinate since only two are needed to specify a point on the plane, but this system is useful in that it represents any point on the projective plane without the use of infinity. in general, a homogeneous coordinate system is one where only the ratios of the coordinates are significant and not the actual values. some other common coordinate systems are the following: there are ways of describing curves without coordinates, using intrinsic equations that use invariant quantities such as curvature and arc length. these include: "
87,87,Coplanarity,2,https://en.wikipedia.org/wiki/Coplanarity,"in geometry, a set of points in space are coplanar if there exists a geometric plane that contains them all. for example, three points are always coplanar, and if the points are distinct and non-collinear, the plane they determine is unique. however, a set of four or more distinct points will, in general, not lie in a single plane. two lines in three-dimensional space are coplanar if there is a plane that includes them both. this occurs if the lines are parallel, or if they intersect each other. two lines that are not coplanar are called skew lines. distance geometry provides a solution technique for the problem of determining whether a set of points is coplanar, knowing only the distances between them. in three-dimensional space, two linearly independent vectors with the same initial point determine a plane through that point. their cross product is a normal vector to that plane, and any vector orthogonal to this cross product through the initial point will lie in the plane. this leads to the following coplanarity test using a scalar triple product: four distinct points, x , x , x  and x  are coplanar if and only if, which is also equivalent to if three vectors a, b and c are coplanar, then if a⋅b =   (i.e., a and b are orthogonal) then where a ^ {\displaystyle \mathbf {\hat {a}} } denotes the unit vector in the direction of a. that is, the vector projections of c on a and c on b add to give the original c. since three or fewer points are always coplanar, the problem of determining when a set of points are coplanar is generally of interest only when there are at least four points involved. in the case that there are exactly four points, several ad hoc methods can be employed, but a general method that works for any number of points uses vector methods and the property that a plane is determined by two linearly independent vectors. in an n-dimensional space (n ≥  ), a set of k points, {p , p , ..., pk −  } are coplanar if and only if the matrix of their relative differences, that is, the matrix whose columns (or rows) are the vectors p   p   → , p   p   → , … , p   p k −   → {\displaystyle {\overrightarrow {p_{ }p_{ }}},{\overrightarrow {p_{ }p_{ }}},\ldots ,{\overrightarrow {p_{ }p_{k- }}}} is of rank   or less. "
88,88,Covariance matrix,0,https://en.wikipedia.org/wiki/Covariance_matrix," in probability theory and statistics, a covariance matrix (also known as auto-covariance matrix, dispersion matrix, variance matrix, or variance–covariance matrix) is a square matrix giving the covariance between each pair of elements of a given random vector. any covariance matrix is symmetric and positive semi-definite and its main diagonal contains variances (i.e., the covariance of each element with itself). intuitively, the covariance matrix generalizes the notion of variance to multiple dimensions. as an example, the variation in a collection of random points in two-dimensional space cannot be characterized fully by a single number, nor would the variances in the x {\displaystyle x} and y {\displaystyle y} directions contain all of the necessary information; a   ×   {\displaystyle  \times  } matrix would be necessary to fully characterize the two-dimensional variation. the covariance matrix of a random vector x {\displaystyle \mathbf {x} } is typically denoted by k x x {\displaystyle \operatorname {k} _{\mathbf {x} \mathbf {x} }} or σ {\displaystyle \sigma } . throughout this article, boldfaced unsubscripted x {\displaystyle \mathbf {x} } and y {\displaystyle \mathbf {y} } are used to refer to random vectors, and unboldfaced subscripted x i {\displaystyle x_{i}} and y i {\displaystyle y_{i}} are used to refer to scalar random variables. if the entries in the column vector are random variables, each with finite variance and expected value, then the covariance matrix k x x {\displaystyle \operatorname {k} _{\mathbf {x} \mathbf {x} }} is the matrix whose ( i , j ) {\displaystyle (i,j)} entry is the covariance : p.     where the operator e {\displaystyle \operatorname {e} } denotes the expected value (mean) of its argument. nomenclatures differ. some statisticians, following the probabilist william feller in his two-volume book an introduction to probability theory and its applications, call the matrix k x x {\displaystyle \operatorname {k} _{\mathbf {x} \mathbf {x} }} the variance of the random vector x {\displaystyle \mathbf {x} } , because it is the natural generalization to higher dimensions of the  -dimensional variance. others call it the covariance matrix, because it is the matrix of covariances between the scalar components of the vector x {\displaystyle \mathbf {x} } . both forms are quite standard, and there is no ambiguity between them. the matrix k x x {\displaystyle \operatorname {k} _{\mathbf {x} \mathbf {x} }} is also often called the variance-covariance matrix, since the diagonal terms are in fact variances. "
89,89,Cramer's rule,2,https://en.wikipedia.org/wiki/Cramer%27s_rule," in linear algebra, cramer's rule is an explicit formula for the solution of a system of linear equations with as many equations as unknowns, valid whenever the system has a unique solution. it expresses the solution in terms of the determinants of the (square) coefficient matrix and of matrices obtained from it by replacing one column by the column vector of right-hand-sides of the equations. it is named after gabriel cramer (    –    ), who published the rule for an arbitrary number of unknowns in     , although colin maclaurin also published special cases of the rule in      (and possibly knew of it as early as     ). cramer's rule implemented in a naïve way is computationally inefficient for systems of more than two or three equations. in the case of n equations in n unknowns, it requires computation of n +   determinants, while gaussian elimination produces the result with the same computational complexity as the computation of a single determinant. [verification needed] cramer's rule can also be numerically unstable even for  ×  systems. however, it has recently been shown that cramer's rule can be implemented in o(n ) time, which is comparable to more common methods of solving systems of linear equations, such as gaussian elimination (consistently requiring  .  times as many arithmetic operations for all matrix sizes), while exhibiting comparable numeric stability in most cases. consider a system of n linear equations for n unknowns, represented in matrix multiplication form as follows: where the n × n matrix a has a nonzero determinant, and the vector x = ( x   , … , x n ) t {\displaystyle \mathbf {x} =(x_{ },\ldots ,x_{n})^{\mathsf {t}}} is the column vector of the variables. then the theorem states that in this case the system has a unique solution, whose individual values for the unknowns are given by: where a i {\displaystyle a_{i}} is the matrix formed by replacing the i-th column of a by the column vector b. a more general version of cramer's rule considers the matrix equation where the n × n matrix a has a nonzero determinant, and x, b are n × m matrices. given sequences   ≤ i   < i   < ⋯ < i k ≤ n {\displaystyle  \leq i_{ }<i_{ }<\cdots <i_{k}\leq n} and   ≤ j   < j   < ⋯ < j k ≤ m {\displaystyle  \leq j_{ }<j_{ }<\cdots <j_{k}\leq m} , let x i , j {\displaystyle x_{i,j}} be the k × k submatrix of x with rows in i := ( i   , … , i k ) {\displaystyle i:=(i_{ },\ldots ,i_{k})} and columns in j := ( j   , … , j k ) {\displaystyle j:=(j_{ },\ldots ,j_{k})} . let a b ( i , j ) {\displaystyle a_{b}(i,j)} be the n × n matrix formed by replacing the i s {\displaystyle i_{s}} column of a by the j s {\displaystyle j_{s}} column of b, for all s =   , … , k {\displaystyle s= ,\ldots ,k} . then in the case k =   {\displaystyle k= } , this reduces to the normal cramer's rule. the rule holds for systems of equations with coefficients and unknowns in any field, not just in the real numbers. "
90,90,Cramer's theorem(algebraic curves),0,https://en.wikipedia.org/wiki/Cramer%27s_theorem_(algebraic_curves),"in mathematics, cramer's theorem on algebraic curves gives the necessary and sufficient number of points in the real plane falling on an algebraic curve to uniquely determine the curve in non-degenerate cases. this number is where n is the degree of the curve. the theorem is due to gabriel cramer, who published it in     . for example, a line (of degree  ) is determined by   distinct points on it: one and only one line goes through those two points. likewise, a non-degenerate conic (polynomial equation in x and y with the sum of their powers in any term not exceeding  , hence with degree  ) is uniquely determined by   points in general position (no three of which are on a straight line). the intuition of the conic case is this: suppose the given points fall on, specifically, an ellipse. then five pieces of information are necessary and sufficient to identify the ellipse—the horizontal location of the ellipse's center, the vertical location of the center, the major axis (the length of the longest chord), the minor axis (the length of the shortest chord through the center, perpendicular to the major axis), and the ellipse's rotational orientation (the extent to which the major axis departs from the horizontal). five points in general position suffice to provide these five pieces of information, while four points do not. the number of distinct terms (including those with a zero coefficient) in an n-th degree equation in two variables is (n +  )(n +  ) /  . this is because the n-th degree terms are x n , x n −   y   , … , y n , {\displaystyle x^{n},\,x^{n- }y^{ },\,\dots ,\,y^{n},} numbering n +   in total; the (n −  ) degree terms are x n −   , x n −   y   , … , y n −   , {\displaystyle x^{n- },\,x^{n- }y^{ },\,\dots ,\,y^{n- },} numbering n in total; and so on through the first degree terms x {\displaystyle x} and y , {\displaystyle y,} numbering   in total, and the single zero degree term (the constant). the sum of these is (n +  ) + n + (n –  ) + ... +   +   = (n +  )(n +  ) /   terms, each with its own coefficient. however, one of these coefficients is redundant in determining the curve, because we can always divide through the polynomial equation by any one of the coefficients, giving an equivalent equation with one coefficient fixed at  , and thus [(n +  )(n +  ) /  ] −   = n(n +  ) /   remaining coefficients. for example, a fourth degree equation has the general form with  ( + )/  =    coefficients. determining an algebraic curve through a set of points consists of determining values for these coefficients in the algebraic equation such that each of the points satisfies the equation. given n(n +  ) /   points (xi, yi), each of these points can be used to create a separate equation by substituting it into the general polynomial equation of degree n, giving n(n +  ) /   equations linear in the n(n +  ) /   unknown coefficients. if this system is non-degenerate in the sense of having a non-zero determinant, the unknown coefficients are uniquely determined and hence the polynomial equation and its curve are uniquely determined. more than this number of points would be redundant, and fewer would be insufficient to solve the system of equations uniquely for the coefficients. an example of a degenerate case, in which n(n +  ) /   points on the curve are not sufficient to determine the curve uniquely, was provided by cramer as part of cramer's paradox. let the degree be n =  , and let nine points be all combinations of x = – ,  ,   and y = – ,  ,  . more than one cubic contains all of these points, namely all cubics of equation a ( x   − x ) + b ( y   − y ) =  . {\displaystyle a(x^{ }-x)+b(y^{ }-y)= .} thus these points do not determine a unique cubic, even though there are n(n +  ) /   =   of them. more generally, there are infinitely many cubics that pass through the nine intersection points of two cubics (bézout's theorem implies that two cubics have, in general, nine intersection points) likewise, for the conic case of n =  , if three of five given points all fall on the same straight line, they may not uniquely determine the curve. "
91,91,Cross Product,1,https://en.wikipedia.org/wiki/Cross_product,"in mathematics, the cross product or vector product (occasionally directed area product, to emphasize its geometric significance) is a binary operation on two vectors in a three-dimensional oriented euclidean vector space (named here e {\displaystyle e} ), and is denoted by the symbol × {\displaystyle \times } . given two linearly independent vectors a and b, the cross product, a × b (read ""a cross b""), is a vector that is perpendicular to both a and b, and thus normal to the plane containing them. it has many applications in mathematics, physics, engineering, and computer programming. it should not be confused with the dot product (projection product). if two vectors have the same direction or have the exact opposite direction from each other (that is, they are not linearly independent), or if either one has zero length, then their cross product is zero. more generally, the magnitude of the product equals the area of a parallelogram with the vectors for sides; in particular, the magnitude of the product of two perpendicular vectors is the product of their lengths. the cross product is anticommutative (that is, a × b = − b × a) and is distributive over addition (that is, a × (b + c) = a × b + a × c). the space e {\displaystyle e} together with the cross product is an algebra over the real numbers, which is neither commutative nor associative, but is a lie algebra with the cross product being the lie bracket. like the dot product, it depends on the metric of euclidean space, but unlike the dot product, it also depends on a choice of orientation (or ""handedness"") of the space (it's why an oriented space is needed). in connection with the cross product, the exterior product of vectors can be used in arbitrary dimensions (with a bivector or  -form result) and is independent of the orientation of the space. the product can be generalized in various ways, using the orientation and metric structure just as for the traditional  -dimensional cross product, one can, in n dimensions, take the product of n −   vectors to produce a vector perpendicular to all of them. but if the product is limited to non-trivial binary products with vector results, it exists only in three and seven dimensions. (see § generalizations, below, for other dimensions.) the cross product of two vectors a and b is defined only in three-dimensional space and is denoted by a × b. in physics and applied mathematics, the wedge notation a ∧ b is often used (in conjunction with the name vector product), although in pure mathematics such notation is usually reserved for just the exterior product, an abstraction of the vector product to n dimensions. the cross product a × b is defined as a vector c that is perpendicular (orthogonal) to both a and b, with a direction given by the right-hand rule and a magnitude equal to the area of the parallelogram that the vectors span. the cross product is defined by the formula where: if the vectors a and b are parallel (that is, the angle θ between them is either  ° or    °), by the above formula, the cross product of a and b is the zero vector  . "
92,92,Cube (algebra),1,https://en.wikipedia.org/wiki/Cube_(algebra)," in arithmetic and algebra, the cube of a number n is its third power, that is, the result of multiplying three instances of n together. the cube of a number or any other mathematical expression is denoted by a superscript  , for example    =   or (x +  ) . the cube is also the number multiplied by its square: the cube function is the function x ↦ x  (often denoted y = x ) that maps a number to its cube. it is an odd function, as the volume of a geometric cube is the cube of its side length, giving rise to the name. the inverse operation that consists of finding a number whose cube is n is called extracting the cube root of n. it determines the side of the cube of a given volume. it is also n raised to the one-third power. the graph of the cube function is known as the cubic parabola. because the cube function is an odd function, this curve has a center of symmetry at the origin, but no axis of symmetry. a cube number, or a perfect cube, or sometimes just a cube, is a number which is the cube of an integer. the non-negative perfect cubes up to     are (sequence a       in the oeis): geometrically speaking, a positive integer m is a perfect cube if and only if one can arrange m solid unit cubes into a larger, solid cube. for example,    small cubes can be arranged into one larger one with the appearance of a rubik's cube, since   ×   ×   =   . the difference between the cubes of consecutive integers can be expressed as follows: or "
93,93,Cube Roots,2,https://en.wikipedia.org/wiki/Cube_root," in mathematics, a cube root of a number x is a number y such that y  = x. all nonzero real numbers, have exactly one real cube root and a pair of complex conjugate cube roots, and all nonzero complex numbers have three distinct complex cube roots. for example, the real cube root of  , denoted     {\displaystyle {\sqrt[{ }]{ }}} , is  , because    =  , while the other cube roots of   are −   + i   {\displaystyle - +i{\sqrt { }}} and −   − i   {\displaystyle - -i{\sqrt { }}} . the three cube roots of −  i are in some contexts, particularly when the number whose cube root is to be taken is a real number, one of the cube roots (in this particular case the real one) is referred to as the principal cube root, denoted with the radical sign   . {\displaystyle {\sqrt[{ }]{~^{~}}}.} the cube root is the inverse function of the cube function if considering only real numbers, but not if considering also complex numbers: although one has always ( x   )   = x , {\displaystyle \left({\sqrt[{ }]{x}}\right)^{ }=x,} the cube root of the cube of a number is not always this number. for example, −   + i   {\displaystyle - +i{\sqrt { }}} is a cube root of  , (that is, ( −   + i   )   =   {\displaystyle (- +i{\sqrt { }})^{ }= } ), but −   + i   ≠   = ( −   + i   )     . {\displaystyle - +i{\sqrt { }}\neq  ={\sqrt[{ }]{(- +i{\sqrt { }})^{ }}}.} the cube roots of a number x are the numbers y which satisfy the equation for any real number x, there is one real number y such that y  = x. the cube function is increasing, so does not give the same result for two different inputs, and it covers all real numbers. in other words, it is a bijection, or one-to-one. then we can define an inverse function that is also one-to-one. for real numbers, we can define a unique cube root of all real numbers. if this definition is used, the cube root of a negative number is a negative number. if x and y are allowed to be complex, then there are three solutions (if x is non-zero) and so x has three cube roots. a real number has one real cube root and two further cube roots which form a complex conjugate pair. for instance, the cube roots of   are: the last two of these roots lead to a relationship between all roots of any real or complex number. if a number is one cube root of a particular real or complex number, the other two cube roots can be found by multiplying that cube root by one or the other of the two complex cube roots of  . for complex numbers, the principal cube root is usually defined as the cube root that has the greatest real part, or, equivalently, the cube root whose argument has the least absolute value. it is related to the principal value of the natural logarithm by the formula if we write x as where r is a non-negative real number and θ lies in the range "
94,94,Cubic Equation,2,https://en.wikipedia.org/wiki/Cubic_equation,"in algebra, a cubic equation in one variable is an equation of the form in which a is nonzero. the solutions of this equation are called roots of the cubic function defined by the left-hand side of the equation. if all of the coefficients a, b, c, and d of the cubic equation are real numbers, then it has at least one real root (this is true for all odd-degree polynomial functions). all of the roots of the cubic equation can be found by the following means: the coefficients do not need to be real numbers. much of what is covered below is valid for coefficients in any field with characteristic other than   and  . the solutions of the cubic equation do not necessarily belong to the same field as the coefficients. for example, some cubic equations with rational coefficients have roots that are irrational (and even non-real) complex numbers. cubic equations were known to the ancient babylonians, greeks, chinese, indians, and egyptians. babylonian (  th to   th centuries bc) cuneiform tablets have been found with tables for calculating cubes and cube roots. the babylonians could have used the tables to solve cubic equations, but no evidence exists to confirm that they did. the problem of doubling the cube involves the simplest and oldest studied cubic equation, and one for which the ancient egyptians did not believe a solution existed. in the  th century bc, hippocrates reduced this problem to that of finding two mean proportionals between one line and another of twice its length, but could not solve this with a compass and straightedge construction, a task which is now known to be impossible. methods for solving cubic equations appear in the nine chapters on the mathematical art, a chinese mathematical text compiled around the  nd century bc and commented on by liu hui in the  rd century. in the  rd century ad, the greek mathematician diophantus found integer or rational solutions for some bivariate cubic equations (diophantine equations). hippocrates, menaechmus and archimedes are believed to have come close to solving the problem of doubling the cube using intersecting conic sections, though historians such as reviel netz dispute whether the greeks were thinking about cubic equations or just problems that can lead to cubic equations. some others like t. l. heath, who translated all of archimedes' works, disagree, putting forward evidence that archimedes really solved cubic equations using intersections of two conics, but also discussed the conditions where the roots are  ,   or  . in the  th century, the tang dynasty astronomer mathematician wang xiaotong in his mathematical treatise titled jigu suanjing systematically established and solved numerically    cubic equations of the form x  + px  + qx = n,    of them with p, q ≠  , and two of them with q =  . in the   th century, the persian poet-mathematician, omar khayyam (    –    ), made significant progress in the theory of cubic equations. in an early paper, he discovered that a cubic equation can have more than one solution and stated that it cannot be solved using compass and straightedge constructions. he also found a geometric solution. in his later work, the treatise on demonstration of problems of algebra, he wrote a complete classification of cubic equations with general geometric solutions found by means of intersecting conic sections. in the   th century, the indian mathematician bhaskara ii attempted the solution of cubic equations without general success. however, he gave one example of a cubic equation: x  +   x =  x  +   . in the   th century, another persian mathematician, sharaf al-dīn al-tūsī (    –    ), wrote the al-muʿādalāt (treatise on equations), which dealt with eight types of cubic equations with positive solutions and five types of cubic equations which may not have positive solutions. he used what would later be known as the ""ruffini-horner method"" to numerically approximate the root of a cubic equation. he also used the concepts of maxima and minima of curves in order to solve cubic equations which may not have positive solutions. he understood the importance of the discriminant of the cubic equation to find algebraic solutions to certain types of cubic equations. in his book flos, leonardo de pisa, also known as fibonacci (    –    ), was able to closely approximate the positive solution to the cubic equation x  +  x  +   x =   . writing in babylonian numerals he gave the result as  ,  , ,  ,  , ,   (equivalent to   +   /   +  /    +   /    +   /    +  /    +   /   ), which has a relative error of about   − . in the early   th century, the italian mathematician scipione del ferro (    –    ) found a method for solving a class of cubic equations, namely those of the form x  + mx = n. in fact, all cubic equations can be reduced to this form if one allows m and n to be negative, but negative numbers were not known to him at that time. del ferro kept his achievement secret until just before his death, when he told his student antonio fior about it. "
95,95,Cumulative Distribution Frequency,2,https://en.wikipedia.org/wiki/Cumulative_distribution_function,"in probability theory and statistics, the cumulative distribution function (cdf) of a real-valued random variable x {\displaystyle x} , or just distribution function of x {\displaystyle x} , evaluated at x {\displaystyle x} , is the probability that x {\displaystyle x} will take a value less than or equal to x {\displaystyle x} . every probability distribution supported on the real numbers, discrete or ""mixed"" as well as continuous, is uniquely identified by an upwards continuous monotonic increasing cumulative distribution function f : r → [   ,   ] {\displaystyle f:\mathbb {r} \rightarrow [ , ]} satisfying lim x → − ∞ f ( x ) =   {\displaystyle \lim _{x\rightarrow -\infty }f(x)= } and lim x → ∞ f ( x ) =   {\displaystyle \lim _{x\rightarrow \infty }f(x)= } . in the case of a scalar continuous distribution, it gives the area under the probability density function from minus infinity to x {\displaystyle x} . cumulative distribution functions are also used to specify the distribution of multivariate random variables. the cumulative distribution function of a real-valued random variable x {\displaystyle x} is the function given by : p.    (eq. )where the right-hand side represents the probability that the random variable x {\displaystyle x} takes on a value less than or equal to x {\displaystyle x} . the probability that x {\displaystyle x} lies in the semi-closed interval ( a , b ] {\displaystyle (a,b]} , where a < b {\displaystyle a<b} , is therefore : p.    "
96,96,curve,2,https://en.wikipedia.org/wiki/Curve,"in mathematics, a curve (also called a curved line in older texts) is an object similar to a line, but that does not have to be straight. intuitively, a curve may be thought of as the trace left by a moving point. this is the definition that appeared more than      years ago in euclid's elements: ""the [curved] line[a] is […] the first species of quantity, which has only one dimension, namely length, without any width nor depth, and is nothing else than the flow or run of the point which […] will leave from its imaginary moving some vestige in length, exempt of any width."" this definition of a curve has been formalized in modern mathematics as: a curve is the image of an interval to a topological space by a continuous function. in some contexts, the function that defines the curve is called a parametrization, and the curve is a parametric curve. in this article, these curves are sometimes called topological curves to distinguish them from more constrained curves such as differentiable curves. this definition encompasses most curves that are studied in mathematics; notable exceptions are level curves (which are unions of curves and isolated points), and algebraic curves (see below). level curves and algebraic curves are sometimes called implicit curves, since they are generally defined by implicit equations. nevertheless, the class of topological curves is very broad, and contains some curves that do not look as one may expect for a curve, or even cannot be drawn. this is the case of space-filling curves and fractal curves. for ensuring more regularity, the function that defines a curve is often supposed to be differentiable, and the curve is then said to be a differentiable curve. a plane algebraic curve is the zero set of a polynomial in two indeterminates. more generally, an algebraic curve is the zero set of a finite set of polynomials, which satisfies the further condition of being an algebraic variety of dimension one. if the coefficients of the polynomials belong to a field k, the curve is said to be defined over k. in the common case of a real algebraic curve, where k is the field of real numbers, an algebraic curve is a finite union of topological curves. when complex zeros are considered, one has a complex algebraic curve, which, from the topological point of view, is not a curve, but a surface, and is often called a riemann surface. although not being curves in the common sense, algebraic curves defined over other fields have been widely studied. in particular, algebraic curves over a finite field are widely used in modern cryptography. interest in curves began long before they were the subject of mathematical study. this can be seen in numerous examples of their decorative use in art and on everyday objects dating back to prehistoric times. curves, or at least their graphical representations, are simple to create, for example with a stick on the sand on a beach. historically, the term line was used in place of the more modern term curve. hence the terms straight line and right line were used to distinguish what are today called lines from curved lines. for example, in book i of euclid's elements, a line is defined as a ""breadthless length"" (def.  ), while a straight line is defined as ""a line that lies evenly with the points on itself"" (def.  ). euclid's idea of a line is perhaps clarified by the statement ""the extremities of a line are points,"" (def.  ). later commentators further classified lines according to various schemes. for example: the greek geometers had studied many other kinds of curves. one reason was their interest in solving geometrical problems that could not be solved using standard compass and straightedge construction. these curves include: a fundamental advance in the theory of curves was the introduction of analytic geometry by rené descartes in the seventeenth century. this enabled a curve to be described using an equation rather than an elaborate geometrical construction. this not only allowed new curves to be defined and studied, but it enabled a formal distinction to be made between algebraic curves that can be defined using polynomial equations, and transcendental curves that cannot. previously, curves had been described as ""geometrical"" or ""mechanical"" according to how they were, or supposedly could be, generated. conic sections were applied in astronomy by kepler. newton also worked on an early example in the calculus of variations. solutions to variational problems, such as the brachistochrone and tautochrone questions, introduced properties of curves in new ways (in this case, the cycloid). the catenary gets its name as the solution to the problem of a hanging chain, the sort of question that became routinely accessible by means of differential calculus. "
97,97,Cyclic Permutation,2,https://en.wikipedia.org/wiki/Cyclic_permutation,"in mathematics, and in particular in group theory, a cyclic permutation (or cycle) is a permutation of the elements of some set x which maps the elements of some subset s of x to each other in a cyclic fashion, while fixing (that is, mapping to themselves) all other elements of x. if s has k elements, the cycle is called a k-cycle. cycles are often denoted by the list of their elements enclosed with parentheses, in the order to which they are permuted. for example, given x = { ,  ,  ,  }, the permutation ( ,  ,  ,  ) that sends   to  ,   to  ,   to   and   to   (so s = x) is a  -cycle, and the permutation ( ,  ,  ) that sends   to  ,   to  ,   to   and   to   (so s = { ,  ,  } and   is a fixed element) is a  -cycle. on the other hand, the permutation that sends   to  ,   to  ,   to   and   to   is not a cyclic permutation because it separately permutes the pairs { ,  } and { ,  }. the set s is called the orbit of the cycle. every permutation on finitely many elements can be decomposed into cycles on disjoint orbits. the cyclic parts of a permutation are cycles, thus the second example is composed of a  -cycle and a  -cycle (or fixed point) and the third is composed of two  -cycles, and denoted ( ,  ) ( ,  ). a permutation is called a cyclic permutation if and only if it has a single nontrivial cycle (a cycle of length >  ). for example, the permutation, written in two-line notation (in two ways) and also cycle notations, is a six-cycle; its cycle diagram is shown at right. some authors restrict the definition to only those permutations which consist of one nontrivial cycle (that is, no fixed points allowed). for example, the permutation is a cyclic permutation under this more restrictive definition, while the preceding example is not. "
98,98,cylindrical cooridnate system,0,https://en.wikipedia.org/wiki/Cylindrical_coordinate_system," a cylindrical coordinate system is a three-dimensional coordinate system that specifies point positions by the distance from a chosen reference axis (axis l in the image opposite), the direction from the axis relative to a chosen reference direction (axis a), and the distance from a chosen reference plane perpendicular to the axis (plane containing the purple section). the latter distance is given as a positive or negative number depending on which side of the reference plane faces the point. the origin of the system is the point where all three coordinates can be given as zero. this is the intersection between the reference plane and the axis. the axis is variously called the cylindrical or longitudinal axis, to differentiate it from the polar axis, which is the ray that lies in the reference plane, starting at the origin and pointing in the reference direction. other directions perpendicular to the longitudinal axis are called radial lines. the distance from the axis may be called the radial distance or radius, while the angular coordinate is sometimes referred to as the angular position or as the azimuth. the radius and the azimuth are together called the polar coordinates, as they correspond to a two-dimensional polar coordinate system in the plane through the point, parallel to the reference plane. the third coordinate may be called the height or altitude (if the reference plane is considered horizontal), longitudinal position, or axial position. cylindrical coordinates are useful in connection with objects and phenomena that have some rotational symmetry about the longitudinal axis, such as water flow in a straight pipe with round cross-section, heat distribution in a metal cylinder, electromagnetic fields produced by an electric current in a long, straight wire, accretion disks in astronomy, and so on. they are sometimes called ""cylindrical polar coordinates"" and ""polar cylindrical coordinates"", and are sometimes used to specify the position of stars in a galaxy (""galactocentric cylindrical polar coordinates""). the three coordinates (ρ, φ, z) of a point p are defined as: as in polar coordinates, the same point with cylindrical coordinates (ρ, φ, z) has infinitely many equivalent coordinates, namely (ρ, φ ± n×   °, z) and (−ρ, φ ± ( n +  )×   °, z), where n is any integer. moreover, if the radius ρ is zero, the azimuth is arbitrary. in situations where someone wants a unique set of coordinates for each point, one may restrict the radius to be non-negative (ρ ≥  ) and the azimuth φ to lie in a specific interval spanning    °, such as [−   °,+   °] or [ ,   °]. the notation for cylindrical coordinates is not uniform. the iso standard   -   recommends (ρ, φ, z), where ρ is the radial coordinate, φ the azimuth, and z the height. however, the radius is also often denoted r or s, the azimuth by θ or t, and the third coordinate by h or (if the cylindrical axis is considered horizontal) x, or any context-specific letter. "
99,99,De Moivre's formula,2,https://en.wikipedia.org/wiki/De_Moivre%27s_formula," in mathematics, de moivre's formula (also known as de moivre's theorem and de moivre's identity) states that for any real number x and integer n it holds that where i is the imaginary unit (i  = − ). the formula is named after abraham de moivre, although he never stated it in his works. the expression cos x + i sin x is sometimes abbreviated to cis x. the formula is important because it connects complex numbers and trigonometry. by expanding the left hand side and then comparing the real and imaginary parts under the assumption that x is real, it is possible to derive useful expressions for cos nx and sin nx in terms of cos x and sin x. as written, the formula is not valid for non-integer powers n. however, there are generalizations of this formula valid for other exponents. these can be used to give explicit expressions for the nth roots of unity, that is, complex numbers z such that zn =  . for x =    ∘ {\displaystyle x=  ^{\circ }} and n =   {\displaystyle n= } , de moivre's formula asserts that de moivre's formula is a precursor to euler's formula one can derive de moivre's formula using euler's formula and the exponential law for integer powers since euler's formula implies that the left side is equal to ( cos ⁡ x + i sin ⁡ x ) n {\displaystyle \left(\cos x+i\sin x\right)^{n}} while the right side is equal to the truth of de moivre's theorem can be established by using mathematical induction for natural numbers, and extended to all integers from there. for an integer n, call the following statement s(n): "
100,100,De-Moivre Laplace Theorem,0,https://en.wikipedia.org/wiki/De_Moivre–Laplace_theorem,
101,101,De Morgan's law,1,https://en.wikipedia.org/wiki/De_Morgan%27s_laws,"in propositional logic and boolean algebra, de morgan's laws are a pair of transformation rules that are both valid rules of inference. they are named after augustus de morgan, a   th-century british mathematician. the rules allow the expression of conjunctions and disjunctions purely in terms of each other via negation. the rules can be expressed in english as: or or where ""a or b"" is an ""inclusive or"" meaning at least one of a or b rather than an ""exclusive or"" that means exactly one of a or b. in set theory and boolean algebra, these are written formally as where in formal language, the rules are written as and where "
102,102,Degenerate Conic,0,https://en.wikipedia.org/wiki/Degenerate_conic,"in geometry, a degenerate conic is a conic (a second-degree plane curve, defined by a polynomial equation of degree two) that fails to be an irreducible curve. this means that the defining equation is factorable over the complex numbers (or more generally over an algebraically closed field) as the product of two linear polynomials.[note  ] using the alternative definition of the conic as the intersection in three-dimensional space of a plane and a double cone, a conic is degenerate if the plane goes through the vertex of the cones. in the real plane, a degenerate conic can be two lines that may or may not be parallel, a single line (either two coinciding lines or the union of a line and the line at infinity), a single point (in fact, two complex conjugate lines), or the null set (twice the line at infinity or two parallel complex conjugate lines). all these degenerate conics may occur in pencils of conics. that is, if two real non-degenerated conics are defined by quadratic polynomial equations f =   and g =  , the conics of equations af + bg =   form a pencil, which contains one or three degenerate conics. for any degenerate conic in the real plane, one may choose f and g so that the given degenerate conic belongs to the pencil they determine. the conic section with equation x   − y   =   {\displaystyle x^{ }-y^{ }= } is degenerate as its equation can be written as ( x − y ) ( x + y ) =   {\displaystyle (x-y)(x+y)= } , and corresponds to two intersecting lines forming an ""x"". this degenerate conic occurs as the limit case a =   , b =   {\displaystyle a= ,b= } in the pencil of hyperbolas of equations a ( x   − y   ) − b =  . {\displaystyle a(x^{ }-y^{ })-b= .} the limiting case a =   , b =   {\displaystyle a= ,b= } is an example of a degenerate conic consisting of twice the line at infinity. similarly, the conic section with equation x   + y   =   {\displaystyle x^{ }+y^{ }= } , which has only one real point, is degenerate, as x   + y   {\displaystyle x^{ }+y^{ }} is factorable as ( x + i y ) ( x − i y ) {\displaystyle (x+iy)(x-iy)} over the complex numbers. the conic consists thus of two complex conjugate lines that intersect in the unique real point, (   ,   ) {\displaystyle ( , )} , of the conic. the pencil of ellipses of equations a x   + b ( y   −   ) =   {\displaystyle ax^{ }+b(y^{ }- )= } degenerates, for a =   , b =   {\displaystyle a= ,b= } , into two parallel lines and, for a =   , b =   {\displaystyle a= ,b= } , into a double line. the pencil of circles of equations a ( x   + y   −   ) − b x =   {\displaystyle a(x^{ }+y^{ }- )-bx= } degenerates for a =   {\displaystyle a= } into two lines, the line at infinity and the line of equation x =   {\displaystyle x= } . over the complex projective plane there are only two types of degenerate conics – two different lines, which necessarily intersect in one point, or one double line. any degenerate conic may be transformed by a projective transformation into any other degenerate conic of the same type. over the real affine plane the situation is more complicated. a degenerate real conic may be: "
103,103,Derangement,3,https://en.wikipedia.org/wiki/Derangement,"in combinatorial mathematics, a derangement is a permutation of the elements of a set, such that no element appears in its original position. in other words, a derangement is a permutation that has no fixed points. the number of derangements of a set of size n is known as the subfactorial of n or the n-th derangement number or n-th de montmort number. notations for subfactorials in common use include !n, dn, dn, or n¡. the subfactorial !n equals the nearest integer to n!/e, where n! denotes the factorial of n and e is euler's number. the problem of counting derangements was first considered by pierre raymond de montmort in     ; he solved it in     , as did nicholas bernoulli at about the same time. suppose that a professor gave a test to   students – a, b, c, and d – and wants to let them grade each other's tests. of course, no student should grade their own test. how many ways could the professor hand the tests back to the students for grading, such that no student received their own test back? out of    possible permutations ( !) for handing back the tests, there are only   derangements (shown in blue italics above). in every other permutation of this  -member set, at least one student gets their own test back (shown in bold red). another version of the problem arises when we ask for the number of ways n letters, each addressed to a different person, can be placed in n pre-addressed envelopes so that no letter appears in the correctly addressed envelope. counting derangements of a set amounts to the hat-check problem, in which one considers the number of ways in which n hats (call them h  through hn) can be returned to n people (p  through pn) such that no hat makes it back to its owner. each person may receive any of the n −   hats that is not their own. call whichever hat p  receives hi and consider hi’s owner: pi receives either p 's hat, h , or some other. accordingly, the problem splits into two possible cases: for each of the n −   hats that p  may receive, the number of ways that p , …, pn may all receive hats is the sum of the counts for the two cases. "
104,104,Derivative,1,https://en.wikipedia.org/wiki/Derivative," in mathematics, the derivative of a function of a real variable measures the sensitivity to change of the function value (output value) with respect to a change in its argument (input value). derivatives are a fundamental tool of calculus. for example, the derivative of the position of a moving object with respect to time is the object's velocity: this measures how quickly the position of the object changes when time advances. the derivative of a function of a single variable at a chosen input value, when it exists, is the slope of the tangent line to the graph of the function at that point. the tangent line is the best linear approximation of the function near that input value. for this reason, the derivative is often described as the ""instantaneous rate of change"", the ratio of the instantaneous change in the dependent variable to that of the independent variable. derivatives can be generalized to functions of several real variables. in this generalization, the derivative is reinterpreted as a linear transformation whose graph is (after an appropriate translation) the best linear approximation to the graph of the original function. the jacobian matrix is the matrix that represents this linear transformation with respect to the basis given by the choice of independent and dependent variables. it can be calculated in terms of the partial derivatives with respect to the independent variables. for a real-valued function of several variables, the jacobian matrix reduces to the gradient vector. the process of finding a derivative is called differentiation. the reverse process is called antidifferentiation. the fundamental theorem of calculus relates antidifferentiation with integration. differentiation and integration constitute the two fundamental operations in single-variable calculus.[note  ] a function of a real variable y = f(x) is differentiable at a point a of its domain, if its domain contains an open interval i containing a, and the limit exists. this means that, for every positive real number ε {\displaystyle \varepsilon } (even very small), there exists a positive real number δ {\displaystyle \delta } such that, for every h such that | h | < δ {\displaystyle |h|<\delta } and h ≠   {\displaystyle h\neq  } then f ( a + h ) {\displaystyle f(a+h)} is defined, and where the vertical bars denote the absolute value (see (ε, δ)-definition of limit). if the function f is differentiable at a, that is if the limit l exists, then this limit is called the derivative of f at a, and denoted f ′ ( a ) {\displaystyle f'(a)} (read as ""f prime of a"") or d f d x ( a ) {\textstyle {\frac {df}{dx}}(a)} (read as ""the derivative of f with respect to x at a"", ""dy by dx at a"", or ""dy over dx at a""); see § notation (details), below. "
105,105,Determiant of Square matrix,2,https://en.wikipedia.org/wiki/Determinant,"in mathematics, the determinant is a scalar value that is a function of the entries of a square matrix. it allows characterizing some properties of the matrix and the linear map represented by the matrix. in particular, the determinant is nonzero if and only if the matrix is invertible and the linear map represented by the matrix is an isomorphism. the determinant of a product of matrices is the product of their determinants (the preceding property is a corollary of this one). the determinant of a matrix a is denoted det(a), det a, or |a|. in the case of a   ×   matrix the determinant can be defined as similarly, for a   ×   matrix a, its determinant is each determinant of a   ×   matrix in this equation is called a minor of the matrix a. this procedure can be extended to give a recursive definition for the determinant of an n × n matrix, known as laplace expansion. determinants occur throughout mathematics. for example, a matrix is often used to represent the coefficients in a system of linear equations, and determinants can be used to solve these equations (cramer's rule), although other methods of solution are computationally much more efficient. determinants are used for defining the characteristic polynomial of a matrix, whose roots are the eigenvalues. in geometry, the signed n-dimensional volume of a n-dimensional parallelepiped is expressed by a determinant. this is used in calculus with exterior differential forms and the jacobian determinant, in particular for changes of variables in multiple integrals. the determinant of a   ×   matrix ( a b c d ) {\displaystyle {\begin{pmatrix}a&b\\c&d\end{pmatrix}}} is denoted either by ""det"" or by vertical bars around the matrix, and is defined as for example, the determinant has several key properties that can be proved by direct evaluation of the definition for   ×   {\displaystyle  \times  } -matrices, and that continue to hold for determinants of larger matrices. they are as follows: first, the determinant of the identity matrix (         ) {\displaystyle {\begin{pmatrix} & \\ & \end{pmatrix}}} is  . second, the determinant is zero if two rows are the same: this holds similarly if the two columns are the same. moreover, finally, if any column is multiplied by some number r {\displaystyle r} (i.e., all entries in that column are multiplied by that number), the determinant is also multiplied by that number: "
106,106,Determinant,2,https://en.wikipedia.org/wiki/Determinant2,
107,107,Diagonal Matrix,1,https://en.wikipedia.org/wiki/Diagonal_matrix," in linear algebra, a diagonal matrix is a matrix in which the entries outside the main diagonal are all zero; the term usually refers to square matrices. elements of the main diagonal can either be zero or nonzero. an example of a  ×  diagonal matrix is [         ] {\displaystyle \left[{\begin{smallmatrix} & \\ & \end{smallmatrix}}\right]} , while an example of a  ×  diagonal matrix is [                   ] {\displaystyle \left[{\begin{smallmatrix} & & \\ & & \\ & & \end{smallmatrix}}\right]} . an identity matrix of any size, or any multiple of it (a scalar matrix), is a diagonal matrix. a diagonal matrix is sometimes called a scaling matrix, since matrix multiplication with it results in changing scale (size). its determinant is the product of its diagonal values. as stated above, a diagonal matrix is a matrix in which all off-diagonal entries are zero. that is, the matrix d = (di,j) with n columns and n rows is diagonal if however, the main diagonal entries are unrestricted. the term diagonal matrix may sometimes refer to a rectangular diagonal matrix, which is an m-by-n matrix with all the entries not of the form di,i being zero. for example: more often, however, diagonal matrix refers to square matrices, which can be specified explicitly as a square diagonal matrix. a square diagonal matrix is a symmetric matrix, so this can also be called a symmetric diagonal matrix. the following matrix is square diagonal matrix: if the entries are real numbers or complex numbers, then it is a normal matrix as well. in the remainder of this article we will consider only square diagonal matrices, and refer to them simply as ""diagonal matrices"". "
108,108,Differentiable function,2,https://en.wikipedia.org/wiki/Differentiable_function,"in mathematics, a differentiable function of one real variable is a function whose derivative exists at each point in its domain. in other words, the graph of a differentiable function has a non-vertical tangent line at each interior point in its domain. a differentiable function is smooth (the function is locally well approximated as a linear function at each interior point) and does not contain any break, angle, or cusp. if x  is an interior point in the domain of a function f, then f is said to be differentiable at x  if the derivative f ′ ( x   ) {\displaystyle f'(x_{ })} exists. in other words, the graph of f has a non-vertical tangent line at the point (x , f(x )). f is said to be differentiable on u if it is differentiable at every point of u. f is said to be continuously differentiable if its derivative is also a continuous function. generally speaking, f is said to be of class c k {\displaystyle c^{k}} if its first k {\displaystyle k} derivatives f ′ ( x ) , f ′ ′ ( x ) , … , f ( k ) ( x ) {\displaystyle f^{\prime }(x),f^{\prime \prime }(x),\ldots ,f^{(k)}(x)} exist and are continuous. a function f : u → r {\displaystyle f:u\to \mathbb {r} } , defined on an open set u ⊂ r {\displaystyle u\subset \mathbb {r} } , is said to be differentiable at a ∈ u {\displaystyle a\in u} if the derivative exists. this implies that the function is continuous at a. this function f is said to be differentiable on u if it is differentiable at every point of u. in this case, the derivative of f is thus a function from u into r . {\displaystyle \mathbb {r} .} a continuous function is not necessarily differentiable, but a differentiable function is necessarily continuous (at every point where it is differentiable) as being shown below (in the section differentiability and continuity). a function is said to be continuously differentiable if its derivative is also a continuous function; there exists a function that is differentiable but not continuously differentiable as being shown below (in the section differentiability classes). if f is differentiable at a point x , then f must also be continuous at x . in particular, any differentiable function must be continuous at every point in its domain. the converse does not hold: a continuous function need not be differentiable. for example, a function with a bend, cusp, or vertical tangent may be continuous, but fails to be differentiable at the location of the anomaly. most functions that occur in practice have derivatives at all points or at almost every point. however, a result of stefan banach states that the set of functions that have a derivative at some point is a meagre set in the space of all continuous functions. informally, this means that differentiable functions are very atypical among continuous functions. the first known example of a function that is continuous everywhere but differentiable nowhere is the weierstrass function. a function f {\displaystyle f} is said to be continuously differentiable if the derivative f ′ ( x ) {\displaystyle f^{\prime }(x)} exists and is itself a continuous function. although the derivative of a differentiable function never has a jump discontinuity, it is possible for the derivative to have an essential discontinuity. for example, the function similarly to how continuous functions are said to be of class c   , {\displaystyle c^{ },} continuously differentiable functions are sometimes said to be of class c   . {\displaystyle c^{ }.} a function is of class c   {\displaystyle c^{ }} if the first and second derivative of the function both exist and are continuous. more generally, a function is said to be of class c k {\displaystyle c^{k}} if the first k {\displaystyle k} derivatives f ′ ( x ) , f ′ ′ ( x ) , … , f ( k ) ( x ) {\displaystyle f^{\prime }(x),f^{\prime \prime }(x),\ldots ,f^{(k)}(x)} all exist and are continuous. if derivatives f ( n ) {\displaystyle f^{(n)}} exist for all positive integers n , {\displaystyle n,} the function is smooth or equivalently, of class c ∞ . {\displaystyle c^{\infty }.} "
109,109,Differential algebra,0,https://en.wikipedia.org/wiki/Differential_algebra,"in mathematics, differential rings, differential fields, and differential algebras are rings, fields, and algebras equipped with finitely many derivations, which are unary functions that are linear and satisfy the leibniz product rule. a natural example of a differential field is the field of rational functions in one variable over the complex numbers, c ( t ) {\displaystyle \mathbb {c} (t)} , where the derivation is differentiation with respect to t. differential algebra refers also to the area of mathematics consisting in the study of these algebraic objects and their use for an algebraic study of the differential equations. differential algebra was introduced by joseph ritt in     . a differential ring is a ring r equipped with one or more derivations, that are homomorphisms of additive groups such that each derivation ∂ satisfies the leibniz product rule for every r   , r   ∈ r {\displaystyle r_{ },r_{ }\in r} . note that the ring could be noncommutative, so the somewhat standard d(xy) = xdy + ydx form of the product rule in commutative settings may be false. if m : r × r → r {\displaystyle m:r\times r\to r} is multiplication on the ring, the product rule is the identity where f × g {\displaystyle f\times g} means the function which maps a pair ( x , y ) {\displaystyle (x,y)} to the pair ( f ( x ) , g ( y ) ) {\displaystyle (f(x),g(y))} . note that a differential ring is a (not necessarily graded) z {\displaystyle \mathbb {z} } -differential algebra. a differential field is a commutative field k equipped with derivations. the well-known formula for differentiating fractions follows from the product rule. indeed, we must have "
110,110,Differential calculus,2,https://en.wikipedia.org/wiki/Differential_calculus,"in mathematics, differential calculus is a subfield of calculus that studies the rates at which quantities change. it is one of the two traditional divisions of calculus, the other being integral calculus—the study of the area beneath a curve. the primary objects of study in differential calculus are the derivative of a function, related notions such as the differential, and their applications. the derivative of a function at a chosen input value describes the rate of change of the function near that input value. the process of finding a derivative is called differentiation. geometrically, the derivative at a point is the slope of the tangent line to the graph of the function at that point, provided that the derivative exists and is defined at that point. for a real-valued function of a single real variable, the derivative of a function at a point generally determines the best linear approximation to the function at that point. differential calculus and integral calculus are connected by the fundamental theorem of calculus, which states that differentiation is the reverse process to integration. differentiation has applications in nearly all quantitative disciplines. in physics, the derivative of the displacement of a moving body with respect to time is the velocity of the body, and the derivative of the velocity with respect to time is acceleration. the derivative of the momentum of a body with respect to time equals the force applied to the body; rearranging this derivative statement leads to the famous f = ma equation associated with newton's second law of motion. the reaction rate of a chemical reaction is a derivative. in operations research, derivatives determine the most efficient ways to transport materials and design factories. derivatives are frequently used to find the maxima and minima of a function. equations involving derivatives are called differential equations and are fundamental in describing natural phenomena. derivatives and their generalizations appear in many fields of mathematics, such as complex analysis, functional analysis, differential geometry, measure theory, and abstract algebra. the derivative of f ( x ) {\displaystyle f(x)} at the point x = a {\displaystyle x=a} is the slope of the tangent to ( a , f ( a ) ) {\displaystyle (a,f(a))} . in order to gain an intuition for this, one must first be familiar with finding the slope of a linear equation, written in the form y = m x + b {\displaystyle y=mx+b} . the slope of an equation is its steepness. it can be found by picking any two points and dividing the change in y {\displaystyle y} by the change in x {\displaystyle x} , meaning that slope = change in y change in x {\displaystyle {\text{slope }}={\frac {{\text{ change in }}y}{{\text{change in }}x}}} . for, the graph of y = −   x +    {\displaystyle y=- x+  } has a slope of −   {\displaystyle - } , as shown in the diagram below: for brevity, change in y change in x {\displaystyle {\frac {{\text{change in }}y}{{\text{change in }}x}}} is often written as δ y δ x {\displaystyle {\frac {\delta y}{\delta x}}} , with δ {\displaystyle \delta } being the greek letter delta, meaning 'change in'. the slope of a linear equation is constant, meaning that the steepness is the same everywhere. however, many graphs, for instance y = x   {\displaystyle y=x^{ }} , vary in their steepness. this means that you can no longer pick any two arbitrary points and compute the slope. instead, the slope of the graph can be computed by considering the tangent line—a line that 'just touches' a particular point.[note  ] the slope of a curve at a particular point is equal to the slope of the tangent to that point. for example, y = x   {\displaystyle y=x^{ }} has a slope of   {\displaystyle  } at x =   {\displaystyle x= } because the slope of the tangent line to that point is equal to   {\displaystyle  } : the derivative of a function is then simply the slope of this tangent line.[note  ] even though the tangent line only touches a single point at the point of tangency, it can be approximated by a line that goes through two points. this is known as a secant line. if the two points that the secant line goes through are close together, then the secant line closely resembles the tangent line, and, as a result, its slope is also very similar: the advantage of using a secant line is that its slope can be calculated directly. consider the two points on the graph ( x , f ( x ) ) {\displaystyle (x,f(x))} and ( x + δ x , f ( x + δ x ) ) {\displaystyle (x+\delta x,f(x+\delta x))} , where δ x {\displaystyle \delta x} is a small number. as before, the slope of the line passing through these two points can be calculated with the formula slope = δ y δ x {\displaystyle {\text{slope }}={\frac {\delta y}{\delta x}}} . this gives as δ x {\displaystyle \delta x} gets closer and closer to   {\displaystyle  } , the slope of the secant line gets closer and closer to the slope of the tangent line. this is formally written as "
111,111,Differential Equation,0,https://en.wikipedia.org/wiki/Differential_equation,"in mathematics, a differential equation is an equation that relates one or more unknown functions and their derivatives. in applications, the functions generally represent physical quantities, the derivatives represent their rates of change, and the differential equation defines a relationship between the two. such relations are common; therefore, differential equations play a prominent role in many disciplines including engineering, physics, economics, and biology. mainly the study of differential equations consists of the study of their solutions (the set of functions that satisfy each equation), and of the properties of their solutions. only the simplest differential equations are solvable by explicit formulas; however, many properties of solutions of a given differential equation may be determined without computing them exactly. often when a closed-form expression for the solutions is not available, solutions may be approximated numerically using computers. the theory of dynamical systems puts emphasis on qualitative analysis of systems described by differential equations, while many numerical methods have been developed to determine solutions with a given degree of accuracy. differential equations first came into existence with the invention of calculus by newton and leibniz. in chapter   of his      work methodus fluxionum et serierum infinitarum, isaac newton listed three kinds of differential equations: in all these cases, y is an unknown function of x (or of x  and x ), and f is a given function. he solves these examples and others using infinite series and discusses the non-uniqueness of solutions. jacob bernoulli proposed the bernoulli differential equation in     . this is an ordinary differential equation of the form for which the following year leibniz obtained solutions by simplifying it. historically, the problem of a vibrating string such as that of a musical instrument was studied by jean le rond d'alembert, leonhard euler, daniel bernoulli, and joseph-louis lagrange. in     , d’alembert discovered the one-dimensional wave equation, and within ten years euler discovered the three-dimensional wave equation. the euler–lagrange equation was developed in the     s by euler and lagrange in connection with their studies of the tautochrone problem. this is the problem of determining a curve on which a weighted particle will fall to a fixed point in a fixed amount of time, independent of the starting point. lagrange solved this problem in      and sent the solution to euler. both further developed lagrange's method and applied it to mechanics, which led to the formulation of lagrangian mechanics. "
112,112,differntiation,2,https://en.wikipedia.org/wiki/Differentiation,differentiation may refer to: 
113,113,Differentiation of trigonometric functions,1,https://en.wikipedia.org/wiki/Differentiation_of_trigonometric_functions,"the differentiation of trigonometric functions is the mathematical process of finding the derivative of a trigonometric function, or its rate of change with respect to a variable. for example, the derivative of the sine function is written sin′(a) = cos(a), meaning that the rate of change of sin(x) at a particular angle x = a is given by the cosine of that angle. all derivatives of circular trigonometric functions can be found from those of sin(x) and cos(x) by means of the quotient rule applied to functions such as tan(x) = sin(x)/cos(x). knowing these derivatives, the derivatives of the inverse trigonometric functions are found using implicit differentiation. the diagram at right shows a circle with centre o and radius r =  . let two radii oa and ob make an arc of θ radians. since we are considering the limit as θ tends to zero, we may assume θ is a small positive number, say   < θ < ½ π in the first quadrant. in the diagram, let r  be the triangle oab, r  the circular sector oab, and r  the triangle oac. the area of triangle oab is: the area of the circular sector oab is a r e a ( r   ) =     θ {\displaystyle \mathrm {area} (r_{ })={\tfrac { }{ }}\theta } , while the area of the triangle oac is given by since each region is contained in the next, one has: moreover, since sin θ >   in the first quadrant, we may divide through by ½ sin θ, giving: in the last step we took the reciprocals of the three positive terms, reversing the inequities. we conclude that for   < θ < ½ π, the quantity sin(θ)/θ is always less than   and always greater than cos(θ). thus, as θ gets closer to  , sin(θ)/θ is ""squeezed"" between a ceiling at height   and a floor at height cos θ, which rises towards  ; hence sin(θ)/θ must tend to   as θ tends to   from the positive side: lim θ →   + sin ⁡ θ θ =   . {\displaystyle \lim _{\theta \to  ^{+}}{\frac {\sin \theta }{\theta }}= \,.} "
114,114,"derivative of the sum, difference, product",1,https://en.wikipedia.org/wiki/Differentiation_rules,"this is a summary of differentiation rules, that is, rules for computing the derivative of a function in calculus. unless otherwise stated, all functions are functions of real numbers (r) that return real values; although more generally, the formulae below apply wherever they are well defined — including the case of complex numbers (c). for any functions f {\displaystyle f} and g {\displaystyle g} and any real numbers a {\displaystyle a} and b {\displaystyle b} , the derivative of the function h ( x ) = a f ( x ) + b g ( x ) {\displaystyle h(x)=af(x)+bg(x)} with respect to x {\displaystyle x} is in leibniz's notation this is written as: special cases include: for the functions f and g, the derivative of the function h(x) = f(x) g(x) with respect to x is in leibniz's notation this is written the derivative of the function h ( x ) = f ( g ( x ) ) {\displaystyle h(x)=f(g(x))} is in leibniz's notation, this is written as: often abridged to "
115,115,Dihedral angle,3,https://en.wikipedia.org/wiki/Dihedral_angle,"right interior adjacent vertical complementary supplementary a dihedral angle is the angle between two intersecting planes or half-planes. in chemistry, it is the clockwise angle between half-planes through two sets of three atoms, having two atoms in common. in solid geometry, it is defined as the union of a line and two half-planes that have this line as a common edge. in higher dimensions, a dihedral angle represents the angle between two hyperplanes. the planes of a flying machine are said to be at positive dihedral angle when both starboard and port main planes are upwardly inclined to the lateral axis. when downwardly inclined they are said to be at a negative dihedral angle. when the two intersecting planes are described in terms of cartesian coordinates by the two equations the dihedral angle, φ {\displaystyle \varphi } between them is given by: and satisfies   ≤ φ ≤ π /  . {\displaystyle  \leq \varphi \leq \pi / .} alternatively, if na and nb are normal vector to the planes, one has where na · nb is the dot product of the vectors and |na| |nb| is the product of their lengths. the absolute value is required in above formulas, as the planes are not changed when changing all coefficient signs in one equation, or replacing one normal vector by its opposite. however the absolute values can be and should be avoided when considering the dihedral angle of two half planes whose boundaries are the same line. in this case, the half planes can be described by a point p of their intersection, and three vectors b , b  and b  such that p + b , p + b  and p + b  belong respectively to the intersection line, the first half plane, and the second half plane. the dihedral angle of these two half planes is defined by "
116,116,Diophantine Geometry,0,https://en.wikipedia.org/wiki/Diophantine_geometry,"in mathematics, diophantine geometry is the study of diophantine equations by means of powerful methods in algebraic geometry. by the   th century it became clear for some mathematicians that methods of algebraic geometry are ideal tools to study these equations. four theorems in diophantine geometry which are of fundamental importance include: serge lang published a book diophantine geometry in the area in     , and by this book he coined the term ""diophantine geometry"". the traditional arrangement of material on diophantine equations was by degree and number of variables, as in mordell's diophantine equations (    ). mordell's book starts with a remark on homogeneous equations f =   over the rational field, attributed to c. f. gauss, that non-zero solutions in integers (even primitive lattice points) exist if non-zero rational solutions do, and notes a caveat of l. e. dickson, which is about parametric solutions. the hilbert–hurwitz result from      reducing the diophantine geometry of curves of genus   to degrees   and   (conic sections) occurs in chapter   , as does mordell's conjecture. siegel's theorem on integral points occurs in chapter   . mordell's theorem on the finite generation of the group of rational points on an elliptic curve is in chapter   , and integer points on the mordell curve in chapter   . in a hostile review of lang's book, mordell wrote: in recent times, powerful new geometric ideas and methods have been developed by means of which important new arithmetical theorems and related results have been found and proved and some of these are not easily proved otherwise. further, there has been a tendency to clothe the old results, their extensions, and proofs in the new geometrical language. sometimes, however, the full implications of results are best described in a geometrical setting. lang has these aspects very much in mind in this book, and seems to miss no opportunity for geometric presentation. this accounts for his title ""diophantine geometry."" he notes that the content of the book is largely versions of the mordell–weil theorem, thue–siegel–roth theorem, siegel's theorem, with a treatment of hilbert's irreducibility theorem and applications (in the style of siegel). leaving aside issues of generality, and a completely different style, the major mathematical difference between the two books is that lang used abelian varieties and offered a proof of siegel's theorem, while mordell noted that the proof ""is of a very advanced character"" (p.    ). despite a bad press initially, lang's conception has been sufficiently widely accepted for a      tribute to call the book ""visionary"". a larger field sometimes called arithmetic of abelian varieties now includes diophantine geometry along with class field theory, complex multiplication, local zeta-functions and l-functions. paul vojta wrote: a single equation defines a hypersurface, and simultaneous diophantine equations give rise to a general algebraic variety v over k; the typical question is about the nature of the set v(k) of points on v with co-ordinates in k, and by means of height functions quantitative questions about the ""size"" of these solutions may be posed, as well as the qualitative issues of whether any points exist, and if so whether there are an infinite number. given the geometric approach, the consideration of homogeneous equations and homogeneous co-ordinates is fundamental, for the same reasons that projective geometry is the dominant approach in algebraic geometry. rational number solutions therefore are the primary consideration; but integral solutions (i.e. lattice points) can be treated in the same way as an affine variety may be considered inside a projective variety that has extra points at infinity. the general approach of diophantine geometry is illustrated by faltings's theorem (a conjecture of l. j. mordell) stating that an algebraic curve c of genus g >   over the rational numbers has only finitely many rational points. the first result of this kind may have been the theorem of hilbert and hurwitz dealing with the case g =  . the theory consists both of theorems and many conjectures and open questions. "
117,117,Directed Graph,1,https://en.wikipedia.org/wiki/Directed_graph,"in mathematics, and more specifically in graph theory, a directed graph (or digraph) is a graph that is made up of a set of vertices connected by directed edges often called arcs. in formal terms, a directed graph is an ordered pair g = (v, a) where it differs from an ordinary or undirected graph, in that the latter is defined in terms of unordered pairs of vertices, which are usually called edges, links or lines. the aforementioned definition does not allow a directed graph to have multiple arrows with the same source and target nodes, but some authors consider a broader definition that allows directed graphs to have such multiple arcs (namely, they allow the arc set to be a multiset). more specifically, these entities are addressed as directed multigraphs (or multidigraphs). on the other hand, the aforementioned definition allows a directed graph to have loops (that is, arcs that directly connect nodes with themselves), but some authors consider a narrower definition that doesn't allow directed graphs to have loops. more specifically, directed graphs without loops are addressed as simple directed graphs, while directed graphs with loops are addressed as loop-digraphs (see section types of directed graphs). an arc (x, y) is considered to be directed from x to y; y is called the head and x is called the tail of the arc; y is said to be a direct successor of x and x is said to be a direct predecessor of y. if a path leads from x to y, then y is said to be a successor of x and reachable from x, and x is said to be a predecessor of y. the arc (y, x) is called the reversed arc of (x, y). the adjacency matrix of a multidigraph with loops is the integer-valued matrix with rows and columns corresponding to the vertices, where a nondiagonal entry aij is the number of arcs from vertex i to vertex j, and the diagonal entry aii is the number of loops at vertex i. the adjacency matrix of a directed graph is unique up to identical permutation of rows and columns. another matrix representation for a directed graph is its incidence matrix. see direction for more definitions. for a vertex, the number of head ends adjacent to a vertex is called the indegree of the vertex and the number of tail ends adjacent to a vertex is its outdegree (called branching factor in trees). let g = (v, a) and v ∈ v. the indegree of v is denoted deg−(v) and its outdegree is denoted deg+(v). "
118,118,Direction cosine,2,https://en.wikipedia.org/wiki/Direction_cosine,"in analytic geometry, the direction cosines (or directional cosines) of a vector are the cosines of the angles between the vector and the three positive coordinate axes. equivalently, they are the contributions of each component of the basis to a unit vector in that direction. if v is a euclidean vector in three-dimensional euclidean space, r , where ex, ey, ez are the standard basis in cartesian notation, then the direction cosines are it follows that by squaring each equation and adding the results here α, β and γ are the direction cosines and the cartesian coordinates of the unit vector v/|v|, and a, b and c are the direction angles of the vector v. the direction angles a, b and c are acute or obtuse angles, i.e.,   ≤ a ≤ π,   ≤ b ≤ π and   ≤ c ≤ π, and they denote the angles formed between v and the unit basis vectors, ex, ey and ez. more generally, direction cosine refers to the cosine of the angle between any two vectors. they are useful for forming direction cosine matrices that express one set of orthonormal basis vectors in terms of another set, or for expressing a known vector in a different basis. "
119,119,Directional Derivative,3,https://en.wikipedia.org/wiki/Directional_derivative,"in mathematics, the directional derivative of a multivariable differentiable (scalar) function along a given vector v at a given point x intuitively represents the instantaneous rate of change of the function, moving through x with a velocity specified by v. the directional derivative of a scalar function f with respect to a vector v at a point (e.g., position) x may be denoted by any of the following: it therefore generalizes the notion of a partial derivative, in which the rate of change is taken along one of the curvilinear coordinate curves, all other coordinates being constant. the directional derivative is a special case of the gateaux derivative. the directional derivative of a scalar function along a vector is the function ∇ v f {\displaystyle \nabla _{\mathbf {v} }{f}} defined by the limit this definition is valid in a broad range of contexts, for example where the norm of a vector (and hence a unit vector) is undefined. if the function f is differentiable at x, then the directional derivative exists along any vector v, and one has where the ∇ {\displaystyle \nabla } on the right denotes the gradient and ⋅ {\displaystyle \cdot } is the dot product. this follows from defining a path h ( t ) = x + t v {\displaystyle h(t)=x+tv} and using the definition of the derivative as a limit which can be calculated along this path to get: intuitively, the directional derivative of f at a point x represents the rate of change of f, in the direction of v with respect to time, when moving past x. "
120,120,Director circle,2,https://en.wikipedia.org/wiki/Director_circle,"in geometry, the director circle of an ellipse or hyperbola (also called the orthoptic circle or fermat–apollonius circle) is a circle consisting of all points where two perpendicular tangent lines to the ellipse or hyperbola cross each other. the director circle of an ellipse circumscribes the minimum bounding box of the ellipse. it has the same center as the ellipse, with radius a   + b   {\displaystyle {\sqrt {a^{ }+b^{ }}}} , where a {\displaystyle a} and b {\displaystyle b} are the semi-major axis and semi-minor axis of the ellipse. additionally, it has the property that, when viewed from any point on the circle, the ellipse spans a right angle. the director circle of a hyperbola has radius √a  - b , and so, may not exist in the euclidean plane, but could be a circle with imaginary radius in the complex plane. more generally, for any collection of points pi, weights wi, and constant c, one can define a circle as the locus of points x such that the director circle of an ellipse is a special case of this more general construction with two points p  and p  at the foci of the ellipse, weights w  = w  =  , and c equal to the square of the major axis of the ellipse. the apollonius circle, the locus of points x such that the ratio of distances of x to two foci p  and p  is a fixed constant r, is another special case, with w  =  , w  = −r , and c =  . in the case of a parabola the director circle degenerates to a straight line, the directrix of the parabola. "
121,121,Discriminant,0,https://en.wikipedia.org/wiki/Discriminant,"in mathematics, the discriminant of a polynomial is a quantity that depends on the coefficients and determines various properties of the roots. it is generally defined as a polynomial function of the coefficients of the original polynomial. the discriminant is widely used in polynomial factoring, number theory, and algebraic geometry. it is often denoted by the symbol δ {\displaystyle \delta } . the discriminant of the quadratic polynomial a x   + b x + c {\displaystyle ax^{ }+bx+c\,} with a ≠   is: the quantity which appears under the square root in the quadratic formula. this discriminant is zero if and only if the polynomial has a double root. in the case of real coefficients, it is positive if the polynomial has two distinct real roots, and negative if it has two distinct complex conjugate roots. similarly, for a cubic polynomial, there is a discriminant which is zero if and only if the polynomial has a multiple root. in the case of a cubic with real coefficients, the discriminant is positive if the polynomial has three distinct real roots, and negative if it has one real root and two distinct complex conjugate roots. more generally, the discriminant of a univariate polynomial of positive degree is zero if and only if the polynomial has a multiple root. for real coefficients and no multiple roots, the discriminant is positive if the number of non-real roots is a multiple of   (including none), and negative otherwise. several generalizations are also called discriminant: the discriminant of an algebraic number field; the discriminant of a quadratic form; and more generally, the discriminant of a form, of a homogeneous polynomial, or of a projective hypersurface (these three concepts are essentially equivalent). the term ""discriminant"" was coined in      by the british mathematician james joseph sylvester. let be a polynomial of degree n (this means a n ≠   {\displaystyle a_{n}\neq  } ), such that the coefficients a   , … , a n {\displaystyle a_{ },\ldots ,a_{n}} belong to a field, or, more generally, to a commutative ring. the resultant of a and its derivative a ′ ( x ) = n a n x n −   + ( n −   ) a n −   x n −   + ⋯ + a   {\displaystyle a'(x)=na_{n}x^{n- }+(n- )a_{n- }x^{n- }+\cdots +a_{ }} is a polynomial in a   , … , a n {\displaystyle a_{ },\ldots ,a_{n}} with integer coefficients, which is the determinant of the sylvester matrix of a and a′. the nonzero entries of the first column of the sylvester matrix are a n {\displaystyle a_{n}} and n a n , {\displaystyle na_{n},} and the resultant is thus a multiple of a n . {\displaystyle a_{n}.} hence the discriminant—up to its sign—is defined as the quotient of the resultant of a and a' by a n {\displaystyle a_{n}} : historically, this sign has been chosen such that, over the reals, the discriminant will be positive when all the roots of the polynomial are real. the division by a n {\displaystyle a_{n}} may not be well defined if the ring of the coefficients contains zero divisors. such a problem may be avoided by replacing a n {\displaystyle a_{n}} by   in the first column of the sylvester matrix—before computing the determinant. in any case, the discriminant is a polynomial in a   , … , a n {\displaystyle a_{ },\ldots ,a_{n}} with integer coefficients. when the polynomial is defined over a field, it has n roots, r , r , ..., rn, not necessarily all distinct, in any algebraically closed extension of the field. (if the coefficients are real numbers, the roots may be taken in the field of complex numbers, where the fundamental theorem of algebra applies.) "
122,122,distance between two points,1,https://en.wikipedia.org/wiki/Distance,"distance is a numerical measurement of how far apart objects or points are. in physics or everyday usage, distance may refer to a physical length or an estimation based on other criteria (e.g. ""two counties over""). the distance from a point a to a point b is sometimes denoted as | a b | {\displaystyle |ab|} . in most cases, ""distance from a to b"" is interchangeable with ""distance from b to a"". in mathematics, a distance function or metric is a generalization of the concept of physical distance; it is a way of describing what it means for elements of some space to be ""close to"", or ""far away from"" each other. in psychology and social sciences, distance is a non-numerical measurement; psychological distance is defined as ""the different ways in which an object might be removed from"" the self along dimensions such as ""time, space, social distance, and hypotheticality. a physical distance can mean several different things: ""circular distance"" is the distance traveled by a wheel, which can be useful when designing vehicles or mechanical gears. the circumference of the wheel is  π × radius, and assuming the radius to be  , then each revolution of the wheel is equivalent of the distance  π radians. in engineering ω =  πƒ is often used, where ƒ is the frequency. unusual definitions of distance can be helpful to model certain physical situations, but are also used in theoretical mathematics: distance measures in cosmology are complicated by the expansion of the universe, and by effects described by the theory of relativity (such as length contraction of moving objects). the term ""distance"" is also used by analogy to measure non-physical entities in certain ways. in computer science, there is the notion of the ""edit distance"" between two strings. for example, the words ""dog"" and ""dot"", which vary by only one letter, are closer than ""dog"" and ""cat"", which differ by three letters. this idea is used in spell checkers and in coding theory, and is mathematically formalized in several different ways such as: in mathematics, a metric space is a set for which distances between all members of the set are defined. in this way, many different types of ""distances"" can be calculated, such as for traversal of graphs, comparison of distributions and curves, and using unusual definitions of ""space"" (for example using a manifold or reflections). the notion of distance in graph theory has been used to describe social networks, for example with the erdős number or the bacon number—the number of collaborative relationships away a person is from prolific mathematician paul erdős and actor kevin bacon, respectively. in psychology, human geography, and the social sciences, distance is often theorized not as an objective metric, but as a subjective experience. both distance and displacement measure the movement of an object. distance cannot be negative, and never decreases. distance is a scalar quantity, or a magnitude, whereas displacement is a vector quantity with both magnitude and direction. it can be negative, zero, or positive. directed distance does not measure movement; it measures the separation of two points, and can be a positive, zero, or negative vector. "
123,123,distance between two parallel lines,1,https://en.wikipedia.org/wiki/Distance_between_two_parallel_lines,"the distance between two parallel lines in the plane is the minimum distance between any two points. because the lines are parallel, the perpendicular distance between them is a constant, so it does not matter which point is chosen to measure the distance. given the equations of two non-vertical parallel lines the distance between the two lines is the distance between the two intersection points of these lines with the perpendicular line this distance can be found by first solving the linear systems and to get the coordinates of the intersection points. the solutions to the linear systems are the points and the distance between the points is which reduces to when the lines are given by "
124,124,Distance from a point to a line,2,https://en.wikipedia.org/wiki/Distance_from_a_point_to_a_line,"in euclidean geometry, the distance from a point to a line is the shortest distance from a given point to any point on an infinite straight line. it is the perpendicular distance of the point to the line, the length of the line segment which joins the point to nearest point on the line. the formula for calculating it can be derived and expressed in several ways. knowing the distance from a point to a line can be useful in various situations—for example, finding the shortest distance to reach a road, quantifying the scatter on a graph, etc. in deming regression, a type of linear curve fitting, if the dependent and independent variables have equal variance this results in orthogonal regression in which the degree of imperfection of the fit is measured for each data point as the perpendicular distance of the point from the regression line. in the case of a line in the plane given by the equation ax + by + c =  , where a, b and c are real constants with a and b not both zero, the distance from the line to a point (x , y ) is : p.   the point on this line which is closest to (x , y ) has coordinates: horizontal and vertical lines in the general equation of a line, ax + by + c =  , a and b cannot both be zero unless c is also zero, in which case the equation does not define a line. if a =   and b ≠  , the line is horizontal and has equation y = −c/b. the distance from (x , y ) to this line is measured along a vertical line segment of length |y  − (−c/b)| = |by  + c|/|b| in accordance with the formula. similarly, for vertical lines (b =  ) the distance between the same point and the line is |ax  + c|/|a|, as measured along a horizontal line segment. if the line passes through two points p  = (x , y ) and p  = (x , y ) then the distance of (x , y ) from the line is: the denominator of this expression is the distance between p  and p . the numerator is twice the area of the triangle with its vertices at the three points, (x , y ), p  and p . see: area of a triangle § using coordinates. the expression is equivalent to h =  a/b, which can be obtained by rearranging the standard formula for the area of a triangle: a =  /  bh, where b is the length of a side, and h is the perpendicular height from the opposite vertex. if the line passes through the point p = (px, py) with angle θ, then the distance of some point (x , y ) to the line is this proof is valid only if the line is neither vertical nor horizontal, that is, we assume that neither a nor b in the equation of the line is zero. "
125,125,distance of a point from a plane,1,https://en.wikipedia.org/wiki/Distance_from_a_point_to_a_plane,"in euclidean space, the distance from a point to a plane is the distance between a given point and its orthogonal projection on the plane, the perpendicular distance to the nearest point on the plane. it can be found starting with a change of variables that moves the origin to coincide with the given point then finding the point on the shifted plane a x + b y + c z = d {\displaystyle ax+by+cz=d} that is closest to the origin. the resulting point has cartesian coordinates ( x , y , z ) {\displaystyle (x,y,z)} : the distance between the origin and the point ( x , y , z ) {\displaystyle (x,y,z)} is x   + y   + z   {\displaystyle {\sqrt {x^{ }+y^{ }+z^{ }}}} . suppose we wish to find the nearest point on a plane to the point ( x   , y   , z   {\displaystyle x_{ },y_{ },z_{ }} ), where the plane is given by a x + b y + c z = d {\displaystyle ax+by+cz=d} . we define x = x − x   {\displaystyle x=x-x_{ }} , y = y − y   {\displaystyle y=y-y_{ }} , z = z − z   {\displaystyle z=z-z_{ }} , and d = d − a x   − b y   − c z   {\displaystyle d=d-ax_{ }-by_{ }-cz_{ }} , to obtain a x + b y + c z = d {\displaystyle ax+by+cz=d} as the plane expressed in terms of the transformed variables. now the problem has become one of finding the nearest point on this plane to the origin, and its distance from the origin. the point on the plane in terms of the original coordinates can be found from this point using the above relationships between x {\displaystyle x} and x {\displaystyle x} , between y {\displaystyle y} and y {\displaystyle y} , and between z {\displaystyle z} and z {\displaystyle z} ; the distance in terms of the original coordinates is the same as the distance in terms of the revised coordinates. the formula for the closest point to the origin may be expressed more succinctly using notation from linear algebra. the expression a x + b y + c z {\displaystyle ax+by+cz} in the definition of a plane is a dot product ( a , b , c ) ⋅ ( x , y , z ) {\displaystyle (a,b,c)\cdot (x,y,z)} , and the expression a   + b   + c   {\displaystyle a^{ }+b^{ }+c^{ }} appearing in the solution is the squared norm | ( a , b , c ) |   {\displaystyle |(a,b,c)|^{ }} . thus, if v = ( a , b , c ) {\displaystyle \mathbf {v} =(a,b,c)} is a given vector, the plane may be described as the set of vectors w {\displaystyle \mathbf {w} } for which v ⋅ w = d {\displaystyle \mathbf {v} \cdot \mathbf {w} =d} and the closest point on this plane is the vector the euclidean distance from the origin to the plane is the norm of this point, in either the coordinate or vector formulations, one may verify that the given point lies on the given plane by plugging the point into the equation of the plane. to see that it is the closest point to the origin on the plane, observe that p {\displaystyle \mathbf {p} } is a scalar multiple of the vector v {\displaystyle \mathbf {v} } defining the plane, and is therefore orthogonal to the plane. thus, if q {\displaystyle \mathbf {q} } is any point on the plane other than p {\displaystyle \mathbf {p} } itself, then the line segments from the origin to p {\displaystyle \mathbf {p} } and from p {\displaystyle \mathbf {p} } to q {\displaystyle \mathbf {q} } form a right triangle, and by the pythagorean theorem the distance from the origin to q {\displaystyle q} is since | p − q |   {\displaystyle |\mathbf {p} -\mathbf {q} |^{ }} must be a positive number, this distance is greater than | p | {\displaystyle |\mathbf {p} |} , the distance from the origin to p {\displaystyle \mathbf {p} } . alternatively, it is possible to rewrite the equation of the plane using dot products with p {\displaystyle \mathbf {p} } in place of the original dot product with v {\displaystyle \mathbf {v} } (because these two vectors are scalar multiples of each other) after which the fact that p {\displaystyle \mathbf {p} } is the closest point becomes an immediate consequence of the cauchy–schwarz inequality. "
126,126,Division algorithm,1,https://en.wikipedia.org/wiki/Division_algorithm,"a division algorithm is an algorithm which, given two integers n and d, computes their quotient and/or remainder, the result of euclidean division. some are applied by hand, while others are employed by digital circuit designs and software. division algorithms fall into two main categories: slow division and fast division. slow division algorithms produce one digit of the final quotient per iteration. examples of slow division include restoring, non-performing restoring, non-restoring, and srt division. fast division methods start with a close approximation to the final quotient and produce twice as many digits of the final quotient on each iteration. newton–raphson and goldschmidt algorithms fall into this category. variants of these algorithms allow using fast multiplication algorithms. it results that, for large integers, the computer time needed for a division is the same, up to a constant factor, as the time needed for a multiplication, whichever multiplication algorithm is used. discussion will refer to the form n / d = ( q , r ) {\displaystyle n/d=(q,r)} , where is the input, and is the output. the simplest division algorithm, historically incorporated into a greatest common divisor algorithm presented in euclid's elements, book vii, proposition  , finds the remainder given two positive integers using only subtractions and comparisons: the proof that the quotient and remainder exist and are unique (described at euclidean division) gives rise to a complete division algorithm using additions, subtractions, and comparisons: this procedure always produces r ≥  . although very simple, it takes ω(q) steps, and so is exponentially slower than even slow division algorithms like long division. it is useful if q is known to be small (being an output-sensitive algorithm), and can serve as an executable specification. long division is the standard algorithm used for pen-and-paper division of multi-digit numbers expressed in decimal notation. it shifts gradually from the left to the right end of the dividend, subtracting the largest possible multiple of the divisor (at the digit level) at each stage; the multiples then become the digits of the quotient, and the final difference is then the remainder. "
127,127,Division By Zero,1,https://en.wikipedia.org/wiki/Division_by_zero,"in mathematics, division by zero is division where the divisor (denominator) is zero. such a division can be formally expressed as a   {\textstyle {\tfrac {a}{ }}} , where a is the dividend (numerator). in ordinary arithmetic, the expression has no meaning, as there is no number that, when multiplied by  , gives a (assuming a ≠   {\textstyle a\neq  } ); thus, division by zero is undefined. since any number multiplied by zero is zero, the expression     {\displaystyle {\tfrac { }{ }}} is also undefined; when it is the form of a limit, it is an indeterminate form. historically, one of the earliest recorded references to the mathematical impossibility of assigning a value to a   {\textstyle {\tfrac {a}{ }}} is contained in anglo-irish philosopher george berkeley's criticism of infinitesimal calculus in      in the analyst (""ghosts of departed quantities""). there are mathematical structures in which a   {\textstyle {\tfrac {a}{ }}} is defined for some a such as in the riemann sphere (a model of the extended complex plane) and the projectively extended real line; however, such structures do not satisfy every ordinary rule of arithmetic (the field axioms). in computing, a program error may result from an attempt to divide by zero. depending on the programming environment and the type of number (e.g., floating point, integer) being divided by zero, it may generate positive or negative infinity by the ieee     floating point standard, generate an exception, generate an error message, cause the program to terminate, result in a special not-a-number value, or crash. when division is explained at the elementary arithmetic level, it is often considered as splitting a set of objects into equal parts. as an example, consider having ten cookies, and these cookies are to be distributed equally to five people at a table. each person would receive      =   {\displaystyle {\tfrac {  }{ }}= } cookies. similarly, if there are ten cookies, and only one person at the table, that person would receive      =    {\displaystyle {\tfrac {  }{ }}=  } cookies. so, for dividing by zero, what is the number of cookies that each person receives when    cookies are evenly distributed among   people at a table? certain words can be pinpointed in the question to highlight the problem. the problem with this question is the ""when"". there is no way to distribute    cookies to nobody. therefore,      {\displaystyle {\tfrac {  }{ }}} —at least in elementary arithmetic—is said to be either meaningless or undefined. if there are, say,   cookies and   people, the problem is in ""evenly distribute"". in any integer partition of   things into   parts, either one of the parts of the partition will have more elements than the other or there will be a remainder (written as  /  =   r ). or, the problem with   cookies and   people can be solved by cutting one cookie in half, which introduces the idea of fractions ( /  =  + / ) . the problem with   cookies and   people, on the other hand, cannot be solved in any way that preserves the meaning of ""divides"". in elementary algebra, another way of looking at division by zero is that division can always be checked using multiplication. considering the   /  example above, setting x =   / , if x equals ten divided by zero, then x times zero equals ten, but there is no x that, when multiplied by zero, gives ten (or any number other than zero). if, instead of x =   / , x =  / , then every x satisfies the question ""what number x, multiplied by zero, gives zero?"" the brāhmasphuṭasiddhānta of brahmagupta (c.    –   ) is the earliest text to treat zero as a number in its own right and to define operations involving zero. the author could not explain division by zero in his texts: his definition can be easily proven to lead to algebraic absurdities. according to brahmagupta, a positive or negative number when divided by zero is a fraction with the zero as denominator. zero divided by a negative or positive number is either zero or is expressed as a fraction with zero as numerator and the finite quantity as denominator. zero divided by zero is zero.in    , mahāvīra unsuccessfully tried to correct brahmagupta's mistake in his book in ganita sara samgraha: ""a number remains unchanged when divided by zero."" "
128,128,Domain of a Function,1,https://en.wikipedia.org/wiki/Domain_of_a_function,"in mathematics, the domain of a function is the set of inputs accepted by the function. it is sometimes denoted by dom ⁡ ( f ) {\displaystyle \operatorname {dom} (f)} , where f is the function. more precisely, given a function f : x → y {\displaystyle f\colon x\to y} , the domain of f is x. note that in modern mathematical language, the domain is part of the definition of a function rather than a property of it. in the special case that x and y are both subsets of r {\displaystyle \mathbb {r} } , the function f can be graphed in the cartesian coordinate system. in this case, the domain is represented on the x-axis of the graph, as the projection of the graph of the function onto the x-axis. for a function f : x → y {\displaystyle f\colon x\to y} , the set y is called the codomain, and the set of values attained by the function (which is a subset of y) is called its range or image. any function can be restricted to a subset of its domain. the restriction of f : x → y {\displaystyle f\colon x\to y} to a {\displaystyle a} , where a ⊆ x {\displaystyle a\subseteq x} , is written as f | a : a → y {\displaystyle \left.f\right|_{a}\colon a\to y} . if a real function f is given by a formula, it may be not defined for some values of the variable. in this case, it is a partial function, and the set of real numbers on which the formula can be evaluated to a real number is called the natural domain or domain of definition of f. in many contexts, a partial function is called simply a function, and its natural domain is called simply its domain. the word ""domain"" is used with other related meanings in some areas of mathematics. in topology, a domain is a connected open set. in real and complex analysis, a domain is an open connected subset of a real or complex vector space. in the study of partial differential equations, a domain is the open connected subset of the euclidean space r n {\displaystyle \mathbb {r} ^{n}} where a problem is posed (i.e., where the unknown function(s) are defined). for example, it is sometimes convenient in set theory to permit the domain of a function to be a proper class x, in which case there is formally no such thing as a triple (x, y, g). with such a definition, functions do not have a domain, although some authors still use it informally after introducing a function in the form f: x → y. "
129,129,dot and cross products,1,https://en.wikipedia.org/wiki/Dot_product,"in mathematics, the dot product or scalar product[note  ] is an algebraic operation that takes two equal-length sequences of numbers (usually coordinate vectors), and returns a single number. in euclidean geometry, the dot product of the cartesian coordinates of two vectors is widely used. it is often called ""the"" inner product (or rarely projection product) of euclidean space, even though it is not the only inner product that can be defined on euclidean space (see inner product space for more). algebraically, the dot product is the sum of the products of the corresponding entries of the two sequences of numbers. geometrically, it is the product of the euclidean magnitudes of the two vectors and the cosine of the angle between them. these definitions are equivalent when using cartesian coordinates. in modern geometry, euclidean spaces are often defined by using vector spaces. in this case, the dot product is used for defining lengths (the length of a vector is the square root of the dot product of the vector by itself) and angles (the cosine of the angle of two vectors is the quotient of their dot product by the product of their lengths). the name ""dot product"" is derived from the centered dot "" · "" that is often used to designate this operation; the alternative name ""scalar product"" emphasizes that the result is a scalar, rather than a vector, as is the case for the vector product in three-dimensional space. the dot product may be defined algebraically or geometrically. the geometric definition is based on the notions of angle and distance (magnitude of vectors). the equivalence of these two definitions relies on having a cartesian coordinate system for euclidean space. in modern presentations of euclidean geometry, the points of space are defined in terms of their cartesian coordinates, and euclidean space itself is commonly identified with the real coordinate space rn. in such a presentation, the notions of length and angles are defined by means of the dot product. the length of a vector is defined as the square root of the dot product of the vector by itself, and the cosine of the (non oriented) angle of two vectors of length one is defined as their dot product. so the equivalence of the two definitions of the dot product is a part of the equivalence of the classical and the modern formulations of euclidean geometry. the dot product of two vectors a = [a , a , ..., an] and b = [b , b , ..., bn] is defined as: where σ denotes summation and n is the dimension of the vector space. for instance, in three-dimensional space, the dot product of vectors [ ,  , − ] and [ , − , − ] is: if vectors are identified with row matrices, the dot product can also be written as a matrix product where b t {\displaystyle \mathbf {\color {blue}b} ^{\mathsf {t}}} denotes the transpose of b {\displaystyle \mathbf {\color {blue}b} } . expressing the above example in this way, a   ×   matrix (row vector) is multiplied by a   ×   matrix (column vector) to get a   ×   matrix that is identified with its unique entry: "
130,130,dynamical system,3,https://en.wikipedia.org/wiki/Dynamical_system,"in mathematics, a dynamical system is a system in which a function describes the time dependence of a point in an ambient space. examples include the mathematical models that describe the swinging of a clock pendulum, the flow of water in a pipe, and the number of fish each springtime in a lake. the most general definition unifies several concepts in mathematics such as ordinary differential equations and ergodic theory by allowing different choices of the space and how time is measured. time can be measured by integers, by real or complex numbers or can be a more general algebraic object, losing the memory of its physical origin, and the space may be a manifold or simply a set, without the need of a smooth space-time structure defined on it. at any given time, a dynamical system has a state representing a point in an appropriate state space. this state is often given by a tuple of real numbers or by a vector in a geometrical manifold. the evolution rule of the dynamical system is a function that describes what future states follow from the current state. often the function is deterministic, that is, for a given time interval only one future state follows from the current state. however, some systems are stochastic, in that random events also affect the evolution of the state variables. in physics, a dynamical system is described as a ""particle or ensemble of particles whose state varies over time and thus obeys differential equations involving time derivatives"". in order to make a prediction about the system's future behavior, an analytical solution of such equations or their integration over time through computer simulation is realized. the study of dynamical systems is the focus of dynamical systems theory, which has applications to a wide variety of fields such as mathematics, physics, biology, chemistry, engineering, economics, history, and medicine. dynamical systems are a fundamental part of chaos theory, logistic map dynamics, bifurcation theory, the self-assembly and self-organization processes, and the edge of chaos concept. the concept of a dynamical system has its origins in newtonian mechanics. there, as in other natural sciences and engineering disciplines, the evolution rule of dynamical systems is an implicit relation that gives the state of the system for only a short time into the future. (the relation is either a differential equation, difference equation or other time scale.) to determine the state for all future times requires iterating the relation many times—each advancing time a small step. the iteration procedure is referred to as solving the system or integrating the system. if the system can be solved, given an initial point it is possible to determine all its future positions, a collection of points known as a trajectory or orbit. before the advent of computers, finding an orbit required sophisticated mathematical techniques and could be accomplished only for a small class of dynamical systems. numerical methods implemented on electronic computing machines have simplified the task of determining the orbits of a dynamical system. for simple dynamical systems, knowing the trajectory is often sufficient, but most dynamical systems are too complicated to be understood in terms of individual trajectories. the difficulties arise because: many people regard french mathematician henri poincaré as the founder of dynamical systems. poincaré published two now classical monographs, ""new methods of celestial mechanics"" (    –    ) and ""lectures on celestial mechanics"" (    –    ). in them, he successfully applied the results of their research to the problem of the motion of three bodies and studied in detail the behavior of solutions (frequency, stability, asymptotic, and so on). these papers included the poincaré recurrence theorem, which states that certain systems will, after a sufficiently long but finite time, return to a state very close to the initial state. aleksandr lyapunov developed many important approximation methods. his methods, which he developed in     , make it possible to define the stability of sets of ordinary differential equations. he created the modern theory of the stability of a dynamical system. in     , george david birkhoff proved poincaré's ""last geometric theorem"", a special case of the three-body problem, a result that made him world-famous. in     , he published his dynamical systems. birkhoff's most durable result has been his      discovery of what is now called the ergodic theorem. combining insights from physics on the ergodic hypothesis with measure theory, this theorem solved, at least in principle, a fundamental problem of statistical mechanics. the ergodic theorem has also had repercussions for dynamics. "
131,131,Napier's Constant,2,https://en.wikipedia.org/wiki/E_(mathematical_constant),"the number e, also known as euler's number, is a mathematical constant approximately equal to  .      which can be characterized in many ways. it is the base of the natural logarithms. it is the limit of (  +  /n)n as n approaches infinity, an expression that arises in the study of compound interest. it can also be calculated as the sum of the infinite series it is also the unique positive number a such that the graph of the function y = ax has a slope of   at x =  . the (natural) exponential function f(x) = ex is the unique function f that equals its own derivative and satisfies the equation f( ) =  ; hence one can also define e as f( ). the natural logarithm, or logarithm to base e, is the inverse function to the natural exponential function. the natural logarithm of a number k >   can be defined directly as the area under the curve y =  /x between x =   and x = k, in which case e is the value of k for which this area equals one (see image). there are various other characterizations. e is sometimes called euler's number (not to be confused with euler's constant γ {\displaystyle \gamma } ), after the swiss mathematician leonhard euler, or napier's constant. the constant was discovered by the swiss mathematician jacob bernoulli while studying compound interest. the number e is of great importance in mathematics, [page needed] alongside  ,  , π, and i. all five appear in one formulation of euler's identity, and play important and recurring roles across mathematics. like the constant π, e is irrational (that is, it cannot be represented as a ratio of integers) and transcendental (that is, it is not a root of any non-zero polynomial with rational coefficients). to    decimal places the value of e is: the first references to the constant were published in      in the table of an appendix of a work on logarithms by john napier. however, this did not contain the constant itself, but simply a list of logarithms calculated from the constant. it is assumed that the table was written by william oughtred. the discovery of the constant itself is credited to jacob bernoulli in     , who attempted to find the value of the following expression (which is equal to e): the first known use of the constant, represented by the letter b, was in correspondence from gottfried leibniz to christiaan huygens in      and     . leonhard euler introduced the letter e as the base for natural logarithms, writing in a letter to christian goldbach on    november     . euler started to use the letter e for the constant in      or     , in an unpublished paper on explosive forces in cannons, while the first appearance of e in a publication was in euler's mechanica (    ). although some researchers used the letter c in the subsequent years, the letter e was more common and eventually became standard.[citation needed] in mathematics, the most common typographical convention is to typeset the constant as ""e"", in italics, although sometimes ""e"" in roman is used. on the other hand, the iso      - :     standard recommends typesetting constants in an upright style.[citation needed] jacob bernoulli discovered this constant in     , while studying a question about compound interest: "
132,132,Eccentricity,1,https://en.wikipedia.org/wiki/Eccentricity_(mathematics),"in mathematics, the eccentricity of a conic section is a non-negative real number that uniquely characterizes its shape. more formally two conic sections are similar if and only if they have the same eccentricity. one can think of the eccentricity as a measure of how much a conic section deviates from being circular. in particular: any conic section can be defined as the locus of points whose distances to a point (the focus) and a line (the directrix) are in a constant ratio. that ratio is called the eccentricity, commonly denoted as e. the eccentricity can also be defined in terms of the intersection of a plane and a double-napped cone associated with the conic section. if the cone is oriented with its axis vertical, the eccentricity is where β is the angle between the plane and the horizontal and α is the angle between the cone's slant generator and the horizontal. for β =   {\displaystyle \beta = } the plane section is a circle, for β = α {\displaystyle \beta =\alpha } a parabola. (the plane must not meet the vertex of the cone.) the linear eccentricity of an ellipse or hyperbola, denoted c (or sometimes f or e), is the distance between its center and either of its two foci. the eccentricity can be defined as the ratio of the linear eccentricity to the semimajor axis a: that is, e = c a {\displaystyle e={\frac {c}{a}}} (lacking a center, the linear eccentricity for parabolas is not defined). the eccentricity is sometimes called the first eccentricity to distinguish it from the second eccentricity and third eccentricity defined for ellipses (see below). the eccentricity is also sometimes called the numerical eccentricity. in the case of ellipses and hyperbolas the linear eccentricity is sometimes called the half-focal separation. three notational conventions are in common use: "
133,133,Eigen function,0,https://en.wikipedia.org/wiki/Eigenfunction,"in mathematics, an eigenfunction of a linear operator d defined on some function space is any non-zero function f {\displaystyle f} in that space that, when acted upon by d, is only multiplied by some scaling factor called an eigenvalue. as an equation, this condition can be written as an eigenfunction is a type of eigenvector. in general, an eigenvector of a linear operator d defined on some vector space is a nonzero vector in the domain of d that, when d acts upon it, is simply scaled by some scalar value called an eigenvalue. in the special case where d is defined on a function space, the eigenvectors are referred to as eigenfunctions. that is, a function f is an eigenfunction of d if it satisfies the equation ( )where λ is a scalar. the solutions to equation ( ) may also be subject to boundary conditions. because of the boundary conditions, the possible values of λ are generally limited, for example to a discrete set λ , λ , … or to a continuous set over some range. the set of all possible eigenvalues of d is sometimes called its spectrum, which may be discrete, continuous, or a combination of both. each value of λ corresponds to one or more eigenfunctions. if multiple linearly independent eigenfunctions have the same eigenvalue, the eigenvalue is said to be degenerate and the maximum number of linearly independent eigenfunctions associated with the same eigenvalue is the eigenvalue's degree of degeneracy or geometric multiplicity. "
134,134,Elementary Event,1,https://en.wikipedia.org/wiki/Elementary_event,"in probability theory, an elementary event, also called an atomic event or sample point, is an event which contains only a single outcome in the sample space. using set theory terminology, an elementary event is a singleton. elementary events and their corresponding outcomes are often written interchangeably for simplicity, as such an event corresponding to precisely one outcome. the following are examples of elementary events: elementary events may occur with probabilities that are between zero and one (inclusively). in a discrete probability distribution whose sample space is finite, each elementary event is assigned a particular probability. in contrast, in a continuous distribution, individual elementary events must all have a probability of zero because there are infinitely many of them— then non-zero probabilities can only be assigned to non-elementary events. some ""mixed"" distributions contain both stretches of continuous elementary events and some discrete elementary events; the discrete elementary events in such distributions can be called atoms or atomic events and can have non-zero probabilities. under the measure-theoretic definition of a probability space, the probability of an elementary event need not even be defined. in particular, the set of events on which probability is defined may be some σ-algebra on s {\displaystyle s} and not necessarily the full power set. this probability-related article is a stub. you can help wikipedia by expanding it.this statistics-related article is a stub. you can help wikipedia by expanding it."
135,135,Elementary matrix,1,https://en.wikipedia.org/wiki/Elementary_matrix,"in mathematics, an elementary matrix is a matrix which differs from the identity matrix by one single elementary row operation. the elementary matrices generate the general linear group gln(f) when f is a field. left multiplication (pre-multiplication) by an elementary matrix represents elementary row operations, while right multiplication (post-multiplication) represents elementary column operations. elementary row operations are used in gaussian elimination to reduce a matrix to row echelon form. they are also used in gauss–jordan elimination to further reduce the matrix to reduced row echelon form. there are three types of elementary matrices, which correspond to three types of row operations (respectively, column operations): if e is an elementary matrix, as described below, to apply the elementary row operation to a matrix a, one multiplies a by the elementary matrix on the left, ea. the elementary matrix for any row operation is obtained by executing the operation on the identity matrix. this fact can be understood as an instance of the yoneda lemma applied to the category of matrices. the first type of row operation on a matrix a switches all matrix elements on row i with their counterparts on row j. the corresponding elementary matrix is obtained by swapping row i and row j of the identity matrix. so tija is the matrix produced by exchanging row i and row j of a. coefficient wise, the matrix t i , j {\displaystyle t_{i,j}} is defined by : the next type of row operation on a matrix a multiplies all elements on row i by m where m is a non-zero scalar (usually a real number). the corresponding elementary matrix is a diagonal matrix, with diagonal entries   everywhere except in the ith position, where it is m. so di(m)a is the matrix produced from a by multiplying row i by m. coefficient wise, the d i ( m ) {\displaystyle d_{i}(m)} matrix is defined by : "
136,136,Ellipse,2,https://en.wikipedia.org/wiki/Ellipse,"in mathematics, an ellipse is a plane curve surrounding two focal points, such that for all points on the curve, the sum of the two distances to the focal points is a constant. as such, it generalizes a circle, which is the special type of ellipse in which the two focal points are the same. the elongation of an ellipse is measured by its eccentricity e {\displaystyle e} , a number ranging from e =   {\displaystyle e= } (the limiting case of a circle) to e =   {\displaystyle e= } (the limiting case of infinite elongation, no longer an ellipse but a parabola). an ellipse has a simple algebraic solution for its area, but only approximations for its perimeter (also known as circumference), for which integration is required to obtain an exact solution. analytically, the equation of a standard ellipse centered at the origin with width   a {\displaystyle  a} and height   b {\displaystyle  b} is: assuming a ≥ b {\displaystyle a\geq b} , the foci are ( ± c ,   ) {\displaystyle (\pm c, )} for c = a   − b   {\textstyle c={\sqrt {a^{ }-b^{ }}}} . the standard parametric equation is: ellipses are the closed type of conic section: a plane curve tracing the intersection of a cone with a plane (see figure). ellipses have many similarities with the other two forms of conic sections, parabolas and hyperbolas, both of which are open and unbounded. an angled cross section of a cylinder is also an ellipse. an ellipse may also be defined in terms of one focal point and a line outside the ellipse called the directrix: for all points on the ellipse, the ratio between the distance to the focus and the distance to the directrix is a constant. this constant ratio is the above-mentioned eccentricity: ellipses are common in physics, astronomy and engineering. for example, the orbit of each planet in the solar system is approximately an ellipse with the sun at one focus point (more precisely, the focus is the barycenter of the sun–planet pair). the same is true for moons orbiting planets and all other systems of two astronomical bodies. the shapes of planets and stars are often well described by ellipsoids. a circle viewed from a side angle looks like an ellipse: that is, the ellipse is the image of a circle under parallel or perspective projection. the ellipse is also the simplest lissajous figure formed when the horizontal and vertical motions are sinusoids with the same frequency: a similar effect leads to elliptical polarization of light in optics. the name, ἔλλειψις (élleipsis, ""omission""), was given by apollonius of perga in his conics. an ellipse can be defined geometrically as a set or locus of points in the euclidean plane: the midpoint c {\displaystyle c} of the line segment joining the foci is called the center of the ellipse. the line through the foci is called the major axis, and the line perpendicular to it through the center is the minor axis. the major axis intersects the ellipse at two vertices v   , v   {\displaystyle v_{ },v_{ }} , which have distance a {\displaystyle a} to the center. the distance c {\displaystyle c} of the foci to the center is called the focal distance or linear eccentricity. the quotient e = c a {\displaystyle e={\tfrac {c}{a}}} is the eccentricity. "
137,137,Ellipsoid,3,https://en.wikipedia.org/wiki/Ellipsoid,"an ellipsoid is a surface that may be obtained from a sphere by deforming it by means of directional scalings, or more generally, of an affine transformation. an ellipsoid is a quadric surface; that is, a surface that may be defined as the zero set of a polynomial of degree two in three variables. among quadric surfaces, an ellipsoid is characterized by either of the two following properties. every planar cross section is either an ellipse, or is empty, or is reduced to a single point (this explains the name, meaning ""ellipse-like""). it is bounded, which means that it may be enclosed in a sufficiently large sphere. an ellipsoid has three pairwise perpendicular axes of symmetry which intersect at a center of symmetry, called the center of the ellipsoid. the line segments that are delimited on the axes of symmetry by the ellipsoid are called the principal axes, or simply axes of the ellipsoid. if the three axes have different lengths, the ellipsoid is said to be triaxial or rarely scalene, and the axes are uniquely defined. if two of the axes have the same length, then the ellipsoid is an ellipsoid of revolution, also called a spheroid. in this case, the ellipsoid is invariant under a rotation around the third axis, and there are thus infinitely many ways of choosing the two perpendicular axes of the same length. if the third axis is shorter, the ellipsoid is an oblate spheroid; if it is longer, it is a prolate spheroid. if the three axes have the same length, the ellipsoid is a sphere. using a cartesian coordinate system in which the origin is the center of the ellipsoid and the coordinate axes are axes of the ellipsoid, the implicit equation of the ellipsoid has the standard form where a, b, c are positive real numbers. the points (a,  ,  ), ( , b,  ) and ( ,  , c) lie on the surface. the line segments from the origin to these points are called the principal semi-axes of the ellipsoid, because a, b, c are half the length of the principal axes. they correspond to the semi-major axis and semi-minor axis of an ellipse. if a = b > c, one has an oblate spheroid; if a = b < c, one has a prolate spheroid; if a = b = c, one has a sphere. the ellipsoid may be parameterized in several ways, which are simpler to express when the ellipsoid axes coincide with coordinate axes. a common choice is where "
138,138,Engel expansion,0,https://en.wikipedia.org/wiki/Engel_expansion,"the engel expansion of a positive real number x is the unique non-decreasing sequence of positive integers { a   , a   , a   , … } {\displaystyle \{a_{ },a_{ },a_{ },\dots \}} such that for instance, euler's constant e has the engel expansion corresponding to the infinite series rational numbers have a finite engel expansion, while irrational numbers have an infinite engel expansion. if x is rational, its engel expansion provides a representation of x as an egyptian fraction. engel expansions are named after friedrich engel, who studied them in     . an expansion analogous to an engel expansion, in which alternating terms are negative, is called a pierce expansion. kraaikamp & wu (    ) observe that an engel expansion can also be written as an ascending variant of a continued fraction: they claim that ascending continued fractions such as this have been studied as early as fibonacci's liber abaci (    ). this claim appears to refer to fibonacci's compound fraction notation in which a sequence of numerators and denominators sharing the same fraction bar represents an ascending continued fraction: if such a notation has all numerators   or  , as occurs in several instances in liber abaci, the result is an engel expansion. however, engel expansion as a general technique does not seem to be described by fibonacci. to find the engel expansion of x, let and "
139,139,equiprobable,1,https://en.wikipedia.org/wiki/Equiprobability,"equiprobability is a property for a collection of events that each have the same probability of occurring. in statistics and probability theory it is applied in the discrete uniform distribution and the equidistribution theorem for rational numbers. if there are n {\textstyle n} events under consideration, the probability of each occurring is   n . {\textstyle {\frac { }{n}}.} in philosophy it corresponds to a concept that allows one to assign equal probabilities to outcomes when they are judged to be equipossible or to be ""equally likely"" in some sense. the best-known formulation of the rule is laplace's principle of indifference (or principle of insufficient reason), which states that, when ""we have no other information than"" that exactly n {\displaystyle n} mutually exclusive events can occur, we are justified in assigning each the probability   n . {\textstyle {\frac { }{n}}.} this subjective assignment of probabilities is especially justified for situations such as rolling dice and lotteries since these experiments carry a symmetry structure, and one's state of knowledge must clearly be invariant under this symmetry. a similar argument could lead to the seemingly absurd conclusion that the sun is as likely to rise as to not rise tomorrow morning. however, the conclusion that the sun is equally likely to rise as it is to not rise is only absurd when additional information is known, such as the laws of gravity and the sun's history. similar applications of the concept are effectively instances of circular reasoning, with ""equally likely"" events being assigned equal probabilities, which means in turn that they are equally likely. despite this, the notion remains useful in probabilistic and statistical modeling. in bayesian probability, one needs to establish prior probabilities for the various hypotheses before applying bayes' theorem. one procedure is to assume that these prior probabilities have some symmetry which is typical of the experiment, and then assign a prior which is proportional to the haar measure for the symmetry group: this generalization of equiprobability is known as the principle of transformation groups and leads to misuse of equiprobability as a model for incertitude. "
140,140,Equivalence Relation,2,https://en.wikipedia.org/wiki/Equivalence_relation,"in mathematics, an equivalence relation is a binary relation that is reflexive, symmetric and transitive. the relation is equal to is the canonical example of an equivalence relation. each equivalence relation provides a partition of the underlying set into disjoint equivalence classes. two elements of the given set are equivalent to each other if and only if they belong to the same equivalence class. various notations are used in the literature to denote that two elements a {\displaystyle a} and b {\displaystyle b} of a set are equivalent with respect to an equivalence relation r ; {\displaystyle r;} the most common are "" a ∼ b {\displaystyle a\sim b} "" and ""a ≡ b"", which are used when r {\displaystyle r} is implicit, and variations of "" a ∼ r b {\displaystyle a\sim _{r}b} "", ""a ≡r b"", or "" a r ⁡ b {\displaystyle {a\mathop {r} b}} "" to specify r {\displaystyle r} explicitly. non-equivalence may be written ""a ≁ b"" or "" a ≢ b {\displaystyle a\not \equiv b} "". a binary relation ∼ {\displaystyle \,\sim \,} on a set x {\displaystyle x} is said to be an equivalence relation, if and only if it is reflexive, symmetric and transitive. that is, for all a , b , {\displaystyle a,b,} and c {\displaystyle c} in x : {\displaystyle x:} x {\displaystyle x} together with the relation ∼ {\displaystyle \,\sim \,} is called a setoid. the equivalence class of a {\displaystyle a} under ∼ , {\displaystyle \,\sim ,} denoted [ a ] , {\displaystyle [a],} is defined as [ a ] = { x ∈ x : x ∼ a } . {\displaystyle [a]=\{x\in x:x\sim a\}.} on the set x = { a , b , c } {\displaystyle x=\{a,b,c\}} , the relation r = { ( a , a ) , ( b , b ) , ( c , c ) , ( b , c ) , ( c , b ) } {\displaystyle r=\{(a,a),(b,b),(c,c),(b,c),(c,b)\}} is an equivalence relation. the following sets are equivalence classes of this relation: the set of all equivalence classes for r {\displaystyle r} is { { a } , { b , c } } . {\displaystyle \{\{a\},\{b,c\}\}.} this set is a partition of the set x {\displaystyle x} with respect to r {\displaystyle r} . the following relations are all equivalence relations: if ∼ {\displaystyle \,\sim \,} is an equivalence relation on x , {\displaystyle x,} and p ( x ) {\displaystyle p(x)} is a property of elements of x , {\displaystyle x,} such that whenever x ∼ y , {\displaystyle x\sim y,} p ( x ) {\displaystyle p(x)} is true if p ( y ) {\displaystyle p(y)} is true, then the property p {\displaystyle p} is said to be well-defined or a class invariant under the relation ∼ . {\displaystyle \,\sim .} a frequent particular case occurs when f {\displaystyle f} is a function from x {\displaystyle x} to another set y ; {\displaystyle y;} if x   ∼ x   {\displaystyle x_{ }\sim x_{ }} implies f ( x   ) = f ( x   ) {\displaystyle f\left(x_{ }\right)=f\left(x_{ }\right)} then f {\displaystyle f} is said to be a morphism for ∼ , {\displaystyle \,\sim ,} a class invariant under ∼ , {\displaystyle \,\sim ,} or simply invariant under ∼ . {\displaystyle \,\sim .} this occurs, e.g. in the character theory of finite groups. the latter case with the function f {\displaystyle f} can be expressed by a commutative triangle. see also invariant. some authors use ""compatible with ∼ {\displaystyle \,\sim } "" or just ""respects ∼ {\displaystyle \,\sim } "" instead of ""invariant under ∼ {\displaystyle \,\sim } "". "
141,141,Euclid Lemma,2,https://en.wikipedia.org/wiki/Euclid%27s_lemma," in algebra and number theory, euclid's lemma is a lemma that captures a fundamental property of prime numbers, namely:[note  ] euclid's lemma — if a prime p divides the product ab of two integers a and b, then p must divide at least one of those integers a and b. for example, if p =   , a =    , b =    , then ab =     ×     =      , and since this is divisible by   , the lemma implies that one or both of     or     must be as well. in fact,     =    ×  . if the premise of the lemma does not hold, i.e., p is a composite number, its consequent may be either true or false. for example, in the case of p =   , a =  , b =   , composite number    divides ab =   ×    =   , but    divides neither   nor   . this property is the key in the proof of the fundamental theorem of arithmetic.[note  ] it is used to define prime elements, a generalization of prime numbers to arbitrary commutative rings. euclid's lemma shows that in the integers irreducible elements are also prime elements. the proof uses induction so it does not apply to all integral domains. let p {\displaystyle p} be a prime number, and assume p {\displaystyle p} divides the product of two integers a {\displaystyle a} and b {\displaystyle b} . in symbols, this is written p ∣ a b {\displaystyle p\mid ab} . its negation, p {\displaystyle p} does not divide a b {\displaystyle ab} , is written p ∤ a b {\displaystyle p\nmid ab} . then p ∣ a {\displaystyle p\mid a} or p ∣ b {\displaystyle p\mid b} (or both). equivalent statements are: euclid's lemma can be generalized from prime numbers to any integers: theorem — if n ∣ a b {\displaystyle n\mid ab} , and n {\displaystyle n} is relatively prime to a {\displaystyle a} , then n ∣ b {\displaystyle n\mid b} . this is a generalization because if n {\displaystyle n} is prime, either the lemma first appears as proposition    in book vii of euclid's elements. it is included in practically every book that covers elementary number theory. "
142,142,conditional dependence,1,https://en.wikipedia.org/wiki/Euclidean_geometry,"euclidean geometry is a mathematical system attributed to ancient greek mathematician euclid, which he described in his textbook on geometry: the elements. euclid's approach consists in assuming a small set of intuitively appealing axioms, and deducing many other propositions (theorems) from these. although many of euclid's results had been stated earlier, euclid was the first to organize these propositions into a logical system in which each result is proved from axioms and previously proved theorems. the elements begins with plane geometry, still taught in secondary school (high school) as the first axiomatic system and the first examples of mathematical proofs. it goes on to the solid geometry of three dimensions. much of the elements states results of what are now called algebra and number theory, explained in geometrical language. for more than two thousand years, the adjective ""euclidean"" was unnecessary because no other sort of geometry had been conceived. euclid's axioms seemed so intuitively obvious (with the possible exception of the parallel postulate) that any theorem proved from them was deemed true in an absolute, often metaphysical, sense. today, however, many other self-consistent non-euclidean geometries are known, the first ones having been discovered in the early   th century. an implication of albert einstein's theory of general relativity is that physical space itself is not euclidean, and euclidean space is a good approximation for it only over short distances (relative to the strength of the gravitational field). euclidean geometry is an example of synthetic geometry, in that it proceeds logically from axioms describing basic properties of geometric objects such as points and lines, to propositions about those objects. this is in contrast to analytic geometry, introduced almost  ,    years later by rené descartes, which uses coordinates to express geometric properties as algebraic formulas. the elements is mainly a systematization of earlier knowledge of geometry. its improvement over earlier treatments was rapidly recognized, with the result that there was little interest in preserving the earlier ones, and they are now nearly all lost. there are    books in the elements: books i–iv and vi discuss plane geometry. many results about plane figures are proved, for example, ""in any triangle, two angles taken together in any manner are less than two right angles."" (book i proposition   ) and the pythagorean theorem ""in right-angled triangles the square on the side subtending the right angle is equal to the squares on the sides containing the right angle."" (book i, proposition   ) books v and vii–x deal with number theory, with numbers treated geometrically as lengths of line segments or areas of surface regions. notions such as prime numbers and rational and irrational numbers are introduced. it is proved that there are infinitely many prime numbers. books xi–xiii concern solid geometry. a typical result is the  :  ratio between the volume of a cone and a cylinder with the same height and base. the platonic solids are constructed. euclidean geometry is an axiomatic system, in which all theorems (""true statements"") are derived from a small number of simple axioms. until the advent of non-euclidean geometry, these axioms were considered to be obviously true in the physical world, so that all the theorems would be equally true. however, euclid's reasoning from assumptions to conclusions remains valid independent of their physical reality. "
143,143,Euclidean space,3,https://en.wikipedia.org/wiki/Euclidean_space,"euclidean space is the fundamental space of classical geometry. originally, it was the three-dimensional space of euclidean geometry, but in modern mathematics there are euclidean spaces of any nonnegative integer dimension, including the three-dimensional space and the euclidean plane (dimension two). it was introduced by the ancient greek mathematician euclid of alexandria, and the qualifier euclidean is used to distinguish it from other spaces that were later discovered in physics and modern mathematics. ancient greek geometers introduced euclidean space for modeling the physical universe. their great innovation was to prove all properties of the space as theorems by starting from a few fundamental properties, called postulates, which either were considered as evident (for example, there is exactly one straight line passing through two points), or seemed impossible to prove (parallel postulate). after the introduction at the end of   th century of non-euclidean geometries, the old postulates were re-formalized to define euclidean spaces through axiomatic theory. another definition of euclidean spaces by means of vector spaces and linear algebra has been shown to be equivalent to the axiomatic definition. it is this definition that is more commonly used in modern mathematics, and detailed in this article. in all definitions, euclidean spaces consist of points, which are defined only by the properties that they must have for forming a euclidean space. there is essentially only one euclidean space of each dimension; that is, all euclidean spaces of a given dimension are isomorphic. therefore, in many cases, it is possible to work with a specific euclidean space, which is generally the real n-space r n , {\displaystyle \mathbb {r} ^{n},} equipped with the dot product. an isomorphism from a euclidean space to r n {\displaystyle \mathbb {r} ^{n}} associates with each point an n-tuple of real numbers which locate that point in the euclidean space and are called the cartesian coordinates of that point. euclidean space was introduced by ancient greeks as an abstraction of our physical space. their great innovation, appearing in euclid's elements was to build and prove all geometry by starting from a few very basic properties, which are abstracted from the physical world, and cannot be mathematically proved because of the lack of more basic tools. these properties are called postulates, or axioms in modern language. this way of defining euclidean space is still in use under the name of synthetic geometry. in     , rené descartes introduced cartesian coordinates and showed that this allows reducing geometric problems to algebraic computations with numbers. this reduction of geometry to algebra was a major change of point of view, as, until then, the real numbers were defined in terms of lengths and distances. euclidean geometry was not applied in spaces of more than three dimensions until the   th century. ludwig schläfli generalized euclidean geometry to spaces of n dimensions using both synthetic and algebraic methods, and discovered all of the regular polytopes (higher-dimensional analogues of the platonic solids) that exist in euclidean spaces of any number of dimensions. despite the wide use of descartes' approach, which was called analytic geometry, the definition of euclidean space remained unchanged until the end of   th century. the introduction of abstract vector spaces allowed their use in defining euclidean spaces with a purely algebraic definition. this new definition has been shown to be equivalent to the classical definition in terms of geometric axioms. it is this algebraic definition that is now most often used for introducing euclidean spaces. one way to think of the euclidean plane is as a set of points satisfying certain relationships, expressible in terms of distance and angles. for example, there are two fundamental operations (referred to as motions) on the plane. one is translation, which means a shifting of the plane so that every point is shifted in the same direction and by the same distance. the other is rotation around a fixed point in the plane, in which all points in the plane turn around that fixed point through the same angle. one of the basic tenets of euclidean geometry is that two figures (usually considered as subsets) of the plane should be considered equivalent (congruent) if one can be transformed into the other by some sequence of translations, rotations and reflections (see below). "
144,144,Addition of vectors,1,https://en.wikipedia.org/wiki/Euclidean_vector,"in mathematics, physics and engineering, a euclidean vector or simply a vector (sometimes called a geometric vector or spatial vector ) is a geometric object that has magnitude (or length) and direction. vectors can be added to other vectors according to vector algebra. a euclidean vector is frequently represented by a directed line segment, or graphically as an arrow connecting an initial point a with a terminal point b, and denoted by a b → {\displaystyle {\overrightarrow {ab}}} . a vector is what is needed to ""carry"" the point a to the point b; the latin word vector means ""carrier"". it was first used by   th century astronomers investigating planetary revolution around the sun. the magnitude of the vector is the distance between the two points, and the direction refers to the direction of displacement from a to b. many algebraic operations on real numbers such as addition, subtraction, multiplication, and negation have close analogues for vectors, operations which obey the familiar algebraic laws of commutativity, associativity, and distributivity. these operations and associated laws qualify euclidean vectors as an example of the more generalized concept of vectors defined simply as elements of a vector space. vectors play an important role in physics: the velocity and acceleration of a moving object and the forces acting on it can all be described with vectors. many other physical quantities can be usefully thought of as vectors. although most of them do not represent distances (except, for example, position or displacement), their magnitude and direction can still be represented by the length and direction of an arrow. the mathematical representation of a physical vector depends on the coordinate system used to describe it. other vector-like objects that describe physical quantities and transform in a similar way under changes of the coordinate system include pseudovectors and tensors. the concept of vector, as we know it today, evolved gradually over a period of more than     years. about a dozen people made significant contributions to its development. in     , giusto bellavitis abstracted the basic idea when he established the concept of equipollence. working in a euclidean plane, he made equipollent any pair of parallel line segments of the same length and orientation. essentially, he realized an equivalence relation on the pairs of points (bipoints) in the plane, and thus erected the first space of vectors in the plane. :   –  the term vector was introduced by william rowan hamilton as part of a quaternion, which is a sum q = s + v of a real number s (also called scalar) and a  -dimensional vector. like bellavitis, hamilton viewed vectors as representative of classes of equipollent directed segments. as complex numbers use an imaginary unit to complement the real line, hamilton considered the vector v to be the imaginary part of a quaternion: the algebraically imaginary part, being geometrically constructed by a straight line, or radius vector, which has, in general, for each determined quaternion, a determined length and determined direction in space, may be called the vector part, or simply the vector of the quaternion.several other mathematicians developed vector-like systems in the middle of the nineteenth century, including augustin cauchy, hermann grassmann, august möbius, comte de saint-venant, and matthew o'brien. grassmann's      work theorie der ebbe und flut (theory of the ebb and flow) was the first system of spatial analysis that is similar to today's system, and had ideas corresponding to the cross product, scalar product and vector differentiation. grassmann's work was largely neglected until the     s. peter guthrie tait carried the quaternion standard after hamilton. his      elementary treatise of quaternions included extensive treatment of the nabla or del operator ∇. in     , elements of dynamic was published by william kingdon clifford. clifford simplified the quaternion study by isolating the dot product and cross product of two vectors from the complete quaternion product. this approach made vector calculations available to engineers—and others working in three dimensions and skeptical of the fourth. "
145,145,Eulers formula and identity,2,https://en.wikipedia.org/wiki/Euler%27s_formula," euler's formula, named after leonhard euler, is a mathematical formula in complex analysis that establishes the fundamental relationship between the trigonometric functions and the complex exponential function. euler's formula states that for any real number x: where e is the base of the natural logarithm, i is the imaginary unit, and cos and sin are the trigonometric functions cosine and sine respectively. this complex exponential function is sometimes denoted cis x (""cosine plus i sine""). the formula is still valid if x is a complex number, and so some authors refer to the more general complex version as euler's formula. euler's formula is ubiquitous in mathematics, physics, and engineering. the physicist richard feynman called the equation ""our jewel"" and ""the most remarkable formula in mathematics"". when x = π, euler's formula may be rewritten as eiπ +   =  , which is known as euler's identity. in     , the english mathematician roger cotes presented a geometrical argument that can be interpreted (after correcting a misplaced factor of −   {\displaystyle {\sqrt {- }}} ) as: exponentiating this equation yields euler's formula. note that the logarithmic statement is not universally correct for complex numbers, since a complex logarithm can have infinitely many values, differing by multiples of  πi. around      leonhard euler turned his attention to the exponential function and derived the equation named after him by comparing the series expansions of the exponential and trigonometric expressions. the formula was first published in      in his foundational work introductio in analysin infinitorum. johann bernoulli had found that and since "
146,146,Even and odd functions,1,https://en.wikipedia.org/wiki/Even_and_odd_functions,"in mathematics, even functions and odd functions are functions which satisfy particular symmetry relations, with respect to taking additive inverses. they are important in many areas of mathematical analysis, especially the theory of power series and fourier series. they are named for the parity of the powers of the power functions which satisfy each condition: the function f ( x ) = x n {\displaystyle f(x)=x^{n}} is an even function if n is an even integer, and it is an odd function if n is an odd integer. evenness and oddness are generally considered for real functions, that is real-valued functions of a real variable. however, the concepts may be more generally defined for functions whose domain and codomain both have a notion of additive inverse. this includes abelian groups, all rings, all fields, and all vector spaces. thus, for example, a real function could be odd or even (or neither), as could a complex-valued function of a vector variable, and so on. the given examples are real functions, to illustrate the symmetry of their graphs. let f be a real-valued function of a real variable. then f is even if the following equation holds for all x such that x and −x are in the domain of f: : p.    (eq. )or equivalently if the following equation holds for all such x: "
147,147,Event,1,https://en.wikipedia.org/wiki/Event_(probability_theory),"in probability theory, an event is a set of outcomes of an experiment (a subset of the sample space) to which a probability is assigned. a single outcome may be an element of many different events, and different events in an experiment are usually not equally likely, since they may include very different groups of outcomes. an event consisting of only a single outcome is called an elementary event or an atomic event; that is, it is a singleton set. an event s {\displaystyle s} is said to occur if s {\displaystyle s} contains the outcome x {\displaystyle x} of the experiment (or trial) (that is, if x ∈ s {\displaystyle x\in s} ). the probability (with respect to some probability measure) that an event s {\displaystyle s} occurs is the probability that s {\displaystyle s} contains the outcome x {\displaystyle x} of an experiment (that is, it is the probability that x ∈ s {\displaystyle x\in s} ). an event defines a complementary event, namely the complementary set (the event not occurring), and together these define a bernoulli trial: did the event occur or not? typically, when the sample space is finite, any subset of the sample space is an event (that is, all elements of the power set of the sample space are defined as events). however, this approach does not work well in cases where the sample space is uncountably infinite. so, when defining a probability space it is possible, and often necessary, to exclude certain subsets of the sample space from being events (see events in probability spaces, below). if we assemble a deck of    playing cards with no jokers, and draw a single card from the deck, then the sample space is a   -element set, as each card is a possible outcome. an event, however, is any subset of the sample space, including any singleton set (an elementary event), the empty set (an impossible event, with probability zero) and the sample space itself (a certain event, with probability one). other events are proper subsets of the sample space that contain multiple elements. so, for example, potential events include: since all events are sets, they are usually written as sets (for example, { ,  ,  }), and represented graphically using venn diagrams. in the situation where each outcome in the sample space ω is equally likely, the probability p {\displaystyle p} of an event a {\displaystyle a} is the following formula: defining all subsets of the sample space as events works well when there are only finitely many outcomes, but gives rise to problems when the sample space is infinite. for many standard probability distributions, such as the normal distribution, the sample space is the set of real numbers or some subset of the real numbers. attempts to define probabilities for all subsets of the real numbers run into difficulties when one considers 'badly behaved' sets, such as those that are nonmeasurable. hence, it is necessary to restrict attention to a more limited family of subsets. for the standard tools of probability theory, such as joint and conditional probabilities, to work, it is necessary to use a σ-algebra, that is, a family closed under complementation and countable unions of its members. the most natural choice of σ-algebra is the borel measurable set derived from unions and intersections of intervals. however, the larger class of lebesgue measurable sets proves more useful in practice. in the general measure-theoretic description of probability spaces, an event may be defined as an element of a selected 𝜎-algebra of subsets of the sample space. under this definition, any subset of the sample space that is not an element of the 𝜎-algebra is not an event, and does not have a probability. with a reasonable specification of the probability space, however, all events of interest are elements of the 𝜎-algebra. even though events are subsets of some sample space ω , {\displaystyle \omega ,} they are often written as predicates or indicators involving random variables. for example, if x {\displaystyle x} is a real-valued random variable defined on the sample space ω , {\displaystyle \omega ,} the event "
148,148,Expectation of a random Variable,3,https://en.wikipedia.org/wiki/Expected_value,"in probability theory, the expected value (also called expectation, expectancy, mathematical expectation, mean, average, or first moment) is a generalization of the weighted average. informally, the expected value is the arithmetic mean of a large number of independently selected outcomes of a random variable. the expected value of a random variable with a finite number of outcomes is a weighted average of all possible outcomes. in the case of a continuum of possible outcomes, the expectation is defined by integration. in the axiomatic foundation for probability provided by measure theory, the expectation is given by lebesgue integration. the expected value of a random variable x is often denoted by e(x), e[x], or ex, with e also often stylized as e or e . {\displaystyle \mathbb {e} .} the idea of the expected value originated in the middle of the   th century from the study of the so-called problem of points, which seeks to divide the stakes in a fair way between two players, who have to end their game before it is properly finished. this problem had been debated for centuries. many conflicting proposals and solutions had been suggested over the years when it was posed to blaise pascal by french writer and amateur mathematician chevalier de méré in     . méré claimed that this problem couldn't be solved and that it showed just how flawed mathematics was when it came to its application to the real world. pascal, being a mathematician, was provoked and determined to solve the problem once and for all. he began to discuss the problem in the famous series of letters to pierre de fermat. soon enough, they both independently came up with a solution. they solved the problem in different computational ways, but their results were identical because their computations were based on the same fundamental principle. the principle is that the value of a future gain should be directly proportional to the chance of getting it. this principle seemed to have come naturally to both of them. they were very pleased by the fact that they had found essentially the same solution, and this in turn made them absolutely convinced that they had solved the problem conclusively; however, they did not publish their findings. they only informed a small circle of mutual scientific friends in paris about it. in dutch mathematician christiaan huygens' book, he considered the problem of points, and presented a solution based on the same principle as the solutions of pascal and fermat. huygens published his treatise in     , (see huygens (    )) ""de ratiociniis in ludo aleæ"" on probability theory just after visiting paris. the book extended the concept of expectation by adding rules for how to calculate expectations in more complicated situations than the original problem (e.g., for three or more players), and can be seen as the first successful attempt at laying down the foundations of the theory of probability. in the foreword to his treatise, huygens wrote: it should be said, also, that for some time some of the best mathematicians of france have occupied themselves with this kind of calculus so that no one should attribute to me the honour of the first invention. this does not belong to me. but these savants, although they put each other to the test by proposing to each other many questions difficult to solve, have hidden their methods. i have had therefore to examine and go deeply for myself into this matter by beginning with the elements, and it is impossible for me for this reason to affirm that i have even started from the same principle. but finally i have found that my answers in many cases do not differ from theirs.during his visit to france in     , huygens learned about de méré's problem. from his correspondence with carcavine a year later (in     ), he realized his method was essentially the same as pascal's. therefore, he knew about pascal's priority in this subject before his book went to press in     . in the mid-nineteenth century, pafnuty chebyshev became the first person to think systematically in terms of the expectations of random variables. "
149,149,Experiment Probability Theory,1,https://en.wikipedia.org/wiki/Experiment_(probability_theory),"in probability theory, an experiment or trial (see below) is any procedure that can be infinitely repeated and has a well-defined set of possible outcomes, known as the sample space. an experiment is said to be random if it has more than one possible outcome, and deterministic if it has only one. a random experiment that has exactly two (mutually exclusive) possible outcomes is known as a bernoulli trial. when an experiment is conducted, one (and only one) outcome results— although this outcome may be included in any number of events, all of which would be said to have occurred on that trial. after conducting many trials of the same experiment and pooling the results, an experimenter can begin to assess the empirical probabilities of the various outcomes and events that can occur in the experiment and apply the methods of statistical analysis. random experiments are often conducted repeatedly, so that the collective results may be subjected to statistical analysis. a fixed number of repetitions of the same experiment can be thought of as a composed experiment, in which case the individual repetitions are called trials. for example, if one were to toss the same coin one hundred times and record each result, each toss would be considered a trial within the experiment composed of all hundred tosses. a random experiment is described or modeled by a mathematical construct known as a probability space. a probability space is constructed and defined with a specific kind of experiment or trial in mind. a mathematical description of an experiment consists of three parts: an outcome is the result of a single execution of the model. since individual outcomes might be of little practical use, more complicated events are used to characterize groups of outcomes. the collection of all such events is a sigma-algebra f {\displaystyle \scriptstyle {\mathcal {f}}} . finally, there is a need to specify each event's likelihood of happening; this is done using the probability measure function, p. once an experiment is designed and established, ω, from the sample space ω. all the events in f {\displaystyle \scriptstyle {\mathcal {f}}} that contain the selected outcome ω (recall that each event is a subset of ω) are said to “have occurred”. the probability function p is defined in such a way that, if the experiment were to be repeated an infinite number of times, the relative frequencies of occurrence of each of the events would approach agreement with the values p assigns them. as a simple experiment, we may flip a coin twice. the sample space (where the order of the two flips is relevant) is {(h, t), (t, h), (t, t), (h, h)} where ""h"" means ""heads"" and ""t"" means ""tails"". note that each of (h, t), (t, h), ... are possible outcomes of the experiment. we may define an event which occurs when a ""heads"" occurs in either of the two flips. this event contains all of the outcomes except (t, t). "
150,150,Exponential function,2,https://en.wikipedia.org/wiki/Exponential_function," the exponential function is a mathematical function denoted by f ( x ) = exp ⁡ ( x ) {\displaystyle f(x)=\exp(x)} or e x {\displaystyle e^{x}} (where the argument x is written as an exponent). unless otherwise specified, the term generally refers to the positive-valued function of a real variable, although it can be extended to the complex numbers or generalized to other mathematical objects like matrices or lie algebras. the exponential function originated from the notion of exponentiation (repeated multiplication), but modern definitions (there are several equivalent characterizations) allow it to be rigorously extended to all real arguments, including irrational numbers. its ubiquitous occurrence in pure and applied mathematics led mathematician walter rudin to opine that the exponential function is ""the most important function in mathematics"". the exponential function satisfies the exponentiation identity while other continuous nonzero functions f : r → r {\displaystyle f:\mathbb {r} \to \mathbb {r} } that satisfy the exponentiation identity are also known as exponential functions, the exponential function exp is the unique real-valued function of a real variable whose derivative is itself and whose value at   is  ; that is, exp ′ ⁡ ( x ) = exp ⁡ ( x ) {\displaystyle \exp '(x)=\exp(x)} for all real x, and exp ⁡ (   ) =  . {\displaystyle \exp( )= .} thus, exp is sometimes called the natural exponential function to distinguish it from these other exponential functions, which are the functions of the form f ( x ) = a b x , {\displaystyle f(x)=ab^{x},} where the base b is a positive real number. the relation b x = e x ln ⁡ b {\displaystyle b^{x}=e^{x\ln b}} for positive b and real or complex x establishes a strong relationship between these functions, which explains this ambiguous terminology. the real exponential function can also be defined as a power series. this power series definition is readily extended to complex arguments to allow the complex exponential function exp : c → c {\displaystyle \exp :\mathbb {c} \to \mathbb {c} } to be defined. the complex exponential function takes on all complex values except for   and is closely related to the complex trigonometric functions, as shown by euler's formula. motivated by more abstract properties and characterizations of the exponential function, the exponential can be generalized to and defined for entirely different kinds of mathematical objects (for example, a square matrix or a lie algebra). in applied settings, exponential functions model a relationship in which a constant change in the independent variable gives the same proportional change (that is, percentage increase or decrease) in the dependent variable. this occurs widely in the natural and social sciences, as in a self-reproducing population, a fund accruing compound interest, or a growing body of manufacturing expertise. thus, the exponential function also appears in a variety of contexts within physics, computer science, chemistry, engineering, mathematical biology, and economics. the real exponential function is a bijection from r {\displaystyle \mathbb {r} } to (   ; ∞ ) {\displaystyle ( ;\infty )} . its inverse function is the natural logarithm, denoted ln , {\displaystyle \ln ,} [nb  ] log , {\displaystyle \log ,} [nb  ] or log e ; {\displaystyle \log _{e};} because of this, some old texts refer to the exponential function as the antilogarithm. the graph of y = e x {\displaystyle y=e^{x}} is upward-sloping, and increases faster as x increases. the graph always lies above the x-axis, but becomes arbitrarily close to it for large negative x; thus, the x-axis is a horizontal asymptote. the equation d d x e x = e x {\displaystyle {\tfrac {d}{dx}}e^{x}=e^{x}} means that the slope of the tangent to the graph at each point is equal to its y-coordinate at that point. the exponential function f ( x ) = e x {\displaystyle f(x)=e^{x}} is sometimes called the natural exponential function for distinguishing it from the other exponential functions. the study of any exponential function can easily be reduced to that of the natural exponential function, since per definition, for positive b, "
151,151,Extreme Value Theorem,2,https://en.wikipedia.org/wiki/Extreme_value_theorem,"in calculus, the extreme value theorem states that if a real-valued function f {\displaystyle f} is continuous on the closed interval [ a , b ] {\displaystyle [a,b]} , then f {\displaystyle f} must attain a maximum and a minimum, each at least once. that is, there exist numbers c {\displaystyle c} and d {\displaystyle d} in [ a , b ] {\displaystyle [a,b]} such that: the extreme value theorem is more specific than the related boundedness theorem, which states merely that a continuous function f {\displaystyle f} on the closed interval [ a , b ] {\displaystyle [a,b]} is bounded on that interval; that is, there exist real numbers m {\displaystyle m} and m {\displaystyle m} such that: this does not say that m {\displaystyle m} and m {\displaystyle m} are necessarily the maximum and minimum values of f {\displaystyle f} on the interval [ a , b ] , {\displaystyle [a,b],} which is what the extreme value theorem stipulates must also be the case. the extreme value theorem is used to prove rolle's theorem. in a formulation due to karl weierstrass, this theorem states that a continuous function from a non-empty compact space to a subset of the real numbers attains a maximum and a minimum. the extreme value theorem was originally proven by bernard bolzano in the     s in a work function theory but the work remained unpublished until     . bolzano's proof consisted of showing that a continuous function on a closed interval was bounded, and then showing that the function attained a maximum and a minimum value. both proofs involved what is known today as the bolzano–weierstrass theorem. the result was also discovered later by weierstrass in     .[citation needed] the following examples show why the function domain must be closed and bounded in order for the theorem to apply. each fails to attain a maximum on the given interval. defining f (   ) =   {\displaystyle f( )= } in the last two examples shows that both theorems require continuity on [ a , b ] {\displaystyle [a,b]} . when moving from the real line r {\displaystyle \mathbb {r} } to metric spaces and general topological spaces, the appropriate generalization of a closed bounded interval is a compact set. a set k {\displaystyle k} is said to be compact if it has the following property: from every collection of open sets u α {\displaystyle u_{\alpha }} such that ⋃ u α ⊃ k {\textstyle \bigcup u_{\alpha }\supset k} , a finite subcollection u α   , … , u α n {\displaystyle u_{\alpha _{ }},\ldots ,u_{\alpha _{n}}} can be chosen such that ⋃ i =   n u α i ⊃ k {\textstyle \bigcup _{i= }^{n}u_{\alpha _{i}}\supset k} . this is usually stated in short as ""every open cover of k {\displaystyle k} has a finite subcover"". the heine–borel theorem asserts that a subset of the real line is compact if and only if it is both closed and bounded. correspondingly, a metric space has the heine–borel property if every closed and bounded set is also compact. the concept of a continuous function can likewise be generalized. given topological spaces v , w {\displaystyle v,\ w} , a function f : v → w {\displaystyle f:v\to w} is said to be continuous if for every open set u ⊂ w {\displaystyle u\subset w} , f −   ( u ) ⊂ v {\displaystyle f^{- }(u)\subset v} is also open. given these definitions, continuous functions can be shown to preserve compactness: theorem. if v , w {\displaystyle v,\ w} are topological spaces, f : v → w {\displaystyle f:v\to w} is a continuous function, and k ⊂ v {\displaystyle k\subset v} is compact, then f ( k ) ⊂ w {\displaystyle f(k)\subset w} is also compact. "
152,152,Factor Theorem,1,https://en.wikipedia.org/wiki/Factor_theorem,"in algebra, the factor theorem is a theorem linking factors and zeros of a polynomial. it is a special case of the polynomial remainder theorem. the factor theorem states that a polynomial f ( x ) {\displaystyle f(x)} has a factor ( x − α ) {\displaystyle (x-\alpha )} if and only if f ( α ) =   {\displaystyle f(\alpha )= } (i.e. α {\displaystyle \alpha } is a root). two problems where the factor theorem is commonly applied are those of factoring a polynomial and finding the roots of a polynomial equation; it is a direct consequence of the theorem that these problems are essentially equivalent. the factor theorem is also used to remove known zeros from a polynomial while leaving all unknown zeros intact, thus producing a lower degree polynomial whose zeros may be easier to find. abstractly, the method is as follows: find the factors of x   +   x   +   x +  . {\displaystyle x^{ }+ x^{ }+ x+ .} solution: let p ( x ) {\displaystyle p(x)} be the above polynomial all possible factors of   are ±   {\displaystyle \pm  } and ±   {\displaystyle \pm  } . substituting x = −   {\displaystyle x=- } , we get: so, ( x − ( −   ) ) {\displaystyle (x-(- ))} , i.e, ( x +   ) {\displaystyle (x+ )} is a factor of p ( x ) {\displaystyle p(x)} . on dividing p ( x ) {\displaystyle p(x)} by ( x +   ) {\displaystyle (x+ )} , we get hence, p ( x ) = ( x   +   x +   ) ( x +   ) {\displaystyle p(x)=(x^{ }+ x+ )(x+ )} out of these, the quadratic factor can be further factored using the quadratic formula, which gives as roots of the quadratic −   ±   . {\displaystyle - \pm {\sqrt { }}.} thus the three irreducible factors of the original polynomial are x +   , {\displaystyle x+ ,} x − ( −   +   ) , {\displaystyle x-(- +{\sqrt { }}),} and x − ( −   −   ) . {\displaystyle x-(- -{\sqrt { }}).} "
153,153,Factorial,1,https://en.wikipedia.org/wiki/Factorial," in mathematics, the factorial of a non-negative integer n {\displaystyle n} , denoted by n ! {\displaystyle n!} , is the product of all positive integers less than or equal to n {\displaystyle n} . the factorial of n {\displaystyle n} also equals the product of n {\displaystyle n} with the next smaller factorial: factorials have been discovered in several ancient cultures, notably in indian mathematics in the canonical works of jain literature, and by jewish mystics in the talmudic book sefer yetzirah. the factorial operation is encountered in many areas of mathematics, notably in combinatorics, where its most basic use counts the possible distinct sequences – the permutations – of n {\displaystyle n} distinct objects: there are n ! {\displaystyle n!} . in mathematical analysis, factorials are used in power series for the exponential function and other functions, and they also have applications in algebra, number theory, probability theory, and computer science. much of the mathematics of the factorial function was developed beginning in the late   th and early   th centuries. stirling's approximation provides an accurate approximation to the factorial of large numbers, showing that it grows more quickly than exponential growth. legendre's formula describes the exponents of the prime numbers in a prime factorization of the factorials, and can be used to count the trailing zeros of the factorials. daniel bernoulli and leonhard euler interpolated the factorial function to a continuous function of complex numbers, except at the negative integers, the (offset) gamma function. many other notable functions and number sequences are closely related to the factorials, including the binomial coefficients, double factorials, falling factorials, primorials, and subfactorials. implementations of the factorial function are commonly used as an example of different computer programming styles, and are included in scientific calculators and scientific computing software libraries. although directly computing large factorials using the product formula or recurrence is not efficient, faster algorithms are known, matching to within a constant factor the time for fast multiplication algorithms for numbers with the same number of digits. the concept of factorials has arisen independently in many cultures: from the late   th century onward, factorials became the subject of study by western mathematicians. in a      treatise, italian mathematician luca pacioli calculated factorials up to   !, in connection with a problem of dining table arrangements. christopher clavius discussed factorials in a      commentary on the work of johannes de sacrobosco, and in the     s, french polymath marin mersenne published large (but not entirely correct) tables of factorials, up to   !, based on the work of clavius. the power series for the exponential function, with the reciprocals of factorials for its coefficients, was first formulated in      by isaac newton in a letter to gottfried wilhelm leibniz. other important works of early european mathematics on factorials include extensive coverage in a      treatise by john wallis, a study of their approximate values for large values of n {\displaystyle n} by abraham de moivre in     , a      letter from james stirling to de moivre stating what became known as stirling's approximation, and work at the same time by daniel bernoulli and leonhard euler formulating the continuous extension of the factorial function to the gamma function. adrien-marie legendre included legendre's formula, describing the exponents in the factorization of factorials into prime powers, in an      text on number theory. the notation n ! {\displaystyle n!} for factorials was introduced by the french mathematician christian kramp in     . many other notations have also been used. another later notation, in which the argument of the factorial was half-enclosed by the left and bottom sides of a box, was popular for some time in britain and america but fell out of use, perhaps because it is difficult to typeset. the word ""factorial"" (originally french: factorielle) was first used in      by louis françois antoine arbogast, in the first work on faà di bruno's formula, but referring to a more general concept of products of arithmetic progressions. the ""factors"" that this name refers to are the terms of the product formula for the factorial. the factorial function of a positive integer n {\displaystyle n} is defined by the product "
154,154,Factorial moments,2,https://en.wikipedia.org/wiki/Factorial_moment,"in probability theory, the factorial moment is a mathematical quantity defined as the expectation or average of the falling factorial of a random variable. factorial moments are useful for studying non-negative integer-valued random variables, and arise in the use of probability-generating functions to derive the moments of discrete random variables. factorial moments serve as analytic tools in the mathematical field of combinatorics, which is the study of discrete mathematical structures. for a natural number r, the r-th factorial moment of a probability distribution on the real or complex numbers, or, in other words, a random variable x with that probability distribution, is where the e is the expectation (operator) and is the falling factorial, which gives rise to the name, although the notation (x)r varies depending on the mathematical field. [a] of course, the definition requires that the expectation is meaningful, which is the case if (x)r ≥   or e[|(x)r|] < ∞. if x is the number of successes in n trials, and pr is the probability that any r of the n trials are all successes, then if a random variable x has a poisson distribution with parameter λ, then the factorial moments of x are which are simple in form compared to its moments, which involve stirling numbers of the second kind. if a random variable x has a binomial distribution with success probability p ∈ [ , ] and number of trials n, then the factorial moments of x are where by convention, ( n r ) {\displaystyle \textstyle {\binom {n}{r}}} and ( n ) r {\displaystyle (n)_{r}} are understood to be zero if r > n. "
155,155,Fermat's Last Theorem,3,https://en.wikipedia.org/wiki/Fermat%27s_Last_Theorem," in number theory, fermat's last theorem (sometimes called fermat's conjecture, especially in older texts) states that no three positive integers a, b, and c satisfy the equation an + bn = cn for any integer value of n greater than  . the cases n =   and n =   have been known since antiquity to have infinitely many solutions. the proposition was first stated as a theorem by pierre de fermat around      in the margin of a copy of arithmetica; fermat added that he had a proof that was too large to fit in the margin. although other statements claimed by fermat without proof were subsequently proven by others and credited as theorems of fermat (for instance, fermat's theorem on sums of two squares), fermat's last theorem resisted proof, leading to doubt that fermat ever had a correct proof and its becoming known as a conjecture rather than a theorem. after     years of effort by mathematicians, the first successful proof was released in      by andrew wiles, and formally published in     ; it was described as a ""stunning advance"" in the citation for wiles's abel prize award in     . it also proved much of the taniyama–shimura conjecture, subsequently known as the modularity theorem, and opened up entire new approaches to numerous other problems and mathematically powerful modularity lifting techniques. the unsolved problem stimulated the development of algebraic number theory in the   th and   th centuries. it is among the most notable theorems in the history of mathematics and prior to its proof was in the guinness book of world records as the ""most difficult mathematical problem"" in part because the theorem has the largest number of unsuccessful proofs. the pythagorean equation, x  + y  = z , has an infinite number of positive integer solutions for x, y, and z; these solutions are known as pythagorean triples (with the simplest example  , , ). around     , fermat wrote in the margin of a book that the more general equation an + bn = cn had no solutions in positive integers if n is an integer greater than  . although he claimed to have a general proof of his conjecture, fermat left no details of his proof, and no proof by him has ever been found. his claim was discovered some    years later, after his death. this claim, which came to be known as fermat's last theorem, stood unsolved for the next three and a half centuries. the claim eventually became one of the most notable unsolved problems of mathematics. attempts to prove it prompted substantial development in number theory, and over time fermat's last theorem gained prominence as an unsolved problem in mathematics. the special case n =  , proved by fermat himself, is sufficient to establish that if the theorem is false for some exponent n that is not a prime number, it must also be false for some smaller n, so only prime values of n need further investigation.[note  ] over the next two centuries (    –    ), the conjecture was proved for only the primes  ,  , and  , although sophie germain innovated and proved an approach that was relevant to an entire class of primes. in the mid-  th century, ernst kummer extended this and proved the theorem for all regular primes, leaving irregular primes to be analyzed individually. building on kummer's work and using sophisticated computer studies, other mathematicians were able to extend the proof to cover all prime exponents up to four million, but a proof for all exponents was inaccessible (meaning that mathematicians generally considered a proof impossible, exceedingly difficult, or unachievable with current knowledge). separately, around     , japanese mathematicians goro shimura and yutaka taniyama suspected a link might exist between elliptic curves and modular forms, two completely different areas of mathematics. known at the time as the taniyama–shimura conjecture (eventually as the modularity theorem), it stood on its own, with no apparent connection to fermat's last theorem. it was widely seen as significant and important in its own right, but was (like fermat's theorem) widely considered completely inaccessible to proof. in     , gerhard frey noticed an apparent link between these two previously unrelated and unsolved problems. an outline suggesting this could be proved was given by frey. the full proof that the two problems were closely linked was accomplished in      by ken ribet, building on a partial proof by jean-pierre serre, who proved all but one part known as the ""epsilon conjecture"" (see: ribet's theorem and frey curve). these papers by frey, serre and ribet showed that if the taniyama–shimura conjecture could be proven for at least the semi-stable class of elliptic curves, a proof of fermat's last theorem would also follow automatically. the connection is described below: any solution that could contradict fermat's last theorem could also be used to contradict the taniyama–shimura conjecture. so if the modularity theorem were found to be true, then by definition no solution contradicting fermat's last theorem could exist, which would therefore have to be true as well. although both problems were daunting and widely considered to be ""completely inaccessible"" to proof at the time, this was the first suggestion of a route by which fermat's last theorem could be extended and proved for all numbers, not just some numbers. unlike fermat's last theorem, the taniyama–shimura conjecture was a major active research area and viewed as more within reach of contemporary mathematics. however, general opinion was that this simply showed the impracticality of proving the taniyama–shimura conjecture. mathematician john coates' quoted reaction was a common one: "
156,156,Fibonacci Sequences,1,https://en.wikipedia.org/wiki/Fibonacci_number,"in mathematics, the fibonacci numbers, commonly denoted fn, form a sequence, the fibonacci sequence, in which each number is the sum of the two preceding ones. the sequence commonly starts from   and  , although some authors omit the initial terms and start the sequence from   and   or from   and  . starting from   and  , the next few values in the sequence are: the fibonacci numbers were first described in indian mathematics, as early as     bc in work by pingala on enumerating possible patterns of sanskrit poetry formed from syllables of two lengths. they are named after the italian mathematician leonardo of pisa, later known as fibonacci, who introduced the sequence to western european mathematics in his      book liber abaci. fibonacci numbers appear unexpectedly often in mathematics, so much so that there is an entire journal dedicated to their study, the fibonacci quarterly. applications of fibonacci numbers include computer algorithms such as the fibonacci search technique and the fibonacci heap data structure, and graphs called fibonacci cubes used for interconnecting parallel and distributed systems. they also appear in biological settings, such as branching in trees, the arrangement of leaves on a stem, the fruit sprouts of a pineapple, the flowering of an artichoke, an uncurling fern, and the arrangement of a pine cone's bracts. fibonacci numbers are strongly related to the golden ratio: binet's formula expresses the nth fibonacci number in terms of n and the golden ratio, and implies that the ratio of two consecutive fibonacci numbers tends to the golden ratio as n increases. fibonacci numbers are also closely related to lucas numbers, which obey the same recurrence relation and with the fibonacci numbers form a complementary pair of lucas sequences. the fibonacci numbers may be defined by the recurrence relation under some older definitions, the value f   =   {\displaystyle f_{ }= } is omitted, so that the sequence starts with f   = f   =   , {\displaystyle f_{ }=f_{ }= ,} and the recurrence f n = f n −   + f n −   {\displaystyle f_{n}=f_{n- }+f_{n- }} is valid for n >  . fibonacci started the sequence with f   =   , f   =   {\displaystyle f_{ }= ,f_{ }= } . the first    fibonacci numbers fn are: the fibonacci sequence appears in indian mathematics in connection with sanskrit prosody, as pointed out by parmanand singh in     . in the sanskrit poetic tradition, there was interest in enumerating all patterns of long (l) syllables of   units duration, juxtaposed with short (s) syllables of   unit duration. counting the different patterns of successive l and s with a given total duration results in the fibonacci numbers: the number of patterns of duration m units is fm +  . knowledge of the fibonacci sequence was expressed as early as pingala (c.     bc–    bc). singh cites pingala's cryptic formula misrau cha (""the two are mixed"") and scholars who interpret it in context as saying that the number of patterns for m beats (fm+ ) is obtained by adding one [s] to the fm cases and one [l] to the fm−  cases. bharata muni also expresses knowledge of the sequence in the natya shastra (c.     bc–c.     ad). however, the clearest exposition of the sequence arises in the work of virahanka (c.     ad), whose own work is lost, but is available in a quotation by gopala (c.     ): variations of two earlier meters [is the variation]... for example, for [a meter of length] four, variations of meters of two [and] three being mixed, five happens. [works out examples  ,   ,   ]... in this way, the process should be followed in all mātrā-vṛttas [prosodic combinations].[a]"
157,157,Ellipse,2,https://en.wikipedia.org/wiki/File:Ellipse-def0.svg,"original file ‎(svg file, nominally     ×     pixels, file size:    kb) https://creativecommons.org/licenses/by-sa/ .  cc by-sa  .  creative commons attribution-share alike  .  truetrue click on a date/time to view the file as it appeared at that time. the following other wikis use this file: this file contains additional information, probably added from the digital camera or scanner used to create or digitize it. if the file has been modified from its original state, some details may not fully reflect the modified file."
158,158,Finite Field,0,https://en.wikipedia.org/wiki/Finite_field,"in mathematics, a finite field or galois field (so-named in honor of évariste galois) is a field that contains a finite number of elements. as with any field, a finite field is a set on which the operations of multiplication, addition, subtraction and division are defined and satisfy certain basic rules. the most common examples of finite fields are given by the integers mod p when p is a prime number. the order of a finite field is its number of elements, which is either a prime number or a prime power. for every prime number p and every positive integer k there are fields of order p k , {\displaystyle p^{k},} all of which are isomorphic. finite fields are fundamental in a number of areas of mathematics and computer science, including number theory, algebraic geometry, galois theory, finite geometry, cryptography and coding theory. a finite field is a finite set which is a field; this means that multiplication, addition, subtraction and division (excluding division by zero) are defined and satisfy the rules of arithmetic known as the field axioms. the number of elements of a finite field is called its order or, sometimes, its size. a finite field of order q exists if and only if q is a prime power pk (where p is a prime number and k is a positive integer). in a field of order pk, adding p copies of any element always results in zero; that is, the characteristic of the field is p. if q = pk, all fields of order q are isomorphic (see § existence and uniqueness below). moreover, a field cannot contain two different finite subfields with the same order. one may therefore identify all finite fields with the same order, and they are unambiguously denoted f q {\displaystyle \mathbb {f} _{q}} , fq or gf(q), where the letters gf stand for ""galois field"". in a finite field of order q, the polynomial xq − x has all q elements of the finite field as roots. the non-zero elements of a finite field form a multiplicative group. this group is cyclic, so all non-zero elements can be expressed as powers of a single element called a primitive element of the field. (in general there will be several primitive elements for a given field.) the simplest examples of finite fields are the fields of prime order: for each prime number p, the prime field of order p, f p {\displaystyle \mathbb {f} _{p}} , may be constructed as the integers modulo p, z/pz. the elements of the prime field of order p may be represented by integers in the range  , ..., p −  . the sum, the difference and the product are the remainder of the division by p of the result of the corresponding integer operation. the multiplicative inverse of an element may be computed by using the extended euclidean algorithm (see extended euclidean algorithm § modular integers). let f be a finite field. for any element x in f and any integer n, denote by n ⋅ x the sum of n copies of x. the least positive n such that n ⋅   =   is the characteristic p of the field. this allows defining a multiplication ( k , x ) ↦ k ⋅ x {\displaystyle (k,x)\mapsto k\cdot x} of an element k of gf(p) by an element x of f by choosing an integer representative for k. this multiplication makes f into a gf(p)-vector space. it follows that the number of elements of f is pn for some integer n. "
159,159,Predicate / First Order Logic,0,https://en.wikipedia.org/wiki/First-order_logic,"first-order logic—also known as predicate logic, quantificational logic, and first-order predicate calculus—is a collection of formal systems used in mathematics, philosophy, linguistics, and computer science. first-order logic uses quantified variables over non-logical objects, and allows the use of sentences that contain variables, so that rather than propositions such as ""socrates is a man"", one can have expressions in the form ""there exists x such that x is socrates and x is a man"", where ""there exists"" is a quantifier, while x is a variable. this distinguishes it from propositional logic, which does not use quantifiers or relations; in this sense, propositional logic is the foundation of first-order logic. a theory about a topic is usually a first-order logic together with a specified domain of discourse (over which the quantified variables range), finitely many functions from that domain to itself, finitely many predicates defined on that domain, and a set of axioms believed to hold about them. sometimes, ""theory"" is understood in a more formal sense, which is just a set of sentences in first-order logic. the adjective ""first-order"" distinguishes first-order logic from higher-order logic, in which there are predicates having predicates or functions as arguments, or in which predicate quantifiers or function quantifiers or both are permitted. :    in first-order theories, predicates are often associated with sets. in interpreted higher-order theories, predicates may be interpreted as sets of sets. there are many deductive systems for first-order logic which are both sound (i.e., all provable statements are true in all models) and complete (i.e. all statements which are true in all models are provable). although the logical consequence relation is only semidecidable, much progress has been made in automated theorem proving in first-order logic. first-order logic also satisfies several metalogical theorems that make it amenable to analysis in proof theory, such as the löwenheim–skolem theorem and the compactness theorem. first-order logic is the standard for the formalization of mathematics into axioms, and is studied in the foundations of mathematics. peano arithmetic and zermelo–fraenkel set theory are axiomatizations of number theory and set theory, respectively, into first-order logic. no first-order theory, however, has the strength to uniquely describe a structure with an infinite domain, such as the natural numbers or the real line. axiom systems that do fully describe these two structures (that is, categorical axiom systems) can be obtained in stronger logics such as second-order logic. the foundations of first-order logic were developed independently by gottlob frege and charles sanders peirce. for a history of first-order logic and how it came to dominate formal logic, see josé ferreirós (    ). while propositional logic deals with simple declarative propositions, first-order logic additionally covers predicates and quantification. a predicate takes an entity or entities in the domain of discourse as input while outputs are either true or false. consider the two sentences ""socrates is a philosopher"" and ""plato is a philosopher"". in propositional logic, these sentences are viewed as being unrelated, and might be denoted, for example, by variables such as p and q. the predicate ""is a philosopher"" occurs in both sentences, which have a common structure of ""a is a philosopher"". the variable a is instantiated as ""socrates"" in the first sentence, and is instantiated as ""plato"" in the second sentence. while first-order logic allows for the use of predicates, such as ""is a philosopher"" in this example, propositional logic does not. relationships between predicates can be stated using logical connectives. consider, for example, the first-order formula ""if a is a philosopher, then a is a scholar"". this formula is a conditional statement with ""a is a philosopher"" as its hypothesis, and ""a is a scholar"" as its conclusion. the truth of this formula depends on which object is denoted by a, and on the interpretations of the predicates ""is a philosopher"" and ""is a scholar"". quantifiers can be applied to variables in a formula. the variable a in the previous formula can be universally quantified, for instance, with the first-order sentence ""for every a, if a is a philosopher, then a is a scholar"". the universal quantifier ""for every"" in this sentence expresses the idea that the claim ""if a is a philosopher, then a is a scholar"" holds for all choices of a. "
160,160,Greatest Integer Function,1,https://en.wikipedia.org/wiki/Floor_and_ceiling_functions," in mathematics and computer science, the floor function is the function that takes as input a real number x, and gives as output the greatest integer less than or equal to x, denoted floor(x) or ⌊x⌋. similarly, the ceiling function maps x to the least integer greater than or equal to x, denoted ceil(x) or ⌈x⌉. for example, ⌊ . ⌋ =  , ⌊− . ⌋ = − , ⌈ . ⌉ =  , and ⌈− . ⌉ = − . the integral part or integer part of x, often denoted [x] is usually defined as the ⌊x⌋ if x is nonnegative, and ⌈x⌉ otherwise. for example, [ . ] =   and [− . ] = − . the operation of truncation generalizes this to a specified number of digits: truncation to zero significant digits is the same as the integer part. some authors define the integer part as the floor regardless of the sign of x, using a variety of notations for this. for n an integer, ⌊n⌋ = ⌈n⌉ = [n] = n. the integral part or integer part of a number (partie entière in the original) was first defined in      by adrien-marie legendre in his proof of the legendre's formula. carl friedrich gauss introduced the square bracket notation [x] in his third proof of quadratic reciprocity (    ). this remained the standard in mathematics until kenneth e. iverson introduced, in his      book a programming language, the names ""floor"" and ""ceiling"" and the corresponding notations ⌊x⌋ and ⌈x⌉. both notations are now used in mathematics, although iverson's notation will be followed in this article. in some sources, boldface or double brackets ⟦x⟧ are used for floor, and reversed brackets ⟧x⟦ or ]x[ for ceiling. sometimes [x] is taken to mean the round-toward-zero function.[citation needed] the fractional part is the sawtooth function, denoted by {x} for real x and defined by the formula "
161,161,Focus(geometry),1,https://en.wikipedia.org/wiki/Focus_(geometry),"in geometry, focuses or foci (/ˈfoʊkaɪ/), singular focus, are special points with reference to which any of a variety of curves is constructed. for example, one or two foci can be used in defining conic sections, the four types of which are the circle, ellipse, parabola, and hyperbola. in addition, two foci are used to define the cassini oval and the cartesian oval, and more than two foci are used in defining an n-ellipse. an ellipse can be defined as the locus of points for which the sum of the distances to two given foci is constant. a circle is the special case of an ellipse in which the two foci coincide with each other. thus, a circle can be more simply defined as the locus of points each of which is a fixed distance from a single given focus. a circle can also be defined as the circle of apollonius, in terms of two different foci, as the locus of points having a fixed ratio of distances to the two foci. a parabola is a limiting case of an ellipse in which one of the foci is a point at infinity. a hyperbola can be defined as the locus of points for which the absolute value of the difference between the distances to two given foci is constant. it is also possible to describe all conic sections in terms of a single focus and a single directrix, which is a given line not containing the focus. a conic is defined as the locus of points for each of which the distance to the focus divided by the distance to the directrix is a fixed positive constant, called the eccentricity e. if   < e <   the conic is an ellipse, if e =   the conic is a parabola, and if e >   the conic is a hyperbola. if the distance to the focus is fixed and the directrix is a line at infinity, so the eccentricity is zero, then the conic is a circle. it is also possible to describe all the conic sections as loci of points that are equidistant from a single focus and a single, circular directrix. for the ellipse, both the focus and the center of the directrix circle have finite coordinates and the radius of the directrix circle is greater than the distance between the center of this circle and the focus; thus, the focus is inside the directrix circle. the ellipse thus generated has its second focus at the center of the directrix circle, and the ellipse lies entirely within the circle. for the parabola, the center of the directrix moves to the point at infinity (see projective geometry). the directrix ""circle"" becomes a curve with zero curvature, indistinguishable from a straight line. the two arms of the parabola become increasingly parallel as they extend, and ""at infinity"" become parallel; using the principles of projective geometry, the two parallels intersect at the point at infinity and the parabola becomes a closed curve (elliptical projection). to generate a hyperbola, the radius of the directrix circle is chosen to be less than the distance between the center of this circle and the focus; thus, the focus is outside the directrix circle. the arms of the hyperbola approach asymptotic lines and the ""right-hand"" arm of one branch of a hyperbola meets the ""left-hand"" arm of the other branch of a hyperbola at the point at infinity; this is based on the principle that, in projective geometry, a single line meets itself at a point at infinity. the two branches of a hyperbola are thus the two (twisted) halves of a curve closed over infinity. in projective geometry, all conics are equivalent in the sense that every theorem that can be stated for one can be stated for the others. "
162,162,Four Color Theorem,0,https://en.wikipedia.org/wiki/Four_color_theorem,"in mathematics, the four color theorem, or the four color map theorem, states that no more than four colors are required to color the regions of any map so that no two adjacent regions have the same color. adjacent means that two regions share a common boundary curve segment, not merely a corner where three or more regions meet. it was the first major theorem to be proved using a computer. initially, this proof was not accepted by all mathematicians because the computer-assisted proof was infeasible for a human to check by hand. the proof has gained wide acceptance since then, although some doubters remain. the four color theorem was proved in      by kenneth appel and wolfgang haken after many false proofs and counterexamples (unlike the five color theorem, proved in the     s, which states that five colors are enough to color a map). to dispel any remaining doubts about the appel–haken proof, a simpler proof using the same ideas and still relying on computers was published in      by robertson, sanders, seymour, and thomas. in     , the theorem was also proved by georges gonthier with general-purpose theorem-proving software. in graph-theoretic terms, the theorem states that for loopless planar graph g {\displaystyle g} , its chromatic number is χ ( g ) ≤   {\displaystyle \chi (g)\leq  } . the intuitive statement of the four color theorem – ""given any separation of a plane into contiguous regions, the regions can be colored using at most four colors so that no two adjacent regions have the same color"" – needs to be interpreted appropriately to be correct. first, regions are adjacent if they share a boundary segment; two regions that share only isolated boundary points are not considered adjacent. second, bizarre regions, such as those with finite area but infinitely long perimeter, are not allowed; maps with such regions can require more than four colors. (to be safe, we can restrict to regions whose boundaries consist of finitely many straight line segments. it is allowed that a region entirely surround one or more other regions.) note that the notion of ""contiguous region"" (technically: connected open subset of the plane) is not the same as that of a ""country"" on regular maps, since countries need not be contiguous (e.g., the cabinda province as part of angola, nakhchivan as part of azerbaijan, kaliningrad as part of russia, and alaska as part of the united states are not contiguous). if we required the entire territory of a country to receive the same color, then four colors are not always sufficient. for instance, consider a simplified map: in this map, the two regions labeled a belong to the same country. if we wanted those regions to receive the same color, then five colors would be required, since the two a regions together are adjacent to four other regions, each of which is adjacent to all the others. forcing two separate regions to have the same color can be modelled by adding a 'handle' joining them outside the plane. such construction makes the problem equivalent to coloring a map on a torus (a surface of genus  ), which requires up to   colors for an arbitrary map. a similar construction also applies if a single color is used for multiple disjoint areas, as for bodies of water on real maps, or there are more countries with disjoint territories. in such cases more colors might be required with a growing genus of a resulting surface. (see the section generalizations below.) a simpler statement of the theorem uses graph theory. the set of regions of a map can be represented more abstractly as an undirected graph that has a vertex for each region and an edge for every pair of regions that share a boundary segment. this graph is planar: it can be drawn in the plane without crossings by placing each vertex at an arbitrarily chosen location within the region to which it corresponds, and by drawing the edges as curves without crossings that lead from one region's vertex, across a shared boundary segment, to an adjacent region's vertex. conversely any planar graph can be formed from a map in this way. in graph-theoretic terminology, the four-color theorem states that the vertices of every planar graph can be colored with at most four colors so that no two adjacent vertices receive the same color, or for short: as far as is known, the conjecture was first proposed on october   ,     , when francis guthrie, while trying to color the map of counties of england, noticed that only four different colors were needed. at the time, guthrie's brother, frederick, was a student of augustus de morgan (the former advisor of francis) at university college london. francis inquired with frederick regarding it, who then took it to de morgan (francis guthrie graduated later in     , and later became a professor of mathematics in south africa). according to de morgan: ""a student of mine [guthrie] asked me to day to give him a reason for a fact which i did not know was a fact—and do not yet. he says that if a figure be any how divided and the compartments differently colored so that figures with any portion of common boundary line are differently colored—four colors may be wanted but not more—the following is his case in which four colors are wanted. query cannot a necessity for five or more be invented…"" (wilson     , p.   )"
163,163,Fourier expansion,0,https://en.wikipedia.org/wiki/Fourier_series,"a fourier series (/ˈfʊrieɪ, -iər/ ) is a sum using only basic waves chosen to mathematically represent the waveform for almost any periodic function. these basic waves are sine and cosine waves whose frequency is an integer multiple of the periodic function's fundamental frequency. they are called harmonically-related sinusoids or harmonics. each harmonic's phase and amplitude can be determined using harmonic analysis so that the sum of all the harmonics represents a desired periodic function. a potentially infinite number of harmonics may be in the fourier series. fourier synthesis is the process of summing the harmonics in a fourier series to create a periodic function. the below animation shows this synthesis process when using the first few harmonics of the fourier series for a square wave: almost any[a] periodic function can be represented by a fourier series that converges.[b] convergence of fourier series means that as more and more harmonics from the series are summed, each successive partial fourier series sum will better approximate the function, and will equal the function with a potentially infinite number of harmonics. the mathematical proofs for this may be collectively referred to as the fourier theorem (see § convergence). fourier series can only represent functions that are periodic. however, non-periodic functions can be handled using an extension of the fourier series called the fourier transform which treats non-periodic functions as a periodic with infinite period. this transform thus can generate frequency domain representations of non-periodic functions as well as periodic functions. the below animation shows a waveform being converted from its time domain representation into its fourier transform, which is a frequency domain representation of the waveform: since fourier's time, many different approaches to defining and understanding the concept of fourier series have been discovered, all of which are consistent with one another, but each of which emphasizes different aspects of the topic. some of the more powerful and elegant approaches are based on mathematical ideas and tools that were not available in fourier's time. fourier originally defined the fourier series for real-valued functions of real arguments, and used the sine and cosine functions as the basis set for the decomposition. many other fourier-related transforms have since been defined, extending his initial idea to many applications and birthing an area of mathematics called fourier analysis. the fourier series s n ( x ) {\displaystyle s_{\scriptscriptstyle n}(x)} represents a synthesis of a periodic function s ( x ) {\displaystyle s(x)} by summing harmonically related sinusoids (called harmonics) whose coefficients are determined by harmonic analysis (which is out of the scope of this article). the fourier series can be represented in different forms. the amplitude-phase form, sine-cosine form, and exponential form are commonly used and are expressed here for a real-valued function s ( x ) {\displaystyle s(x)} . (see § complex-valued functions and § other common notations for alternative forms). the number of terms summed, n {\displaystyle n} , is a potentially infinite integer. even so, the series might not converge or exactly equate to s ( x ) {\displaystyle s(x)} at all values of x {\displaystyle x} (such as a single-point discontinuity) in the analysis interval. for the well-behaved functions typical of physical processes, equality is customarily assumed, and the dirichlet conditions provide sufficient conditions. the integer index, n {\displaystyle n} , is also the number of cycles the n th {\displaystyle n^{\text{th}}} harmonic makes in the function's period p {\displaystyle p} .[c] therefore: the fourier series in amplitude-phase form is:"
164,164,Fourier Transform,3,https://en.wikipedia.org/wiki/Fourier_transform,"a fourier transform (ft) is a mathematical transform that decomposes functions depending on space or time into functions depending on spatial frequency or temporal frequency. an example application would be decomposing the waveform of a musical chord into terms of the intensity of its constituent pitches. the term fourier transform refers to both the frequency domain representation and the mathematical operation that associates the frequency domain representation to a function of space or time. the fourier transform of a function is a complex-valued function representing the complex sinusoids that constitute the original function. for each frequency, the magnitude (absolute value) of the complex value represents the amplitude of a constituent complex sinusoid with that frequency, and the argument of the complex value represents that complex sinusoid's phase offset. if a frequency is not present, the transform has a value of   for that frequency. the fourier transform is not limited to functions of time, but the domain of the original function is commonly referred to as the time domain. the fourier inversion theorem provides an inverse fourier transform that synthesizes the original function from its frequency domain representation. functions that are localized in the time domain have fourier transforms that are spread out across the frequency domain and vice versa, a phenomenon known as the uncertainty principle. the critical case for this principle is the gaussian function, of substantial importance in probability theory and statistics as well as in the study of physical phenomena exhibiting normal distribution (e.g., diffusion). the fourier transform of a gaussian function is another gaussian function. joseph fourier introduced the transform in his study of heat transfer, where gaussian functions appear as solutions of the heat equation. the fourier transform can be formally defined as an improper riemann integral, making it an integral transform, although this definition is not suitable for many applications requiring a more sophisticated integration theory.[note  ] for example, many relatively simple applications use the dirac delta function, which can be treated formally as if it were a function, but the justification requires a mathematically more sophisticated viewpoint.[note  ] the fourier transform can also be generalized to functions of several variables on euclidean space, sending a function of  -dimensional 'position space' to a function of  -dimensional momentum (or a function of space and time to a function of  -momentum). this idea makes the spatial fourier transform very natural in the study of waves, as well as in quantum mechanics, where it is important to be able to represent wave solutions as functions of either position or momentum and sometimes both. in general, functions to which fourier methods are applicable are complex-valued, and possibly vector-valued.[note  ] still further generalization is possible to functions on groups, which, besides the original fourier transform on r or rn (viewed as groups under addition), notably includes the discrete-time fourier transform (dtft, group = z), the discrete fourier transform (dft, group = z mod n) and the fourier series or circular fourier transform (group = s , the unit circle ≈ closed finite interval with endpoints identified). the latter is routinely employed to handle periodic functions. the fast fourier transform (fft) is an algorithm for computing the dft. there are several common conventions for defining the fourier transform of an integrable function f : r → c {\displaystyle f:\mathbb {r} \to \mathbb {c} } . one of them is: "
165,165,Frequency distribution,0,https://en.wikipedia.org/wiki/Frequency_distribution,"in statistics, a frequency distribution is a list, table (i.e.: frequency table) or graph (i.e.: bar plot or histogram) that displays the frequency of various outcomes in a sample. each entry in the table contains the frequency or count of the occurrences of values within a particular group or interval. here is an example of a univariate (=single variable) frequency table. the frequency of each response to a survey question is depicted. a different tabulation scheme aggregates values into bins such that each bin encompasses a range of values. for example, the heights of the students in a class could be organized into the following frequency table. a frequency distribution shows us a summarized grouping of data divided into mutually exclusive classes and the number of occurrences in a class. it is a way of showing unorganized data notably to show results of an election, income of people for a certain region, sales of a product within a certain period, student loan amounts of graduates, etc. some of the graphs that can be used with frequency distributions are histograms, line charts, bar charts and pie charts. frequency distributions are used for both qualitative and quantitative data. generally the class interval or class width is the same for all classes. the classes all taken together must cover at least the distance from the lowest value (minimum) in the data to the highest (maximum) value. equal class intervals are preferred in frequency distribution, while unequal class intervals (for example logarithmic intervals) may be necessary in certain situations to produce a good spread of observations between the classes and avoid a large number of empty, or almost empty classes. bivariate joint frequency distributions are often presented as (two-way) contingency tables: the total row and total column report the marginal frequencies or marginal distribution, while the body of the table reports the joint frequencies. managing and operating on frequency tabulated data is much simpler than operation on raw data. there are simple algorithms to calculate median, mean, standard deviation etc. from these tables. statistical hypothesis testing is founded on the assessment of differences and similarities between frequency distributions. this assessment involves measures of central tendency or averages, such as the mean and median, and measures of variability or statistical dispersion, such as the standard deviation or variance. a frequency distribution is said to be skewed when its mean and median are significantly different, or more generally when it is asymmetric. the kurtosis of a frequency distribution is a measure of the proportion of extreme values (outliers), which appear at either end of the histogram. if the distribution is more outlier-prone than the normal distribution it is said to be leptokurtic; if less outlier-prone it is said to be platykurtic. "
166,166,Functions,1,https://en.wikipedia.org/wiki/Function_(mathematics),"in mathematics, a function from a set x to a set y assigns to each element of x exactly one element of y. the set x is called the domain of the function and the set y is called the codomain of the function.[citation needed] functions were originally the idealization of how a varying quantity depends on another quantity. for example, the position of a planet is a function of time. historically, the concept was elaborated with the infinitesimal calculus at the end of the   th century, and, until the   th century, the functions that were considered were differentiable (that is, they had a high degree of regularity). the concept of a function was formalized at the end of the   th century in terms of set theory, and this greatly enlarged the domains of application of the concept. a function is most often denoted by letters such as f, g and h, and the value of a function f at an element x of its domain is denoted by f(x). a function is uniquely represented by the set of all pairs (x, f (x)), called the graph of the function.[note  ] when the domain and the codomain are sets of real numbers, each such pair may be thought of as the cartesian coordinates of a point in the plane. the set of these points is called the graph of the function; it is a popular means of illustrating the function. functions are widely used in science, and in most fields of mathematics. it has been said that functions are ""the central objects of investigation"" in most fields of mathematics. a function from a set x to a set y is an assignment of an element of y to each element of x. the set x is called the domain of the function and the set y is called the codomain of the function. a function, its domain, and its codomain, are declared by the notation f: x→y, and the value of a function f at an element x of x, denoted by f(x), is called the image of x under f, or the value of f applied to the argument x. functions are also called maps or mappings, though some authors make some distinction between ""maps"" and ""functions"" (see § other terms). two functions f and g are equal if their domain and codomain sets are the same and their output values agree on the whole domain. more formally, given f: x → y and g: x → y, we have f = g if and only if f(x) = g(x) for all x ∈ x. [note  ] the domain and codomain are not always explicitly given when a function is defined, and, without some (possibly difficult) computation, one might only know that the domain is contained in a larger set. typically, this occurs in mathematical analysis, where ""a function from x to y "" often refers to a function that may have a proper subset[note  ] of x as domain. for example, a ""function from the reals to the reals"" may refer to a real-valued function of a real variable. however, a ""function from the reals to the reals"" does not mean that the domain of the function is the whole set of the real numbers, but only that the domain is a set of real numbers that contains a non-empty open interval. such a function is then called a partial function. for example, if f is a function that has the real numbers as domain and codomain, then a function mapping the value x to the value g(x) =  /f(x) is a function g from the reals to the reals, whose domain is the set of the reals x, such that f(x) ≠  . "
167,167,composite functions,3,https://en.wikipedia.org/wiki/Function_composition," in mathematics, function composition is an operation ∘ that takes two functions f and g, and produces a function h = g ∘ f such that h(x) = g(f(x)). in this operation, the function g is applied to the result of applying the function f to x. that is, the functions f : x → y and g : y → z are composed to yield a function that maps x in domain x to g(f(x)) in codomain z. intuitively, if z is a function of y, and y is a function of x, then z is a function of x. the resulting composite function is denoted g ∘ f : x → z, defined by (g ∘ f )(x) = g(f(x)) for all x in x.[nb  ] the notation g ∘ f is read as ""g of f "", ""g after f "", ""g circle f "", ""g round f "", ""g about f "", ""g composed with f "", ""g following f "", ""f then g"", or ""g on f "", or ""the composition of g and f "". intuitively, composing functions is a chaining process in which the output of function f feeds the input of function g. the composition of functions is a special case of the composition of relations, sometimes also denoted by ∘ {\displaystyle \circ } . as a result, all properties of composition of relations are true of composition of functions, such as the property of associativity. but composition of functions is different from multiplication of functions (if defined at all), and has some quite different properties; in particular, composition of functions is not commutative. the composition of functions is always associative—a property inherited from the composition of relations. that is, if f, g, and h are composable, then f ∘ (g ∘ h) = (f ∘ g) ∘ h. since the parentheses do not change the result, they are generally omitted. in a strict sense, the composition g ∘ f is only meaningful if the codomain of f equals the domain of g; in a wider sense, it is sufficient that the former be a subset of the latter.[nb  ] moreover, it is often convenient to tacitly restrict the domain of f, such that f produces only values in the domain of g. for example, the composition g ∘ f of the functions f : r → (−∞,+ ] defined by f(x) =   − x  and g : [ ,+∞) → r defined by g ( x ) = x {\displaystyle g(x)={\sqrt {x}}} can be defined on the interval [− ,+ ]. the functions g and f are said to commute with each other if g ∘ f = f ∘ g. commutativity is a special property, attained only by particular functions, and often in special circumstances. for example, |x| +   = |x +  | only when x ≥  . the picture shows another example. the composition of one-to-one (injective) functions is always one-to-one. similarly, the composition of onto (surjective) functions is always onto. it follows that the composition of two bijections is also a bijection. the inverse function of a composition (assumed invertible) has the property that (f ∘ g)−  = g− ∘ f− . derivatives of compositions involving differentiable functions can be found using the chain rule. higher derivatives of such functions are given by faà di bruno's formula. suppose one has two (or more) functions f: x → x, g: x → x having the same domain and codomain; these are often called transformations. then one can form chains of transformations composed together, such as f ∘ f ∘ g ∘ f. such chains have the algebraic structure of a monoid, called a transformation monoid or (much more seldom) a composition monoid. in general, transformation monoids can have remarkably complicated structure. one particular notable example is the de rham curve. the set of all functions f: x → x is called the full transformation semigroup or symmetric semigroup on x. (one can actually define two semigroups depending how one defines the semigroup operation as the left or right composition of functions. ) "
168,168,Function of real variable,1,https://en.wikipedia.org/wiki/Function_of_a_real_variable,"in mathematical analysis, and applications in geometry, applied mathematics, engineering, and natural sciences, a function of a real variable is a function whose domain is the real numbers r {\displaystyle \mathbb {r} } , or a subset of r {\displaystyle \mathbb {r} } that contains an interval of positive length. most real functions that are considered and studied are differentiable in some interval. the most widely considered such functions are the real functions, which are the real-valued functions of a real variable, that is, the functions of a real variable whose codomain is the set of real numbers. nevertheless, the codomain of a function of a real variable may be any set. however, it is often assumed to have a structure of r {\displaystyle \mathbb {r} } -vector space over the reals. that is, the codomain may be a euclidean space, a coordinate vector, the set of matrices of real numbers of a given size, or an r {\displaystyle \mathbb {r} } -algebra, such as the complex numbers or the quaternions. the structure r {\displaystyle \mathbb {r} } -vector space of the codomain induces a structure of r {\displaystyle \mathbb {r} } -vector space on the functions. if the codomain has a structure of r {\displaystyle \mathbb {r} } -algebra, the same is true for the functions. the image of a function of a real variable is a curve in the codomain. in this context, a function that defines curve is called a parametric equation of the curve. when the codomain of a function of a real variable is a finite-dimensional vector space, the function may be viewed as a sequence of real functions. this is often used in applications. a real function is a function from a subset of r {\displaystyle \mathbb {r} } to r , {\displaystyle \mathbb {r} ,} where r {\displaystyle \mathbb {r} } denotes as usual the set of real numbers. that is, the domain of a real function is a subset r {\displaystyle \mathbb {r} } , and its codomain is r . {\displaystyle \mathbb {r} .} it is generally assumed that the domain contains an interval of positive length. for many commonly used real functions, the domain is the whole set of real numbers, and the function is continuous and differentiable at every point of the domain. one says that these functions are defined, continuous and differentiable everywhere. this is the case of: some functions are defined everywhere, but not continuous at some points. for example some functions are defined and continuous everywhere, but not everywhere differentiable. for example many common functions are not defined everywhere, but are continuous and differentiable everywhere where they are defined. for example: some functions are continuous in their whole domain, and not differentiable at some points. this is the case of: "
169,169,Canonical Representation of a positive integer,1,https://en.wikipedia.org/wiki/Fundamental_theorem_of_arithmetic,"in mathematics, the fundamental theorem of arithmetic, also called the unique factorization theorem and prime factorization theorem, states that every integer greater than   can be represented uniquely as a product of prime numbers, up to the order of the factors. for example, the theorem says two things about this example: first, that      can be represented as a product of primes, and second, that no matter how this is done, there will always be exactly four  s, one  , two  s, and no other primes in the product. the requirement that the factors be prime is necessary: factorizations containing composite numbers may not be unique (for example,    =   ⋅   =   ⋅   {\displaystyle   = \cdot  = \cdot  } ). this theorem is one of the main reasons why   is not considered a prime number: if   were prime, then factorization into primes would not be unique; for example,   =   ⋅   =   ⋅   ⋅   = … {\displaystyle  = \cdot  = \cdot  \cdot  =\ldots } this theorem generalizes to other algebraic structures, in particular to polynomial rings over a field. these structures are called unique factorization domains. book vii, propositions   ,    and   , and book ix, proposition    of euclid's elements are essentially the statement and proof of the fundamental theorem. if two numbers by multiplying one another make some number, and any prime number measure the product, it will also measure one of the original numbers. (in modern terminology: if a prime p divides the product ab, then p divides either a or b or both.) proposition    is referred to as euclid's lemma, and it is the key in the proof of the fundamental theorem of arithmetic. any composite number is measured by some prime number. (in modern terminology: every integer greater than one is divided evenly by some prime number.) proposition    is proved directly by infinite descent. "
170,170,Fundamental theorem of calculus,1,https://en.wikipedia.org/wiki/Fundamental_theorem_of_calculus,"the fundamental theorem of calculus is a theorem that links the concept of differentiating a function (calculating the gradient) with the concept of integrating a function (calculating the area under the curve). the two operations are inverses of each other apart from a constant value which is dependent on where one starts to compute area. the first part of the theorem, sometimes called the first fundamental theorem of calculus, states that one of the antiderivatives (also known as an indefinite integral), say f, of some function f may be obtained as the integral of f with a variable bound of integration. this implies the existence of antiderivatives for continuous functions. conversely, the second part of the theorem, sometimes called the second fundamental theorem of calculus, states that the integral of a function f over some interval can be computed by using any one, say f, of its infinitely many antiderivatives. this part of the theorem has key practical applications, because explicitly finding the antiderivative of a function by symbolic integration avoids numerical integration to compute integrals. the fundamental theorem of calculus relates differentiation and integration, showing that these two operations are essentially inverses of one another. before the discovery of this theorem, it was not recognized that these two operations were related. ancient greek mathematicians knew how to compute area via infinitesimals, an operation that we would now call integration. the origins of differentiation likewise predate the fundamental theorem of calculus by hundreds of years; for example, in the fourteenth century the notions of continuity of functions and motion were studied by the oxford calculators and other scholars. the historical relevance of the fundamental theorem of calculus is not the ability to calculate these operations, but the realization that the two seemingly distinct operations (calculation of geometric areas, and calculation of gradients) are actually closely related. the first published statement and proof of a rudimentary form of the fundamental theorem, strongly geometric in character, was by james gregory (    –    ). isaac barrow (    –    ) proved a more generalized version of the theorem, while his student isaac newton (    –    ) completed the development of the surrounding mathematical theory. gottfried leibniz (    –    ) systematized the knowledge into a calculus for infinitesimal quantities and introduced the notation used today. for a continuous function y = f(x) whose graph is plotted as a curve, each value of x has a corresponding area function a(x), representing the area beneath the curve between   and x. the function a(x) may not be known, but it is given that it represents the area under the curve. the area under the curve between x and x + h could be computed by finding the area between   and x + h, then subtracting the area between   and x. in other words, the area of this “strip” would be a(x + h) − a(x). there is another way to estimate the area of this same strip. as shown in the accompanying figure, h is multiplied by f(x) to find the area of a rectangle that is approximately the same size as this strip. so: in fact, this estimate becomes a perfect equality if we add the red portion of the ""excess"" area shown in the diagram. so: rearranging terms: "
171,171,Game Theory,1,https://en.wikipedia.org/wiki/Game_theory,"collective intelligence collective action self-organized criticality herd mentality phase transition agent-based modelling synchronization ant colony optimization particle swarm optimization swarm behaviour social network analysis small-world networks centrality motifs graph theory scaling robustness systems biology dynamic networks evolutionary computation genetic algorithms genetic programming artificial life machine learning evolutionary developmental biology artificial intelligence evolutionary robotics reaction–diffusion systems partial differential equations dissipative structures percolation cellular automata spatial ecology self-replication information theory entropy feedback goal-oriented homeostasis operationalization second-order cybernetics self-reference system dynamics systems science systems thinking sensemaking variety ordinary differential equations phase space attractors population dynamics chaos multistability bifurcation rational choice theory bounded rationality game theory is the study of mathematical models of strategic interactions among rational agents. it has applications in all fields of social science, as well as in logic, systems science and computer science. originally, it addressed two-person zero-sum games, in which each participant's gains or losses are exactly balanced by those of other participants. in the   st century, game theory applies to a wide range of behavioral relations; it is now an umbrella term for the science of logical decision making in humans, animals, as well as computers. modern game theory began with the idea of mixed-strategy equilibria in two-person zero-sum game and its proof by john von neumann. von neumann's original proof used the brouwer fixed-point theorem on continuous mappings into compact convex sets, which became a standard method in game theory and mathematical economics. his paper was followed by the      book theory of games and economic behavior, co-written with oskar morgenstern, which considered cooperative games of several players. the second edition of this book provided an axiomatic theory of expected utility, which allowed mathematical statisticians and economists to treat decision-making under uncertainty. "
172,172,Gamma Function,3,https://en.wikipedia.org/wiki/Gamma_function," in mathematics, the gamma function (represented by γ, the capital letter gamma from the greek alphabet) is one commonly used extension of the factorial function to complex numbers. the gamma function is defined for all complex numbers except the non-positive integers. for any positive integer n, derived by daniel bernoulli, for complex numbers with a positive real part, the gamma function is defined via a convergent improper integral: the gamma function then is defined as the analytic continuation of this integral function to a meromorphic function that is holomorphic in the whole complex plane except zero and the negative integers, where the function has simple poles. the gamma function has no zeroes, so the reciprocal gamma function  /γ(z) is an entire function. in fact, the gamma function corresponds to the mellin transform of the negative exponential function: other extensions of the factorial function do exist, but the gamma function is the most popular and useful. it is a component in various probability-distribution functions, and as such it is applicable in the fields of probability and statistics, as well as combinatorics. the gamma function can be seen as a solution to the following interpolation problem: a plot of the first few factorials makes clear that such a curve can be drawn, but it would be preferable to have a formula that precisely describes the curve, in which the number of operations does not depend on the size of x. the simple formula for the factorial, x! =   ×   × ⋯ × x, cannot be used directly for fractional values of x since it is only valid when x is a natural number (or positive integer). there are, relatively speaking, no such simple solutions for factorials; no finite combination of sums, products, powers, exponential functions, or logarithms will suffice to express x!; but it is possible to find a general formula for factorials using tools such as integrals and limits from calculus. a good solution to this is the gamma function. "
173,173,Gaussian elimination,2,https://en.wikipedia.org/wiki/Gaussian_elimination,"in mathematics, gaussian elimination, also known as row reduction, is an algorithm for solving systems of linear equations. it consists of a sequence of operations performed on the corresponding matrix of coefficients. this method can also be used to compute the rank of a matrix, the determinant of a square matrix, and the inverse of an invertible matrix. the method is named after carl friedrich gauss (    –    ) although some special cases of the method—albeit presented without proof—were known to chinese mathematicians as early as circa     ce. to perform row reduction on a matrix, one uses a sequence of elementary row operations to modify the matrix until the lower left-hand corner of the matrix is filled with zeros, as much as possible. there are three types of elementary row operations: using these operations, a matrix can always be transformed into an upper triangular matrix, and in fact one that is in row echelon form. once all of the leading coefficients (the leftmost nonzero entry in each row) are  , and every column containing a leading coefficient has zeros elsewhere, the matrix is said to be in reduced row echelon form. this final form is unique; in other words, it is independent of the sequence of row operations used. for example, in the following sequence of row operations (where two elementary operations on different rows are done at the first and third steps), the third and fourth matrices are the ones in row echelon form, and the final matrix is the unique reduced row echelon form. using row operations to convert a matrix into reduced row echelon form is sometimes called gauss–jordan elimination. in this case, the term gaussian elimination refers to the process until it has reached its upper triangular, or (unreduced) row echelon form. for computational reasons, when solving systems of linear equations, it is sometimes preferable to stop row operations before the matrix is completely reduced. the process of row reduction makes use of elementary row operations, and can be divided into two parts. the first part (sometimes called forward elimination) reduces a given system to row echelon form, from which one can tell whether there are no solutions, a unique solution, or infinitely many solutions. the second part (sometimes called back substitution) continues to use row operations until the solution is found; in other words, it puts the matrix into reduced row echelon form. another point of view, which turns out to be very useful to analyze the algorithm, is that row reduction produces a matrix decomposition of the original matrix. the elementary row operations may be viewed as the multiplication on the left of the original matrix by elementary matrices. alternatively, a sequence of elementary operations that reduces a single row may be viewed as multiplication by a frobenius matrix. then the first part of the algorithm computes an lu decomposition, while the second part writes the original matrix as the product of a uniquely determined invertible matrix and a uniquely determined reduced row echelon matrix. there are three types of elementary row operations which may be performed on the rows of a matrix: if the matrix is associated to a system of linear equations, then these operations do not change the solution set. therefore, if one's goal is to solve a system of linear equations, then using these row operations could make the problem easier. for each row in a matrix, if the row does not consist of only zeros, then the leftmost nonzero entry is called the leading coefficient (or pivot) of that row. so if two leading coefficients are in the same column, then a row operation of type   could be used to make one of those coefficients zero. then by using the row swapping operation, one can always order the rows so that for every non-zero row, the leading coefficient is to the right of the leading coefficient of the row above. if this is the case, then matrix is said to be in row echelon form. so the lower left part of the matrix contains only zeros, and all of the zero rows are below the non-zero rows. the word ""echelon"" is used here because one can roughly think of the rows being ranked by their size, with the largest being at the top and the smallest being at the bottom. for example, the following matrix is in row echelon form, and its leading coefficients are shown in red: "
174,174,Geometric Mean,1,https://en.wikipedia.org/wiki/Geometric_mean,"in mathematics, the geometric mean is a mean or average, which indicates the central tendency or typical value of a set of numbers by using the product of their values (as opposed to the arithmetic mean which uses their sum). the geometric mean is defined as the nth root of the product of n numbers, i.e., for a set of numbers x , x , ..., xn, the geometric mean is defined as or, equivalently, as the arithmetic mean in logscale: for instance, the geometric mean of two numbers, say   and  , is just the square root of their product, that is,   ⋅   =   {\displaystyle {\sqrt { \cdot  }}= } . as another example, the geometric mean of the three numbers  ,  , and  /   is the cube root of their product ( / ), which is  / , that is,   ⋅   ⋅   /      =   /   {\displaystyle {\sqrt[{ }]{ \cdot  \cdot  /  }}= / } . the geometric mean applies only to positive numbers. the geometric mean is often used for a set of numbers whose values are meant to be multiplied together or are exponential in nature, such as a set of growth figures: values of the human population or interest rates of a financial investment over time. it also applies to benchmarking, where it is particularly useful for computing means of speedup ratios: since the mean of  . x (half as fast) and  x (twice as fast) will be   (i.e., no speedup overall). the geometric mean can be understood in terms of geometry. the geometric mean of two numbers, a {\displaystyle a} and b {\displaystyle b} , is the length of one side of a square whose area is equal to the area of a rectangle with sides of lengths a {\displaystyle a} and b {\displaystyle b} . similarly, the geometric mean of three numbers, a {\displaystyle a} , b {\displaystyle b} , and c {\displaystyle c} , is the length of one edge of a cube whose volume is the same as that of a cuboid with sides whose lengths are equal to the three given numbers. the geometric mean is one of the three classical pythagorean means, together with the arithmetic mean and the harmonic mean. for all positive data sets containing at least one pair of unequal values, the harmonic mean is always the least of the three means, while the arithmetic mean is always the greatest of the three and the geometric mean is always in between (see inequality of arithmetic and geometric means.) the geometric mean of a data set { a   , a   , … , a n } {\textstyle \left\{a_{ },a_{ },\,\ldots ,\,a_{n}\right\}} is given by: the above figure uses capital pi notation to show a series of multiplications. each side of the equal sign shows that a set of values is multiplied in succession (the number of values is represented by ""n"") to give a total product of the set, and then the nth root of the total product is taken to give the geometric mean of the original set. for example, in a set of four numbers {   ,   ,   ,   } {\textstyle \{ , , , \}} , the product of   ×   ×   ×   {\textstyle  \times  \times  \times  } is    {\textstyle   } , and the geometric mean is the fourth root of   , or ~  .   . the exponent   n {\textstyle {\frac { }{n}}} on the left side is equivalent to the taking nth root. for example,        =      {\textstyle   ^{\frac { }{ }}={\sqrt[{ }]{  }}} . the geometric mean of a data set is less than the data set's arithmetic mean unless all members of the data set are equal, in which case the geometric and arithmetic means are equal. this allows the definition of the arithmetic-geometric mean, an intersection of the two which always lies in between. the geometric mean is also the arithmetic-harmonic mean in the sense that if two sequences ( a n {\textstyle a_{n}} ) and ( h n {\textstyle h_{n}} ) are defined: "
175,175,Geometric probability,1,https://en.wikipedia.org/wiki/Geometric_probability,"problems of the following type, and their solution techniques, were first studied in the   th century, and the general topic became known as geometric probability. for mathematical development see the concise monograph by solomon. since the late   th century, the topic has split into two topics with different emphases. integral geometry sprang from the principle that the mathematically natural probability models are those that are invariant under certain transformation groups. this topic emphasises systematic development of formulas for calculating expected values associated with the geometric objects derived from random points, and can in part be viewed as a sophisticated branch of multivariate calculus. stochastic geometry emphasises the random geometrical objects themselves. for instance: different models for random lines or for random tessellations of the plane; random sets formed by making points of a spatial poisson process be (say) centers of discs. "
176,176,Infinite Geometric Progrtession,2,https://en.wikipedia.org/wiki/Geometric_progression,"in mathematics, a geometric progression, also known as a geometric sequence, is a sequence of non-zero numbers where each term after the first is found by multiplying the previous one by a fixed, non-zero number called the common ratio. for example, the sequence  ,  ,   ,   , ... is a geometric progression with common ratio  . similarly   ,  ,  . ,  .  , ... is a geometric sequence with common ratio  / . examples of a geometric sequence are powers rk of a fixed non-zero number r, such as  k and  k. the general form of a geometric sequence is where r ≠   is the common ratio and a ≠   is a scale factor, equal to the sequence's start value. the distinction between a progression and a series is that a progression is a sequence, whereas a series is a sum. the n-th term of a geometric sequence with initial value a = a  and common ratio r is given by such a geometric sequence also follows the recursive relation generally, to check whether a given sequence is geometric, one simply checks whether successive entries in the sequence all have the same ratio. the common ratio of a geometric sequence may be negative, resulting in an alternating sequence, with numbers alternating between positive and negative. for instance is a geometric sequence with common ratio − . the behaviour of a geometric sequence depends on the value of the common ratio. if the common ratio is: "
177,177,Geometric Series,1,https://en.wikipedia.org/wiki/Geometric_series,"in mathematics, a geometric series is the sum of an infinite number of terms that have a constant ratio between successive terms. for example, the series is geometric, because each successive term can be obtained by multiplying the previous term by  / . in general, a geometric series is written as a + ar + ar  + ar  + ... , where a is the coefficient of each term and r is the common ratio between adjacent terms. geometric series are among the simplest examples of infinite series and can serve as an introduction to taylor series and fourier series. geometric series had an important role in the early development of calculus, are used throughout mathematics, and have important applications in physics, engineering, biology, economics, computer science, queueing theory, and finance. the name geometric series indicates each term is the geometric mean of its two neighboring terms, similar to how the name arithmetic series indicates each term is the arithmetic mean of its two neighboring terms. the sequence of geometric series terms (without any of the additions) is called a geometric sequence or, equivalently, a geometric progression. the geometric series a + ar + ar  + ar  + ... is written in expanded form. every coefficient in the geometric series is the same. in contrast, the power series written as a  + a r + a r  + a r  + ... in expanded form has coefficients ai that can vary from term to term. in other words, the geometric series is a special case of the power series. the first term of a geometric series in expanded form is the coefficient a of that geometric series. in addition to the expanded form of the geometric series, there is a generator form of the geometric series written as and a closed form of the geometric series written as the derivation of the closed form from the expanded form is shown in this article's sum section. however even without that derivation, the result can be confirmed with long division: a divided by (  - r) results in a + ar + ar  + ar  + ... , which is the expanded form of the geometric series. it is often a convenience in notation to set the series equal to the sum s and work with the geometric series typically a geometric series is thought of as a sum of numbers a + ar + ar  + ar  + ... but can also be thought of as a sum of functions a + ar + ar  + ar  + ... that converges to the function a / (  - r) within the range |r| <  . the adjacent image shows the contribution each of the first nine terms (i.e., functions) make to the function a / (  - r) within the range |r| <   when a =  . changing even one of the coefficients to something other than coefficient a would (in addition to changing the geometric series to a power series) change the resulting sum of functions to some function other than a / (  - r) within the range |r| <  . as an aside, a particularly useful change to the coefficients is defined by the taylor series, which describes how to change the coefficients so that the sum of functions converges to any user selected, sufficiently smooth function within a range. the geometric series a + ar + ar  + ar  + ... is an infinite series defined by just two parameters: coefficient a and common ratio r. common ratio r is the ratio of any term with the previous term in the series. or equivalently, common ratio r is the term multiplier used to calculate the next term in the series. the following table shows several geometric series: "
178,178,Golden Ratio,0,https://en.wikipedia.org/wiki/Golden_ratio," in mathematics, two quantities are in the golden ratio if their ratio is the same as the ratio of their sum to the larger of the two quantities. expressed algebraically, for quantities a {\displaystyle a} and b {\displaystyle b} with a > b >   , {\displaystyle a>b> ,} where the greek letter phi ( φ {\displaystyle \varphi } or ϕ {\displaystyle \phi } ) represents the golden ratio.[a] it is an irrational number that is a solution to the quadratic equation x   − x −   =   , {\displaystyle x^{ }-x- = ,} with a value of the golden ratio is also called the golden mean or golden section (latin: sectio aurea). other names include extreme and mean ratio, medial section, divine proportion (latin: proportio divina), divine section (latin: sectio divina), golden proportion, golden cut, and golden number. mathematicians since euclid have studied the properties of the golden ratio, including its appearance in the dimensions of a regular pentagon and in a golden rectangle, which may be cut into a square and a smaller rectangle with the same aspect ratio. the golden ratio has also been used to analyze the proportions of natural objects as well as man-made systems such as financial markets, in some cases based on dubious fits to data. the golden ratio appears in some patterns in nature, including the spiral arrangement of leaves and other parts of vegetation. some   th-century artists and architects, including le corbusier and salvador dalí, have proportioned their works to approximate the golden ratio, believing this to be aesthetically pleasing. these often appear in the form of the golden rectangle, in which the ratio of the longer side to the shorter is the golden ratio. two quantities a {\displaystyle a} and b {\displaystyle b} are said to be in the golden ratio φ {\displaystyle \varphi } if one method for finding the value of φ {\displaystyle \varphi } is to start with the left fraction. through simplifying the fraction and substituting in b / a =   / φ , {\displaystyle b/a= /\varphi ,} therefore, multiplying by φ {\displaystyle \varphi } gives "
179,179,Goodness of fit,0,https://en.wikipedia.org/wiki/Goodness_of_fit,"the goodness of fit of a statistical model describes how well it fits a set of observations. measures of goodness of fit typically summarize the discrepancy between observed values and the values expected under the model in question. such measures can be used in statistical hypothesis testing, e.g. to test for normality of residuals, to test whether two samples are drawn from identical distributions (see kolmogorov–smirnov test), or whether outcome frequencies follow a specified distribution (see pearson's chi-square test). in the analysis of variance, one of the components into which the variance is partitioned may be a lack-of-fit sum of squares. in assessing whether a given distribution is suited to a data-set, the following tests and their underlying measures of fit can be used: in regression analysis, the following topics relate to goodness of fit: the following are examples that arise in the context of categorical data. pearson's chi-square test uses a measure of goodness of fit which is the sum of differences between observed and expected outcome frequencies (that is, counts of observations), each squared and divided by the expectation: the expected frequency is calculated by: the resulting value can be compared with a chi-square distribution to determine the goodness of fit. the chi-square distribution has (k − c) degrees of freedom, where k is the number of non-empty cells and c is the number of estimated parameters (including location and scale parameters and shape parameters) for the distribution plus one. for example, for a  -parameter weibull distribution, c =  . for example, to test the hypothesis that a random sample of     people has been drawn from a population in which men and women are equal in frequency, the observed number of men and women would be compared to the theoretical frequencies of    men and    women. if there were    men in the sample and    women, then "
180,180,Gradient,0,https://en.wikipedia.org/wiki/Gradient,"in vector calculus, the gradient of a scalar-valued differentiable function f of several variables is the vector field (or vector-valued function) ∇ f {\displaystyle \nabla f} whose value at a point p {\displaystyle p} is the vector[a] whose components are the partial derivatives of f {\displaystyle f} at p {\displaystyle p} . that is, for f : r n → r {\displaystyle f\colon \mathbb {r} ^{n}\to \mathbb {r} } , its gradient ∇ f : r n → r n {\displaystyle \nabla f\colon \mathbb {r} ^{n}\to \mathbb {r} ^{n}} is defined at the point p = ( x   , … , x n ) {\displaystyle p=(x_{ },\ldots ,x_{n})} in n-dimensional space as the vector[b] the nabla symbol ∇ {\displaystyle \nabla } , written as an upside-down triangle and pronounced ""del"", denotes the vector differential operator. the gradient vector can be interpreted as the ""direction and rate of fastest increase"". if the gradient of a function is non-zero at a point p, the direction of the gradient is the direction in which the function increases most quickly from p, and the magnitude of the gradient is the rate of increase in that direction, the greatest absolute directional derivative. further, the gradient is the zero vector at a point if and only if it is a stationary point (where the derivative vanishes). the gradient thus plays a fundamental role in optimization theory, where it is used to maximize a function by gradient ascent. the gradient is dual to the total derivative d f {\displaystyle df} : the value of the gradient at a point is a tangent vector – a vector at each point; while the value of the derivative at a point is a cotangent vector – a linear function on vectors.[c] they are related in that the dot product of the gradient of f at a point p with another tangent vector v equals the directional derivative of f at p of the function along v; that is, ∇ f ( p ) ⋅ v = ∂ f ∂ v ( p ) = d f p ( v ) {\textstyle \nabla f(p)\cdot \mathbf {v} ={\frac {\partial f}{\partial \mathbf {v} }}(p)=df_{p}(\mathbf {v} )} . the gradient admits multiple generalizations to more general functions on manifolds; see § generalizations. consider a room where the temperature is given by a scalar field, t, so at each point (x, y, z) the temperature is t(x, y, z), independent of time. at each point in the room, the gradient of t at that point will show the direction in which the temperature rises most quickly, moving away from (x, y, z). the magnitude of the gradient will determine how fast the temperature rises in that direction. consider a surface whose height above sea level at point (x, y) is h(x, y). the gradient of h at a point is a plane vector pointing in the direction of the steepest slope or grade at that point. the steepness of the slope at that point is given by the magnitude of the gradient vector. the gradient can also be used to measure how a scalar field changes in other directions, rather than just the direction of greatest change, by taking a dot product. suppose that the steepest slope on a hill is   %. a road going directly uphill has slope   %, but a road going around the hill at an angle will have a shallower slope. for example, if the road is at a   ° angle from the uphill direction (when both directions are projected onto the horizontal plane), then the slope along the road will be the dot product between the gradient vector and a unit vector along the road, namely   % times the cosine of   °, or   %. more generally, if the hill height function h is differentiable, then the gradient of h dotted with a unit vector gives the slope of the hill in the direction of the vector, the directional derivative of h along the unit vector. the gradient of a function f {\displaystyle f} at point a {\displaystyle a} is usually written as ∇ f ( a ) {\displaystyle \nabla f(a)} . it may also be denoted by any of the following: the gradient (or gradient vector field) of a scalar function f(x , x , x , …, xn) is denoted ∇f or ∇→f where ∇ (nabla) denotes the vector differential operator, del. the notation grad f is also commonly used to represent the gradient. the gradient of f is defined as the unique vector field whose dot product with any vector v at each point x is the directional derivative of f along v. that is, "
181,181,Gram matrix,2,https://en.wikipedia.org/wiki/Gram_matrix,"in linear algebra, the gram matrix (or gramian matrix, gramian) of a set of vectors v   , … , v n {\displaystyle v_{ },\dots ,v_{n}} in an inner product space is the hermitian matrix of inner products, whose entries are given by the inner product g i j = ⟨ v i , v j ⟩ {\displaystyle g_{ij}=\left\langle v_{i},v_{j}\right\rangle } . if the vectors v   , … , v n {\displaystyle v_{ },\dots ,v_{n}} are the columns of matrix x {\displaystyle x} then the gram matrix is x † x {\displaystyle x^{\dagger }x} in the general case that the vector coordinates are complex numbers, which simplifies to x ⊤ x {\displaystyle x^{\top }x} for the case that the vector coordinates are real numbers. an important application is to compute linear independence: a set of vectors are linearly independent if and only if the gram determinant (the determinant of the gram matrix) is non-zero. it is named after jørgen pedersen gram. for finite-dimensional real vectors in r n {\displaystyle \mathbb {r} ^{n}} with the usual euclidean dot product, the gram matrix is g = v ⊤ v {\displaystyle g=v^{\top }v} , where v {\displaystyle v} is a matrix whose columns are the vectors v k {\displaystyle v_{k}} and v ⊤ {\displaystyle v^{\top }} is its transpose whose rows are the vectors v k ⊤ {\displaystyle v_{k}^{\top }} . for complex vectors in c n {\displaystyle \mathbb {c} ^{n}} , g = v † v {\displaystyle g=v^{\dagger }v} , where v † {\displaystyle v^{\dagger }} is the conjugate transpose of v {\displaystyle v} . given square-integrable functions { ℓ i ( ⋅ ) , i =   , … , n } {\displaystyle \{\ell _{i}(\cdot ),\,i= ,\dots ,n\}} on the interval [ t   , t f ] {\displaystyle \left[t_{ },t_{f}\right]} , the gram matrix g = [ g i j ] {\displaystyle g=\left[g_{ij}\right]} is: where ℓ i ∗ ( τ ) {\displaystyle \ell _{i}^{*}(\tau )} is the complex conjugate of ℓ i ( τ ) {\displaystyle \ell _{i}(\tau )} . for any bilinear form b {\displaystyle b} on a finite-dimensional vector space over any field we can define a gram matrix g {\displaystyle g} attached to a set of vectors v   , … , v n {\displaystyle v_{ },\dots ,v_{n}} by g i j = b ( v i , v j ) {\displaystyle g_{ij}=b\left(v_{i},v_{j}\right)} . the matrix will be symmetric if the bilinear form b {\displaystyle b} is symmetric. the gram matrix is symmetric in the case the real product is real-valued; it is hermitian in the general, complex case by definition of an inner product. the gram matrix is positive semidefinite, and every positive semidefinite matrix is the gramian matrix for some set of vectors. the fact that the gramian matrix is positive-semidefinite can be seen from the following simple derivation: the first equality follows from the definition of matrix multiplication, the second and third from the bi-linearity of the inner-product, and the last from the positive definiteness of the inner product. note that this also shows that the gramian matrix is positive definite if and only if the vectors v i {\displaystyle v_{i}} are linearly independent (that is, ∑ i x i v i ≠   {\textstyle \sum _{i}x_{i}v_{i}\neq  } for all x {\displaystyle x} ). "
182,182,Graph Theory,1,https://en.wikipedia.org/wiki/Graph_theory,"in mathematics, graph theory is the study of graphs, which are mathematical structures used to model pairwise relations between objects. a graph in this context is made up of vertices (also called nodes or points) which are connected by edges (also called links or lines). a distinction is made between undirected graphs, where edges link two vertices symmetrically, and directed graphs, where edges link two vertices asymmetrically. graphs are one of the principal objects of study in discrete mathematics. definitions in graph theory vary. the following are some of the more basic ways of defining graphs and related mathematical structures. in one restricted but very common sense of the term, a graph is an ordered pair g = ( v , e ) {\displaystyle g=(v,e)} comprising: to avoid ambiguity, this type of object may be called precisely an undirected simple graph. in the edge { x , y } {\displaystyle \{x,y\}} , the vertices x {\displaystyle x} and y {\displaystyle y} are called the endpoints of the edge. the edge is said to join x {\displaystyle x} and y {\displaystyle y} and to be incident on x {\displaystyle x} and on y {\displaystyle y} . a vertex may exist in a graph and not belong to an edge. multiple edges, not allowed under the definition above, are two or more edges that join the same two vertices. in one more general sense of the term allowing multiple edges, a graph is an ordered triple g = ( v , e , ϕ ) {\displaystyle g=(v,e,\phi )} comprising: to avoid ambiguity, this type of object may be called precisely an undirected multigraph. a loop is an edge that joins a vertex to itself. graphs as defined in the two definitions above cannot have loops, because a loop joining a vertex x {\displaystyle x} to itself is the edge (for an undirected simple graph) or is incident on (for an undirected multigraph) { x , x } = { x } {\displaystyle \{x,x\}=\{x\}} which is not in { { x , y } ∣ x , y ∈ v and x ≠ y } {\displaystyle \{\{x,y\}\mid x,y\in v\;{\textrm {and}}\;x\neq y\}} . so to allow loops the definitions must be expanded. for undirected simple graphs, the definition of e {\displaystyle e} should be modified to e ⊆ { { x , y } ∣ x , y ∈ v } {\displaystyle e\subseteq \{\{x,y\}\mid x,y\in v\}} . for undirected multigraphs, the definition of ϕ {\displaystyle \phi } should be modified to ϕ : e → { { x , y } ∣ x , y ∈ v } {\displaystyle \phi :e\to \{\{x,y\}\mid x,y\in v\}} . to avoid ambiguity, these types of objects may be called undirected simple graph permitting loops and undirected multigraph permitting loops (sometimes also undirected pseudograph), respectively. v {\displaystyle v} and e {\displaystyle e} are usually taken to be finite, and many of the well-known results are not true (or are rather different) for infinite graphs because many of the arguments fail in the infinite case. moreover, v {\displaystyle v} is often assumed to be non-empty, but e {\displaystyle e} is allowed to be the empty set. the order of a graph is | v | {\displaystyle |v|} , its number of vertices. the size of a graph is | e | {\displaystyle |e|} , its number of edges. the degree or valency of a vertex is the number of edges that are incident to it, where a loop is counted twice. the degree of a graph is the maximum of the degrees of its vertices. in an undirected simple graph of order n, the maximum degree of each vertex is n −   and the maximum size of the graph is n(n −  )/ . "
183,183,Green's therom,3,https://en.wikipedia.org/wiki/Green%27s_theorem,"in vector calculus, green's theorem relates a line integral around a simple closed curve c to a double integral over the plane region d bounded by c. it is the two-dimensional special case of stokes' theorem. let c be a positively oriented, piecewise smooth, simple closed curve in a plane, and let d be the region bounded by c. if l and m are functions of (x, y) defined on an open region containing d and having continuous partial derivatives there, then where the path of integration along c is anticlockwise. in physics, green's theorem finds many applications. one is solving two-dimensional flow integrals, stating that the sum of fluid outflowing from a volume is equal to the total outflow summed about an enclosing area. in plane geometry, and in particular, area surveying, green's theorem can be used to determine the area and centroid of plane figures solely by integrating over the perimeter. the following is a proof of half of the theorem for the simplified area d, a type i region where c  and c  are curves connected by vertical lines (possibly of zero length). a similar proof exists for the other half of the theorem when d is a type ii region where c  and c  are curves connected by horizontal lines (again, possibly of zero length). putting these two parts together, the theorem is thus proven for regions of type iii (defined as regions which are both type i and type ii). the general case can then be deduced from this special case by decomposing d into a set of type iii regions. if it can be shown that "
184,184,grouped data,1,https://en.wikipedia.org/wiki/Grouped_data,"grouped data are data formed by aggregating individual observations of a variable into groups, so that a frequency distribution of these groups serves as a convenient means of summarizing or analyzing the data. there are two major types of grouping: data binning of a single-dimensional variable, replacing individual numbers by counts in bins; and grouping multi-dimensional variables by some of the dimensions (especially by independent variables), obtaining the distribution of ungrouped dimensions (especially the dependent variables). the idea of grouped data can be illustrated by considering the following raw dataset: the above data can be grouped in order to construct a frequency distribution in any of several ways. one method is to use intervals as a basis. the smallest value in the above data is   and the largest is   . the interval from   to    is broken up into smaller subintervals (called class intervals). for each class interval, the number of data items falling in this interval is counted. this number is called the frequency of that class interval. the results are tabulated as a frequency table as follows: another method of grouping the data is to use some qualitative characteristics instead of numerical intervals. for example, suppose in the above example, there are three types of students:  ) below normal, if the response time is   to    seconds,  ) normal if it is between    and    seconds, and  ) above normal if it is    seconds or more, then the grouped data looks like: yet another example of grouping the data is the use of some commonly used numerical values, which are in fact ""names"" we assign to the categories. for example, let us look at the age distribution of the students in a class. the students may be    years old,    years old or    years old. these are the age groups,   ,   , and   . note that the students in age group    are from    years and   days, to    years and     days old, and their average age is   .  years old if we look at age in a continuous scale. the grouped data looks like: an estimate, x ¯ {\displaystyle {\bar {x}}} , of the mean of the population from which the data are drawn can be calculated from the grouped data as: in this formula, x refers to the midpoint of the class intervals, and f is the class frequency. note that the result of this will be different from the sample mean of the ungrouped data. the mean for the grouped data in the above example, can be calculated as follows: thus, the mean of the grouped data is the mean for the grouped data in example   above can be calculated as follows: "
185,185,Harmonic Analysis,1,https://en.wikipedia.org/wiki/Harmonic_analysis," harmonic analysis is a branch of mathematics concerned with the representation of functions or signals as the superposition of basic waves, and the study of and generalization of the notions of fourier series and fourier transforms (i.e. an extended form of fourier analysis). in the past two centuries, it has become a vast subject with applications in areas as diverse as number theory, representation theory, signal processing, quantum mechanics, tidal analysis and neuroscience. the term ""harmonics"" originated as the ancient greek word harmonikos, meaning ""skilled in music"". in physical eigenvalue problems, it began to mean waves whose frequencies are integer multiples of one another, as are the frequencies of the harmonics of music notes, but the term has been generalized beyond its original meaning. the classical fourier transform on rn is still an area of ongoing research, particularly concerning fourier transformation on more general objects such as tempered distributions. for instance, if we impose some requirements on a distribution f, we can attempt to translate these requirements in terms of the fourier transform of f. the paley–wiener theorem is an example of this. the paley–wiener theorem immediately implies that if f is a nonzero distribution of compact support (these include functions of compact support), then its fourier transform is never compactly supported (i.e. if a signal is limited in one domain, it is unlimited in the other). this is a very elementary form of an uncertainty principle in a harmonic-analysis setting. fourier series can be conveniently studied in the context of hilbert spaces, which provides a connection between harmonic analysis and functional analysis. there are four versions of the fourier transform, dependent on the spaces that are mapped by the transformation (discrete/periodic-discrete/periodic: digital fourier transform, continuous/periodic-discrete/aperiodic: fourier analysis, discrete/aperiodic-continuous/periodic: fourier synthesis, continuous/aperiodic-continuous/aperiodic: continuous fourier transform). one of the most modern branches of harmonic analysis, having its roots in the mid-  th century, is analysis on topological groups. the core motivating ideas are the various fourier transforms, which can be generalized to a transform of functions defined on hausdorff locally compact topological groups. the theory for abelian locally compact groups is called pontryagin duality. harmonic analysis studies the properties of that duality and fourier transform and attempts to extend those features to different settings, for instance, to the case of non-abelian lie groups. for general non-abelian locally compact groups, harmonic analysis is closely related to the theory of unitary group representations. for compact groups, the peter–weyl theorem explains how one may get harmonics by choosing one irreducible representation out of each equivalence class of representations. this choice of harmonics enjoys some of the useful properties of the classical fourier transform in terms of carrying convolutions to pointwise products, or otherwise showing a certain understanding of the underlying group structure. see also: non-commutative harmonic analysis. if the group is neither abelian nor compact, no general satisfactory theory is currently known (""satisfactory"" means at least as strong as the plancherel theorem). however, many specific cases have been analyzed, for example sln. in this case, representations in infinite dimensions play a crucial role. "
186,186,Harmonic Mean,1,https://en.wikipedia.org/wiki/Harmonic_mean,"in mathematics, the harmonic mean is one of several kinds of average, and in particular, one of the pythagorean means. sometimes it is appropriate for situations when the average rate is desired. the harmonic mean can be expressed as the reciprocal of the arithmetic mean of the reciprocals of the given set of observations. as a simple example, the harmonic mean of  ,  , and   is the harmonic mean h of the positive real numbers x   , x   , … , x n {\displaystyle x_{ },x_{ },\ldots ,x_{n}} is defined to be the third formula in the above equation expresses the harmonic mean as the reciprocal of the arithmetic mean of the reciprocals. from the following formula: it is more apparent that the harmonic mean is related to the arithmetic and geometric means. it is the reciprocal dual of the arithmetic mean for positive inputs: the harmonic mean is a schur-concave function, and dominated by the minimum of its arguments, in the sense that for any positive set of arguments, min ( x   … x n ) ≤ h ( x   … x n ) ≤ n min ( x   … x n ) {\displaystyle \min(x_{ }\ldots x_{n})\leq h(x_{ }\ldots x_{n})\leq n\min(x_{ }\ldots x_{n})} . thus, the harmonic mean cannot be made arbitrarily large by changing some values to bigger ones (while having at least one value unchanged). the harmonic mean is also concave, which is an even stronger property than schur-concavity. one has to take care to only use positive numbers though, since the mean fails to be concave if negative values are used. the harmonic mean is one of the three pythagorean means. for all positive data sets containing at least one pair of nonequal values, the harmonic mean is always the least of the three means, while the arithmetic mean is always the greatest of the three and the geometric mean is always in between. (if all values in a nonempty dataset are equal, the three means are always equal to one another; e.g., the harmonic, geometric, and arithmetic means of { ,  ,  } are all  .) it is the special case m−  of the power mean: "
187,187,Harmonic progression,1,https://en.wikipedia.org/wiki/Harmonic_progression,harmonic progression may refer to: 
188,188,Harmonic progression (mathematics),1,https://en.wikipedia.org/wiki/Harmonic_progression_(mathematics),"in mathematics, a harmonic progression (or harmonic sequence) is a progression formed by taking the reciprocals of an arithmetic progression. equivalently, a sequence is a harmonic progression when each term is the harmonic mean of the neighboring terms. as a third equivalent characterization, it is an infinite sequence of the form where a is not zero and −a/d is not a natural number, or a finite sequence of the form where a is not zero, k is a natural number and −a/d is not a natural number or is greater than k. infinite harmonic progressions are not summable (sum to infinity). it is not possible for a harmonic progression of distinct unit fractions (other than the trivial case where a =   and k =  ) to sum to an integer. the reason is that, necessarily, at least one denominator of the progression will be divisible by a prime number that does not divide any other denominator. if collinear points a, b, c, and d are such that d is the harmonic conjugate of c with respect to a and b, then the distances from any one of these points to the three remaining points form harmonic progression. specifically, each of the sequences ac, ab, ad; bc, ba, bd; ca, cd, cb; and da, dc, db are harmonic progressions, where each of the distances is signed according to a fixed orientation of the line. in a triangle, if the altitudes are in arithmetic progression, then the sides are in harmonic progression. an excellent example of harmonic progression is the leaning tower of lire. in it, uniform blocks are stacked on top of each other to achieve the maximum sideways or lateral distance covered. the blocks are stacked  / ,  / ,  / ,  / ,  /  , … distance sideways below the original block. this ensures that the center of gravity is just at the center of the structure so that it does not collapse. a slight increase in weight on the structure causes it to become unstable and fall. "
189,189,Hermitian matrix,2,https://en.wikipedia.org/wiki/Hermitian_matrix," in mathematics, a hermitian matrix (or self-adjoint matrix) is a complex square matrix that is equal to its own conjugate transpose—that is, the element in the i-th row and j-th column is equal to the complex conjugate of the element in the j-th row and i-th column, for all indices i and j: a hermitian ⟺ a i j = a j i ¯ {\displaystyle a{\text{ hermitian}}\quad \iff \quad a_{ij}={\overline {{a}_{ji}}}} or in matrix form: hermitian matrices can be understood as the complex extension of real symmetric matrices. if the conjugate transpose of a matrix a {\displaystyle a} is denoted by a h {\displaystyle a^{\mathsf {h}}} , then the hermitian property can be written concisely as a hermitian ⟺ a = a h {\displaystyle a{\text{ hermitian}}\quad \iff \quad a=a^{\mathsf {h}}} hermitian matrices are named after charles hermite, who demonstrated in      that matrices of this form share a property with real symmetric matrices of always having real eigenvalues. other, equivalent notations in common use are a h = a † = a ∗ {\displaystyle a^{\mathsf {h}}=a^{\dagger }=a^{\ast }} , although note that in quantum mechanics, a ∗ {\displaystyle a^{\ast }} typically means the complex conjugate only, and not the conjugate transpose. hermitian matrices can be characterized in a number of equivalent ways, some of which are listed below: a square matrix a {\displaystyle a} is hermitian if and only if it is equal to its adjoint, that is, it satisfies "
190,190,hessian matrix,2,https://en.wikipedia.org/wiki/Hessian_matrix,"in mathematics, the hessian matrix or hessian is a square matrix of second-order partial derivatives of a scalar-valued function, or scalar field. it describes the local curvature of a function of many variables. the hessian matrix was developed in the   th century by the german mathematician ludwig otto hesse and later named after him. hesse originally used the term ""functional determinants"". suppose f : r n → r {\displaystyle f:\mathbb {r} ^{n}\to \mathbb {r} } is a function taking as input a vector x ∈ r n {\displaystyle \mathbf {x} \in \mathbb {r} ^{n}} and outputting a scalar f ( x ) ∈ r . {\displaystyle f(\mathbf {x} )\in \mathbb {r} .} if all second partial derivatives of f {\displaystyle f} exist and are continuous over the domain of the function, then the hessian matrix h {\displaystyle \mathbf {h} } of f {\displaystyle f} is a square n × n {\displaystyle n\times n} matrix, usually defined and arranged as follows: the hessian matrix is a symmetric matrix, since the hypothesis of continuity of the second derivatives implies that the order of differentiation does not matter (schwarz's theorem). the determinant of the hessian matrix is called the hessian determinant. the hessian matrix of a function f {\displaystyle f} is the jacobian matrix of the gradient of the function f {\displaystyle f} ; that is: h ( f ( x ) ) = j ( ∇ f ( x ) ) . {\displaystyle \mathbf {h} (f(\mathbf {x} ))=\mathbf {j} (\nabla f(\mathbf {x} )).} if f {\displaystyle f} is a homogeneous polynomial in three variables, the equation f =   {\displaystyle f= } is the implicit equation of a plane projective curve. the inflection points of the curve are exactly the non-singular points where the hessian determinant is zero. it follows by bézout's theorem that a cubic plane curve has at most   {\displaystyle  } inflection points, since the hessian determinant is a polynomial of degree  . {\displaystyle  .} the hessian matrix of a convex function is positive semi-definite. refining this property allows us to test whether a critical point x {\displaystyle x} is a local maximum, local minimum, or a saddle point, as follows: if the hessian is positive-definite at x , {\displaystyle x,} then f {\displaystyle f} attains an isolated local minimum at x . {\displaystyle x.} if the hessian is negative-definite at x , {\displaystyle x,} then f {\displaystyle f} attains an isolated local maximum at x . {\displaystyle x.} if the hessian has both positive and negative eigenvalues, then x {\displaystyle x} is a saddle point for f . {\displaystyle f.} otherwise the test is inconclusive. this implies that at a local minimum the hessian is positive-semidefinite, and at a local maximum the hessian is negative-semidefinite. for positive-semidefinite and negative-semidefinite hessians the test is inconclusive (a critical point where the hessian is semidefinite but not definite may be a local extremum or a saddle point). however, more can be said from the point of view of morse theory. the second-derivative test for functions of one and two variables is simpler than the general case. in one variable, the hessian contains exactly one second derivative; if it is positive, then x {\displaystyle x} is a local minimum, and if it is negative, then x {\displaystyle x} is a local maximum; if it is zero, then the test is inconclusive. in two variables, the determinant can be used, because the determinant is the product of the eigenvalues. if it is positive, then the eigenvalues are both positive, or both negative. if it is negative, then the two eigenvalues have different signs. if it is zero, then the second-derivative test is inconclusive. "
191,191,Hidden_markov_model,0,https://en.wikipedia.org/wiki/Hidden_Markov_model," hidden markov model (hmm) is a statistical markov model in which the system being modeled is assumed to be a markov process — call it x {\displaystyle x} — with unobservable (""hidden"") states. as part of the definition, hmm requires that there be an observable process y {\displaystyle y} whose outcomes are ""influenced"" by the outcomes of x {\displaystyle x} in a known way. since x {\displaystyle x} cannot be observed directly, the goal is to learn about x {\displaystyle x} by observing y . {\displaystyle y.} hmm has an additional requirement that the outcome of y {\displaystyle y} at time t = t   {\displaystyle t=t_{ }} may be ""influenced"" exclusively by the outcome of x {\displaystyle x} at t = t   {\displaystyle t=t_{ }} and that the outcomes of x {\displaystyle x} and y {\displaystyle y} at t < t   {\displaystyle t<t_{ }} must not affect the outcome of y {\displaystyle y} at t = t   . {\displaystyle t=t_{ }.} hidden markov models are known for their applications to thermodynamics, statistical mechanics, physics, chemistry, economics, finance, signal processing, information theory, pattern recognition - such as speech, handwriting, gesture recognition, part-of-speech tagging, musical score following, partial discharges and bioinformatics. let x n {\displaystyle x_{n}} and y n {\displaystyle y_{n}} be discrete-time stochastic processes and n ≥   {\displaystyle n\geq  } . the pair ( x n , y n ) {\displaystyle (x_{n},y_{n})} is a hidden markov model if let x t {\displaystyle x_{t}} and y t {\displaystyle y_{t}} be continuous-time stochastic processes. the pair ( x t , y t ) {\displaystyle (x_{t},y_{t})} is a hidden markov model if the states of the process x n {\displaystyle x_{n}} (resp. x t ) {\displaystyle x_{t})} are called hidden states, and p ⁡ ( y n ∈ a ∣ x n = x n ) {\displaystyle \operatorname {\mathbf {p} } {\bigl (}y_{n}\in a\mid x_{n}=x_{n}{\bigr )}} (resp. p ⁡ ( y t ∈ a ∣ x t ∈ b t ) ) {\displaystyle \operatorname {\mathbf {p} } {\bigl (}y_{t}\in a\mid x_{t}\in b_{t}{\bigr )})} is called emission probability or output probability. in its discrete form, a hidden markov process can be visualized as a generalization of the urn problem with replacement (where each item from the urn is returned to the original urn before the next step). consider this example: in a room that is not visible to an observer there is a genie. the room contains urns x , x , x , ... each of which contains a known mix of balls, each ball labeled y , y , y , ... . the genie chooses an urn in that room and randomly draws a ball from that urn. it then puts the ball onto a conveyor belt, where the observer can observe the sequence of the balls but not the sequence of urns from which they were drawn. the genie has some procedure to choose urns; the choice of the urn for the n-th ball depends only upon a random number and the choice of the urn for the (n −  )-th ball. the choice of urn does not directly depend on the urns chosen before this single previous urn; therefore, this is called a markov process. it can be described by the upper part of figure  . the markov process itself cannot be observed, only the sequence of labeled balls, thus this arrangement is called a ""hidden markov process"". this is illustrated by the lower part of the diagram shown in figure  , where one can see that balls y , y , y , y  can be drawn at each state. even if the observer knows the composition of the urns and has just observed a sequence of three balls, e.g. y , y  and y  on the conveyor belt, the observer still cannot be sure which urn (i.e., at which state) the genie has drawn the third ball from. however, the observer can work out other information, such as the likelihood that the third ball came from each of the urns. consider two friends, alice and bob, who live far apart from each other and who talk together daily over the telephone about what they did that day. bob is only interested in three activities: walking in the park, shopping, and cleaning his apartment. the choice of what to do is determined exclusively by the weather on a given day. alice has no definite information about the weather, but she knows general trends. based on what bob tells her he did each day, alice tries to guess what the weather must have been like. alice believes that the weather operates as a discrete markov chain. there are two states, ""rainy"" and ""sunny"", but she cannot observe them directly, that is, they are hidden from her. on each day, there is a certain chance that bob will perform one of the following activities, depending on the weather: ""walk"", ""shop"", or ""clean"". since bob tells alice about his activities, those are the observations. the entire system is that of a hidden markov model (hmm). "
192,192,Higher-Order Logic,2,https://en.wikipedia.org/wiki/Higher-order_logic,"in mathematics and logic, a higher-order logic is a form of predicate logic that is distinguished from first-order logic by additional quantifiers and, sometimes, stronger semantics. higher-order logics with their standard semantics are more expressive, but their model-theoretic properties are less well-behaved than those of first-order logic. the term ""higher-order logic"", abbreviated as hol, is commonly used to mean higher-order simple predicate logic. here ""simple"" indicates that the underlying type theory is the theory of simple types, also called the simple theory of types (see type theory). leon chwistek and frank p. ramsey proposed this as a simplification of the complicated and clumsy ramified theory of types specified in the principia mathematica by alfred north whitehead and bertrand russell. simple types is nowadays sometimes also meant to exclude polymorphic and dependent types. first-order logic quantifies only variables that range over individuals; second-order logic, in addition, also quantifies over sets; third-order logic also quantifies over sets of sets, and so on. higher-order logic is the union of first-, second-, third-, ..., nth-order logic; i.e., higher-order logic admits quantification over sets that are nested arbitrarily deeply. there are two possible semantics for higher-order logic. in the standard or full semantics, quantifiers over higher-type objects range over all possible objects of that type. for example, a quantifier over sets of individuals ranges over the entire powerset of the set of individuals. thus, in standard semantics, once the set of individuals is specified, this is enough to specify all the quantifiers. hol with standard semantics is more expressive than first-order logic. for example, hol admits categorical axiomatizations of the natural numbers, and of the real numbers, which are impossible with first-order logic. however, by a result of kurt gödel, hol with standard semantics does not admit an effective, sound, and complete proof calculus. the model-theoretic properties of hol with standard semantics are also more complex than those of first-order logic. for example, the löwenheim number of second-order logic is already larger than the first measurable cardinal, if such a cardinal exists. the löwenheim number of first-order logic, in contrast, is ℵ , the smallest infinite cardinal. in henkin semantics, a separate domain is included in each interpretation for each higher-order type. thus, for example, quantifiers over sets of individuals may range over only a subset of the powerset of the set of individuals. hol with these semantics is equivalent to many-sorted first-order logic, rather than being stronger than first-order logic. in particular, hol with henkin semantics has all the model-theoretic properties of first-order logic, and has a complete, sound, effective proof system inherited from first-order logic. higher order logics include the offshoots of church's simple theory of types and the various forms of intuitionistic type theory. gérard huet has shown that unifiability is undecidable in a type-theoretic flavor of third-order logic, that is, there can be no algorithm to decide whether an arbitrary equation between third-order (let alone arbitrary higher-order) terms has a solution. up to a certain notion of isomorphism, the powerset operation is definable in second-order logic. using this observation, jaakko hintikka established in      that second-order logic can simulate higher-order logics in the sense that for every formula of a higher order-logic, one can find an equisatisfiable formula for it in second-order logic. the term ""higher-order logic"" is assumed in some context to refer to classical higher-order logic. however, modal higher-order logic has been studied as well. according to several logicians, gödel's ontological proof is best studied (from a technical perspective) in such a context. "
193,193,Histogram,1,https://en.wikipedia.org/wiki/Histogram,"a histogram is an approximate representation of the distribution of numerical data. it was first introduced by karl pearson. to construct a histogram, the first step is to ""bin"" (or ""bucket"") the range of values—that is, divide the entire range of values into a series of intervals—and then count how many values fall into each interval. the bins are usually specified as consecutive, non-overlapping intervals of a variable. the bins (intervals) must be adjacent and are often (but not required to be) of equal size. if the bins are of equal size, a rectangle is erected over the bin with height proportional to the frequency—the number of cases in each bin. a histogram may also be normalized to display ""relative"" frequencies. it then shows the proportion of cases that fall into each of several categories, with the sum of the heights equaling  . however, bins need not be of equal width; in that case, the erected rectangle is defined to have its area proportional to the frequency of cases in the bin. the vertical axis is then not the frequency but frequency density—the number of cases per unit of the variable on the horizontal axis. examples of variable bin width are displayed on census bureau data below. as the adjacent bins leave no gaps, the rectangles of a histogram touch each other to indicate that the original variable is continuous. histograms give a rough sense of the density of the underlying distribution of the data, and often for density estimation: estimating the probability density function of the underlying variable. the total area of a histogram used for probability density is always normalized to  . if the length of the intervals on the x-axis are all  , then a histogram is identical to a relative frequency plot. a histogram can be thought of as a simplistic kernel density estimation, which uses a kernel to smooth frequencies over the bins. this yields a smoother probability density function, which will in general more accurately reflect distribution of the underlying variable. the density estimate could be plotted as an alternative to the histogram, and is usually drawn as a curve rather than a set of boxes. histograms are nevertheless preferred in applications, when their statistical properties need to be modeled. the correlated variation of a kernel density estimate is very difficult to describe mathematically, while it is simple for a histogram where each bin varies independently. an alternative to kernel density estimation is the average shifted histogram, which is fast to compute and gives a smooth curve estimate of the density without using kernels. the histogram is one of the seven basic tools of quality control. histograms are sometimes confused with bar charts. a histogram is used for continuous data, where the bins represent ranges of data, while a bar chart is a plot of categorical variables. some authors recommend that bar charts have gaps between the rectangles to clarify the distinction. this is the data for the histogram to the right, using     items: "
194,194,Hoeffding's Lemma,2,https://en.wikipedia.org/wiki/Hoeffding%27s_lemma,"in probability theory, hoeffding's lemma is an inequality that bounds the moment-generating function of any bounded random variable. it is named after the finnish–american mathematical statistician wassily hoeffding. the proof of hoeffding's lemma uses taylor's theorem and jensen's inequality. hoeffding's lemma is itself used in the proof of mcdiarmid's inequality. let x be any real-valued random variable such that a ≤ x ≤ b {\displaystyle a\leq x\leq b} almost surely, i.e. with probability one. then, for all λ ∈ r {\displaystyle \lambda \in \mathbb {r} } , or equivalently, without loss of generality, by replacing x {\displaystyle x} by x − e [ x ] {\displaystyle x-\mathbb {e} [x]} , we can assume e [ x ] =   {\displaystyle \mathbb {e} [x]= } , so that a ≤   ≤ b {\displaystyle a\leq  \leq b} . since e λ x {\displaystyle e^{\lambda x}} is a convex function of x {\displaystyle x} , we have that for all x ∈ [ a , b ] {\displaystyle x\in [a,b]} , so, where l ( h ) = h a b − a + ln ⁡ (   + a − e h a b − a ) {\displaystyle l(h)={\frac {ha}{b-a}}+\ln( +{\frac {a-e^{h}a}{b-a}})} . by computing derivatives, we can conclude from taylor's theorem, for some   ≤ θ ≤   {\displaystyle  \leq \theta \leq  } hence, e [ e λ x ] ≤ e     λ   ( b − a )   {\displaystyle \mathbb {e} \left[e^{\lambda x}\right]\leq e^{{\frac { }{ }}\lambda ^{ }(b-a)^{ }}} . "
195,195,Homogenous differential equation,3,https://en.wikipedia.org/wiki/Homogeneous_differential_equation,"a differential equation can be homogeneous in either of two respects. a first order differential equation is said to be homogeneous if it may be written where f and g are homogeneous functions of the same degree of x and y. in this case, the change of variable y = ux leads to an equation of the form which is easy to solve by integration of the two members. otherwise, a differential equation is homogeneous if it is a homogeneous function of the unknown function and its derivatives. in the case of linear differential equations, this means that there are no constant terms. the solutions of any linear ordinary differential equation of any order may be deduced by integration from the solution of the homogeneous equation obtained by removing the constant term. the term homogeneous was first applied to differential equations by johann bernoulli in section   of his      article de integraionibus aequationum differentialium (on the integration of differential equations). a first-order ordinary differential equation in the form: is a homogeneous type if both functions m(x, y) and n(x, y) are homogeneous functions of the same degree n. that is, multiplying each variable by a parameter λ, we find thus, in the quotient m ( t x , t y ) n ( t x , t y ) = m ( x , y ) n ( x , y ) {\textstyle {\frac {m(tx,ty)}{n(tx,ty)}}={\frac {m(x,y)}{n(x,y)}}} , we can let t =  /x to simplify this quotient to a function f of the single variable y/x: "
196,196,Homogenous Polynomial,1,https://en.wikipedia.org/wiki/Homogeneous_polynomial,"in mathematics, a homogeneous polynomial, sometimes called quantic in older texts, is a polynomial whose nonzero terms all have the same degree. for example, x   +   x   y   +   x y   {\displaystyle x^{ }+ x^{ }y^{ }+ xy^{ }} is a homogeneous polynomial of degree  , in two variables; the sum of the exponents in each term is always  . the polynomial x   +   x   y + z   {\displaystyle x^{ }+ x^{ }y+z^{ }} is not homogeneous, because the sum of exponents does not match from term to term. the function defined by a homogeneous polynomial is always a homogeneous function. an algebraic form, or simply form, is a function defined by a homogeneous polynomial. a binary form is a form in two variables. a form is also a function defined on a vector space, which may be expressed as a homogeneous function of the coordinates over any basis. a polynomial of degree   is always homogeneous; it is simply an element of the field or ring of the coefficients, usually called a constant or a scalar. a form of degree   is a linear form. a form of degree   is a quadratic form. in geometry, the euclidean distance is the square root of a quadratic form. homogeneous polynomials are ubiquitous in mathematics and physics. they play a fundamental role in algebraic geometry, as a projective algebraic variety is defined as the set of the common zeros of a set of homogeneous polynomials. a homogeneous polynomial defines a homogeneous function. this means that, if a multivariate polynomial p is homogeneous of degree d, then for every λ {\displaystyle \lambda } in any field containing the coefficients of p. conversely, if the above relation is true for infinitely many λ {\displaystyle \lambda } then the polynomial is homogeneous of degree d. in particular, if p is homogeneous then for every λ . {\displaystyle \lambda .} this property is fundamental in the definition of a projective variety. any nonzero polynomial may be decomposed, in a unique way, as a sum of homogeneous polynomials of different degrees, which are called the homogeneous components of the polynomial. given a polynomial ring r = k [ x   , … , x n ] {\displaystyle r=k[x_{ },\ldots ,x_{n}]} over a field (or, more generally, a ring) k, the homogeneous polynomials of degree d form a vector space (or a module), commonly denoted r d . {\displaystyle r_{d}.} the above unique decomposition means that r {\displaystyle r} is the direct sum of the r d {\displaystyle r_{d}} (sum over all nonnegative integers). "
197,197,hyperbola,2,https://en.wikipedia.org/wiki/Hyperbola,"in mathematics, a hyperbola (/haɪˈpɜːrbələ/ (listen); pl. hyperbolas or hyperbolae /-liː/ (listen); adj. hyperbolic /ˌhaɪpərˈbɒlɪk/ (listen)) is a type of smooth curve lying in a plane, defined by its geometric properties or by equations for which it is the solution set. a hyperbola has two pieces, called connected components or branches, that are mirror images of each other and resemble two infinite bows. the hyperbola is one of the three kinds of conic section, formed by the intersection of a plane and a double cone. (the other conic sections are the parabola and the ellipse. a circle is a special case of an ellipse.) if the plane intersects both halves of the double cone but does not pass through the apex of the cones, then the conic is a hyperbola. hyperbolas arise in many ways: and so on. each branch of the hyperbola has two arms which become straighter (lower curvature) further out from the center of the hyperbola. diagonally opposite arms, one from each branch, tend in the limit to a common line, called the asymptote of those two arms. so there are two asymptotes, whose intersection is at the center of symmetry of the hyperbola, which can be thought of as the mirror point about which each branch reflects to form the other branch. in the case of the curve y ( x ) =   / x {\displaystyle y(x)= /x} the asymptotes are the two coordinate axes. hyperbolas share many of the ellipses' analytical properties such as eccentricity, focus, and directrix. typically the correspondence can be made with nothing more than a change of sign in some term. many other mathematical objects have their origin in the hyperbola, such as hyperbolic paraboloids (saddle surfaces), hyperboloids (""wastebaskets""), hyperbolic geometry (lobachevsky's celebrated non-euclidean geometry), hyperbolic functions (sinh, cosh, tanh, etc.), and gyrovector spaces (a geometry proposed for use in both relativity and quantum mechanics which is not euclidean). the word ""hyperbola"" derives from the greek ὑπερβολή, meaning ""over-thrown"" or ""excessive"", from which the english term hyperbole also derives. hyperbolae were discovered by menaechmus in his investigations of the problem of doubling the cube, but were then called sections of obtuse cones. the term hyperbola is believed to have been coined by apollonius of perga (c.    –c.     bc) in his definitive work on the conic sections, the conics. the names of the other two general conic sections, the ellipse and the parabola, derive from the corresponding greek words for ""deficient"" and ""applied""; all three names are borrowed from earlier pythagorean terminology which referred to a comparison of the side of rectangles of fixed area with a given line segment. the rectangle could be ""applied"" to the segment (meaning, have an equal length), be shorter than the segment or exceed the segment. a hyperbola can be defined geometrically as a set of points (locus of points) in the euclidean plane: the midpoint m {\displaystyle m} of the line segment joining the foci is called the center of the hyperbola. the line through the foci is called the major axis. it contains the vertices v   , v   {\displaystyle v_{ },v_{ }} , which have distance a {\displaystyle a} to the center. the distance c {\displaystyle c} of the foci to the center is called the focal distance or linear eccentricity. the quotient c a {\displaystyle {\tfrac {c}{a}}} is the eccentricity e {\displaystyle e} . the equation | | p f   | − | p f   | | =   a {\displaystyle ||pf_{ }|-|pf_{ }||= a} can be viewed in a different way (see diagram): if c   {\displaystyle c_{ }} is the circle with midpoint f   {\displaystyle f_{ }} and radius   a {\displaystyle  a} , then the distance of a point p {\displaystyle p} of the right branch to the circle c   {\displaystyle c_{ }} equals the distance to the focus f   {\displaystyle f_{ }} : c   {\displaystyle c_{ }} is called the circular directrix (related to focus f   {\displaystyle f_{ }} ) of the hyperbola. in order to get the left branch of the hyperbola, one has to use the circular directrix related to f   {\displaystyle f_{ }} . this property should not be confused with the definition of a hyperbola with help of a directrix (line) below. "
198,198,Hyperbolic functions,2,https://en.wikipedia.org/wiki/Hyperbolic_functions," in mathematics, hyperbolic functions are analogues of the ordinary trigonometric functions, but defined using the hyperbola rather than the circle. just as the points (cos t, sin t) form a circle with a unit radius, the points (cosh t, sinh t) form the right half of the unit hyperbola. also, similarly to how the derivatives of sin(t) and cos(t) are cos(t) and –sin(t), the derivatives of sinh(t) and cosh(t) are cosh(t) and +sinh(t). hyperbolic functions occur in the calculations of angles and distances in hyperbolic geometry. they also occur in the solutions of many linear differential equations (such as the equation defining a catenary), cubic equations, and laplace's equation in cartesian coordinates. laplace's equations are important in many areas of physics, including electromagnetic theory, heat transfer, fluid dynamics, and special relativity. the basic hyperbolic functions are: from which are derived: corresponding to the derived trigonometric functions. the inverse hyperbolic functions are: the hyperbolic functions take a real argument called a hyperbolic angle. the size of a hyperbolic angle is twice the area of its hyperbolic sector. the hyperbolic functions may be defined in terms of the legs of a right triangle covering this sector. in complex analysis, the hyperbolic functions arise as the imaginary parts of sine and cosine. the hyperbolic sine and the hyperbolic cosine are entire functions. as a result, the other hyperbolic functions are meromorphic in the whole complex plane. by lindemann–weierstrass theorem, the hyperbolic functions have a transcendental value for every non-zero algebraic value of the argument. "
199,199,Hyperboloid,3,https://en.wikipedia.org/wiki/Hyperboloid,"in geometry, a hyperboloid of revolution, sometimes called a circular hyperboloid, is the surface generated by rotating a hyperbola around one of its principal axes. a hyperboloid is the surface obtained from a hyperboloid of revolution by deforming it by means of directional scalings, or more generally, of an affine transformation. a hyperboloid is a quadric surface, that is, a surface defined as the zero set of a polynomial of degree two in three variables. among quadric surfaces, a hyperboloid is characterized by not being a cone or a cylinder, having a center of symmetry, and intersecting many planes into hyperbolas. a hyperboloid has three pairwise perpendicular axes of symmetry, and three pairwise perpendicular planes of symmetry. given a hyperboloid, if one chooses a cartesian coordinate system whose axes are the axes of symmetry of the hyperboloid and the origin is the center of symmetry of the hyperboloid, then the hyperboloid may be defined by one of the two following equations: or both surfaces are asymptotic to the cone of the equation the surface is a hyperboloid of revolution if and only if a   = b   . {\displaystyle a^{ }=b^{ }.} otherwise, the axes are uniquely defined (up to the exchange of the x-axis and the y-axis). there are two kinds of hyperboloids. in the first case (+  in the right-hand side of the equation): a one-sheet hyperboloid, also called a hyperbolic hyperboloid. it is a connected surface, which has a negative gaussian curvature at every point. this implies near every point the intersection of the hyperboloid and its tangent plane at the point consists of two branches of curve that have distinct tangents at the point. in the case of the one-sheet hyperboloid, these branches of curves are lines and thus the one-sheet hyperboloid is a doubly ruled surface. in the second case (−  in the right-hand side of the equation): a two-sheet hyperboloid, also called an elliptic hyperboloid. the surface has two connected components and a positive gaussian curvature at every point. thus the surface is convex in the sense that the tangent plane at every point intersects the surface only in this point. cartesian coordinates for the hyperboloids can be defined, similar to spherical coordinates, keeping the azimuth angle θ ∈ [ ,  π), but changing inclination v into hyperbolic trigonometric functions: one-surface hyperboloid: v ∈ (−∞, ∞) "
200,200,Hypergeometric Distribution,0,https://en.wikipedia.org/wiki/Hypergeometric_distribution,"   n k ( n − k ) ( n − n ) ( n −   ) ( n −   ) ⋅ {\displaystyle \left.{\frac { }{nk(n-k)(n-n)(n- )(n- )}}\cdot \right.} [ ( n −   ) n   ( n ( n +   ) −   k ( n − k ) −   n ( n − n ) ) + {\displaystyle {\big [}(n- )n^{ }{\big (}n(n+ )- k(n-k)- n(n-n){\big )}+{}} in probability theory and statistics, the hypergeometric distribution is a discrete probability distribution that describes the probability of k {\displaystyle k} successes (random draws for which the object drawn has a specified feature) in n {\displaystyle n} draws, without replacement, from a finite population of size n {\displaystyle n} that contains exactly k {\displaystyle k} objects with that feature, wherein each draw is either a success or a failure. in contrast, the binomial distribution describes the probability of k {\displaystyle k} successes in n {\displaystyle n} draws with replacement. the following conditions characterize the hypergeometric distribution: a random variable x {\displaystyle x} follows the hypergeometric distribution if its probability mass function (pmf) is given by where the pmf is positive when max (   , n + k − n ) ≤ k ≤ min ( k , n ) {\displaystyle \max( ,n+k-n)\leq k\leq \min(k,n)} . a random variable distributed hypergeometrically with parameters n {\displaystyle n} , k {\displaystyle k} and n {\displaystyle n} is written x ∼ hypergeometric ⁡ ( n , k , n ) {\textstyle x\sim \operatorname {hypergeometric} (n,k,n)} and has probability mass function p x ( k ) {\textstyle p_{x}(k)} above. as required, we have which essentially follows from vandermonde's identity from combinatorics. also note that "
201,201,Identity Functions,2,https://en.wikipedia.org/wiki/Identity_function,"in mathematics, an identity function, also called an identity relation, identity map or identity transformation, is a function that always returns the value that was used as its argument, unchanged. that is, when f is the identity function, the equality f(x) = x is true for all values of x to which f can be applied. formally, if m is a set, the identity function f on m is defined to be a function with m as its domain and codomain, satisfying in other words, the function value f(x) in the codomain m is always the same as the input element x in the domain m. the identity function on m is clearly an injective function as well as a surjective function, so it is bijective. the identity function f on m is often denoted by idm. in set theory, where a function is defined as a particular kind of binary relation, the identity function is given by the identity relation, or diagonal of m. if f : m → n is any function, then we have f ∘ idm = f = idn ∘ f (where ""∘"" denotes function composition). in particular, idm is the identity element of the monoid of all functions from m to m (under function composition). since the identity element of a monoid is unique, one can alternately define the identity function on m to be this identity element. such a definition generalizes to the concept of an identity morphism in category theory, where the endomorphisms of m need not be functions. "
202,202,Imaginary Numbers,1,https://en.wikipedia.org/wiki/Imaginary_number," an imaginary number is a real number multiplied by the imaginary unit i,[note  ] which is defined by its property i  = − . the square of an imaginary number bi is −b . for example,  i is an imaginary number, and its square is −  . by definition, zero is considered to be both real and imaginary. originally coined in the   th century by rené descartes as a derogatory term and regarded as fictitious or useless, the concept gained wide acceptance following the work of leonhard euler (in the   th century) and augustin-louis cauchy and carl friedrich gauss (in the early   th century). an imaginary number bi can be added to a real number a to form a complex number of the form a + bi, where the real numbers a and b are called, respectively, the real part and the imaginary part of the complex number. although the greek mathematician and engineer hero of alexandria is noted as the first to present a calculation involving the square root of a negative number, it was rafael bombelli who first set down the rules for multiplication of complex numbers in     . the concept had appeared in print earlier, such as in work by gerolamo cardano. at the time, imaginary numbers and negative numbers were poorly understood and were regarded by some as fictitious or useless, much as zero once was. many other mathematicians were slow to adopt the use of imaginary numbers, including rené descartes, who wrote about them in his la géométrie in which he coined the term imaginary and meant it to be derogatory. the use of imaginary numbers was not widely accepted until the work of leonhard euler (    –    ) and carl friedrich gauss (    –    ). the geometric significance of complex numbers as points in a plane was first described by caspar wessel (    –    ). in     , william rowan hamilton extended the idea of an axis of imaginary numbers in the plane to a four-dimensional space of quaternion imaginaries in which three of the dimensions are analogous to the imaginary numbers in the complex field. geometrically, imaginary numbers are found on the vertical axis of the complex number plane, which allows them to be presented perpendicular to the real axis. one way of viewing imaginary numbers is to consider a standard number line positively increasing in magnitude to the right and negatively increasing in magnitude to the left. at   on the x-axis, a y-axis can be drawn with ""positive"" direction going up; ""positive"" imaginary numbers then increase in magnitude upwards, and ""negative"" imaginary numbers increase in magnitude downwards. this vertical axis is often called the ""imaginary axis"" and is denoted i r , {\displaystyle i\mathbb {r} ,} i , {\displaystyle \mathbb {i} ,} [citation needed] or ℑ. in this representation, multiplication by –  corresponds to a rotation of     degrees about the origin, which is a half circle. multiplication by i corresponds to a rotation of    degrees about the origin which is a quarter of a circle. both these numbers are roots of   {\displaystyle  } : −     =   {\displaystyle - ^{ }= } , i   =   {\displaystyle i^{ }= } . in the field of complex numbers, for every n ∈ n {\displaystyle n\in \mathbb {n} } ,   {\displaystyle  } has n {\displaystyle n} th roots φ n {\displaystyle \varphi _{n}} , meaning φ n n =   {\displaystyle \varphi _{n}^{n}= } , called roots of unity. multiplying by the first n {\displaystyle n} th root of unity causes a rotation of     n {\displaystyle {\frac {   }{n}}} degrees about the origin. multiplying by a complex number is the same as rotating around the origin by the complex number's argument, followed by a scaling by its magnitude. care must be used when working with imaginary numbers that are expressed as the principal values of the square roots of negative numbers: "
203,203,Implicit Functions,1,https://en.wikipedia.org/wiki/Implicit_function,"in mathematics, an implicit equation is a relation of the form r(x , …, xn) =  , where r is a function of several variables (often a polynomial). for example, the implicit equation of the unit circle is x  + y  −   =  . an implicit function is a function that is defined by an implicit equation, that relates one of the variables, considered as the value of the function, with the others considered as the arguments. :    –    for example, the equation x  + y  −   =   of the unit circle defines y as an implicit function of x if −  ≤ x ≤  , and one restricts y to nonnegative values. the implicit function theorem provides conditions under which some kinds of relations define an implicit function, namely relations defined as the indicator function of the zero set of some continuously differentiable multivariate function. a common type of implicit function is an inverse function. not all functions have a unique inverse function. if g is a function of x that has a unique inverse, then the inverse function of g, called g− , is the unique function giving a solution of the equation for x in terms of y. this solution can then be written as defining g−  as the inverse of g is an implicit definition. for some functions g, g− (y) can be written out explicitly as a closed-form expression — for instance, if g(x) =  x −  , then g− (y) =  / (y +  ). however, this is often not possible, or only by introducing a new notation (as in the product log example below). intuitively, an inverse function is obtained from g by interchanging the roles of the dependent and independent variables. example: the product log is an implicit function giving the solution for x of the equation y − xex =  . an algebraic function is a function that satisfies a polynomial equation whose coefficients are themselves polynomials. for example, an algebraic function in one variable x gives a solution for y of an equation where the coefficients ai(x) are polynomial functions of x. this algebraic function can be written as the right side of the solution equation y = f(x). written like this, f is a multi-valued implicit function. "
204,204,implicit function theorm,2,https://en.wikipedia.org/wiki/Implicit_function_theorem,"in mathematics, more specifically in multivariable calculus, the implicit function theorem[a] is a tool that allows relations to be converted to functions of several real variables. it does so by representing the relation as the graph of a function. there may not be a single function whose graph can represent the entire relation, but there may be such a function on a restriction of the domain of the relation. the implicit function theorem gives a sufficient condition to ensure that there is such a function. more precisely, given a system of m equations fi (x , ..., xn, y , ..., ym) =  , i =  , ..., m (often abbreviated into f(x, y) =  ), the theorem states that, under a mild condition on the partial derivatives (with respect to the yis) at a point, the m variables yi are differentiable functions of the xj in some neighborhood of the point. as these functions can generally not be expressed in closed form, they are implicitly defined by the equations, and this motivated the name of the theorem. in other words, under a mild condition on the partial derivatives, the set of zeros of a system of equations is locally the graph of a function. augustin-louis cauchy (    –    ) is credited with the first rigorous form of the implicit function theorem. ulisse dini (    –    ) generalized the real-variable version of the implicit function theorem to the context of functions of any number of real variables. if we define the function f(x, y) = x  + y , then the equation f(x, y) =   cuts out the unit circle as the level set {(x, y) | f(x, y) =  }. there is no way to represent the unit circle as the graph of a function of one variable y = g(x) because for each choice of x ∈ (− ,  ), there are two choices of y, namely ±   − x   {\displaystyle \pm {\sqrt { -x^{ }}}} . however, it is possible to represent part of the circle as the graph of a function of one variable. if we let g   ( x ) =   − x   {\displaystyle g_{ }(x)={\sqrt { -x^{ }}}} for −  ≤ x ≤  , then the graph of y = g (x) provides the upper half of the circle. similarly, if g   ( x ) = −   − x   {\displaystyle g_{ }(x)=-{\sqrt { -x^{ }}}} , then the graph of y = g (x) gives the lower half of the circle. the purpose of the implicit function theorem is to tell us the existence of functions like g (x) and g (x), even in situations where we cannot write down explicit formulas. it guarantees that g (x) and g (x) are differentiable, and it even works in situations where we do not have a formula for f(x, y). let f : r n + m → r m {\displaystyle f:\mathbb {r} ^{n+m}\to \mathbb {r} ^{m}} be a continuously differentiable function. we think of r n + m {\displaystyle \mathbb {r} ^{n+m}} as the cartesian product r n × r m , {\displaystyle \mathbb {r} ^{n}\times \mathbb {r} ^{m},} and we write a point of this product as ( x , y ) = ( x   , … , x n , y   , … y m ) . {\displaystyle (\mathbf {x} ,\mathbf {y} )=(x_{ },\ldots ,x_{n},y_{ },\ldots y_{m}).} starting from the given function f {\displaystyle f} , our goal is to construct a function g : r n → r m {\displaystyle g:\mathbb {r} ^{n}\to \mathbb {r} ^{m}} whose graph ( x , g ( x ) ) {\displaystyle ({\textbf {x}},g({\textbf {x}}))} is precisely the set of all ( x , y ) {\displaystyle ({\textbf {x}},{\textbf {y}})} such that f ( x , y ) =   {\displaystyle f({\textbf {x}},{\textbf {y}})={\textbf { }}} . as noted above, this may not always be possible. we will therefore fix a point ( a , b ) = ( a   , … , a n , b   , … , b m ) {\displaystyle ({\textbf {a}},{\textbf {b}})=(a_{ },\dots ,a_{n},b_{ },\dots ,b_{m})} which satisfies f ( a , b ) =   {\displaystyle f({\textbf {a}},{\textbf {b}})={\textbf { }}} , and we will ask for a g {\displaystyle g} that works near the point ( a , b ) {\displaystyle ({\textbf {a}},{\textbf {b}})} . in other words, we want an open set u ⊂ r n {\displaystyle u\subset \mathbb {r} ^{n}} containing a {\displaystyle {\textbf {a}}} , an open set v ⊂ r m {\displaystyle v\subset \mathbb {r} ^{m}} containing b {\displaystyle {\textbf {b}}} , and a function g : u → v {\displaystyle g:u\to v} such that the graph of g {\displaystyle g} satisfies the relation f =   {\displaystyle f={\textbf { }}} on u × v {\displaystyle u\times v} , and that no other points within u × v {\displaystyle u\times v} do so. in symbols, "
205,205,Improper integral,2,https://en.wikipedia.org/wiki/Improper_integral,"in mathematical analysis, an improper integral is the limit of a definite integral as an endpoint of the interval(s) of integration approaches either a specified real number or positive or negative infinity; or in some instances as both endpoints approach limits. such an integral is often written symbolically just like a standard definite integral, in some cases with infinity as a limit of integration. specifically, an improper integral is a limit of the form: or in which one takes a limit in one or the other (or sometimes both) endpoints (apostol     , §  .  ). by abuse of notation, improper integrals are often written symbolically just like standard definite integrals, perhaps with infinity among the limits of integration. when the definite integral exists (in the sense of either the riemann integral or the more powerful lebesgue integral), this ambiguity is resolved as both the proper and improper integral will coincide in value. often one is able to compute values for improper integrals, even when the function is not integrable in the conventional sense (as a riemann integral, for instance) because of a singularity in the function or because one of the bounds of integration is infinite. the original definition of the riemann integral does not apply to a function such as   / x   {\displaystyle  /{x^{ }}} on the interval [ , ∞), because in this case the domain of integration is unbounded. however, the riemann integral can often be extended by continuity, by defining the improper integral instead as a limit the narrow definition of the riemann integral also does not cover the function   / x {\textstyle  /{\sqrt {x}}} on the interval [ ,  ]. the problem here is that the integrand is unbounded in the domain of integration (the definition requires that both the domain of integration and the integrand be bounded). however, the improper integral does exist if understood as the limit sometimes integrals may have two singularities where they are improper. consider, for example, the function  /((x +  )√x) integrated from   to ∞ (shown right). at the lower bound, as x goes to   the function goes to ∞, and the upper bound is itself ∞, though the function goes to  . thus this is a doubly improper integral. integrated, say, from   to  , an ordinary riemann sum suffices to produce a result of π/ . to integrate from   to ∞, a riemann sum is not possible. however, any finite upper bound, say t (with t >  ), gives a well-defined result,   arctan(√t) − π/ . this has a finite limit as t goes to infinity, namely π/ . similarly, the integral from  /  to   allows a riemann sum as well, coincidentally again producing π/ . replacing  /  by an arbitrary positive value s (with s <  ) is equally safe, giving π/  −   arctan(√s). this, too, has a finite limit as s goes to zero, namely π/ . combining the limits of the two fragments, the result of this improper integral is this process does not guarantee success; a limit might fail to exist, or might be infinite. for example, over the bounded interval from   to   the integral of  /x does not converge; and over the unbounded interval from   to ∞ the integral of  /√x does not converge. "
206,206,Incenter,2,https://en.wikipedia.org/wiki/Incenter," in geometry, the incenter of a triangle is a triangle center, a point defined for any triangle in a way that is independent of the triangle's placement or scale. the incenter may be equivalently defined as the point where the internal angle bisectors of the triangle cross, as the point equidistant from the triangle's sides, as the junction point of the medial axis and innermost point of the grassfire transform of the triangle, and as the center point of the inscribed circle of the triangle. together with the centroid, circumcenter, and orthocenter, it is one of the four triangle centers known to the ancient greeks, and the only one that does not in general lie on the euler line. it is the first listed center, x( ), in clark kimberling's encyclopedia of triangle centers, and the identity element of the multiplicative group of triangle centers. for polygons with more than three sides, the incenter only exists for tangential polygons - those that have an incircle that is tangent to each side of the polygon. in this case the incenter is the center of this circle and is equally distant from all sides. it is a theorem in euclidean geometry that the three interior angle bisectors of a triangle meet in a single point. in euclid's elements, proposition   of book iv proves that this point is also the center of the inscribed circle of the triangle. the incircle itself may be constructed by dropping a perpendicular from the incenter to one of the sides of the triangle and drawing a circle with that segment as its radius. the incenter lies at equal distances from the three line segments forming the sides of the triangle, and also from the three lines containing those segments. it is the only point equally distant from the line segments, but there are three more points equally distant from the lines, the excenters, which form the centers of the excircles of the given triangle. the incenter and excenters together form an orthocentric system. the medial axis of a polygon is the set of points whose nearest neighbor on the polygon is not unique: these points are equidistant from two or more sides of the polygon. one method for computing medial axes is using the grassfire transform, in which one forms a continuous sequence of offset curves, each at some fixed distance from the polygon; the medial axis is traced out by the vertices of these curves. in the case of a triangle, the medial axis consists of three segments of the angle bisectors, connecting the vertices of the triangle to the incenter, which is the unique point on the innermost offset curve. the straight skeleton, defined in a similar way from a different type of offset curve, coincides with the medial axis for convex polygons and so also has its junction at the incenter. let the bisection of ∠ b a c {\displaystyle \angle {bac}} and b c ¯ {\displaystyle {\overline {bc}}} meet at d {\displaystyle d} , and the bisection of ∠ a b c {\displaystyle \angle {abc}} and a c ¯ {\displaystyle {\overline {ac}}} meet at e {\displaystyle e} , and a d ¯ {\displaystyle {\overline {ad}}} and b e ¯ {\displaystyle {\overline {be}}} meet at i {\displaystyle {i}} . and let c i ¯ {\displaystyle {\overline {ci}}} and a b ¯ {\displaystyle {\overline {ab}}} meet at f {\displaystyle {f}} . then we have to prove that c i ¯ {\displaystyle {\overline {ci}}} is the bisection of ∠ a c b {\displaystyle \angle {acb}} . "
207,207,Incircle and excircles of a triangle,3,https://en.wikipedia.org/wiki/Incircle_and_excircles_of_a_triangle,"in geometry, the incircle or inscribed circle of a triangle is the largest circle contained in the triangle; it touches (is tangent to) the three sides. the center of the incircle is a triangle center called the triangle's incenter. an excircle or escribed circle of the triangle is a circle lying outside the triangle, tangent to one of its sides and tangent to the extensions of the other two. every triangle has three distinct excircles, each tangent to one of the triangle's sides. the center of the incircle, called the incenter, can be found as the intersection of the three internal angle bisectors. the center of an excircle is the intersection of the internal bisector of one angle (at vertex a {\displaystyle a} , for example) and the external bisectors of the other two. the center of this excircle is called the excenter relative to the vertex a {\displaystyle a} , or the excenter of a {\displaystyle a} . because the internal bisector of an angle is perpendicular to its external bisector, it follows that the center of the incircle together with the three excircle centers form an orthocentric system. : p.     all regular polygons have incircles tangent to all sides, but not all polygons do; those that do are tangential polygons. see also tangent lines to circles. suppose △ a b c {\displaystyle \triangle abc} has an incircle with radius r {\displaystyle r} and center i {\displaystyle i} . let a {\displaystyle a} be the length of b c {\displaystyle bc} , b {\displaystyle b} the length of a c {\displaystyle ac} , and c {\displaystyle c} the length of a b {\displaystyle ab} . also let t a {\displaystyle t_{a}} , t b {\displaystyle t_{b}} , and t c {\displaystyle t_{c}} be the touchpoints where the incircle touches b c {\displaystyle bc} , a c {\displaystyle ac} , and a b {\displaystyle ab} . the incenter is the point where the internal angle bisectors of ∠ a b c , ∠ b c a , and ∠ b a c {\displaystyle \angle abc,\angle bca,{\text{ and }}\angle bac} meet. the distance from vertex a {\displaystyle a} to the incenter i {\displaystyle i} is:[citation needed] the trilinear coordinates for a point in the triangle is the ratio of all the distances to the triangle sides. because the incenter is the same distance from all sides of the triangle, the trilinear coordinates for the incenter are the barycentric coordinates for a point in a triangle give weights such that the point is the weighted average of the triangle vertex positions. barycentric coordinates for the incenter are given by[citation needed] where a {\displaystyle a} , b {\displaystyle b} , and c {\displaystyle c} are the lengths of the sides of the triangle, or equivalently (using the law of sines) by "
208,208,independence of events,1,https://en.wikipedia.org/wiki/Independence_(probability_theory),"independence is a fundamental notion in probability theory, as in statistics and the theory of stochastic processes. two events are independent, statistically independent, or stochastically independent if, informally speaking, the occurrence of one does not affect the probability of occurrence of the other (equivalently, does not affect the odds). similarly, two random variables are independent if the realization of one does not affect the probability distribution of the other. when dealing with collections of more than two events, two notions of independence need to be distinguished. the events are called pairwise independent if any two events in the collection are independent of each other, while mutual independence (or collective independence) of events means, informally speaking, that each event is independent of any combination of other events in the collection. a similar notion exists for collections of random variables. mutual independence implies pairwise independence, but not the other way around. in the standard literature of probability theory, statistics, and stochastic processes, independence without further qualification usually refers to mutual independence. two events a {\displaystyle a} and b {\displaystyle b} are independent (often written as a ⊥ b {\displaystyle a\perp b} or a ⊥ ⊥ b {\displaystyle a\perp \!\!\!\perp b} ) iff (if and only if) their joint probability equals the product of their probabilities: : p.    : p.    (eq. )it indicates that two independent events a {\displaystyle a} and b {\displaystyle b} have common elements in their sample space so that they are not mutually exclusive (mutually exclusive iff a ∩ b = ∅ {\displaystyle a\cap b=\emptyset } ). why this defines independence is made clear by rewriting with conditional probabilities p ( a ∣ b ) = p ( a ∩ b ) p ( b ) {\displaystyle p(a\mid b)={\frac {p(a\cap b)}{p(b)}}} as the probability at which the event a {\displaystyle a} occurs provided that the event b {\displaystyle b} has or is assumed to have occurred: "
209,209,Index of dispersion,3,https://en.wikipedia.org/wiki/Index_of_dispersion,"in probability theory and statistics, the index of dispersion, dispersion index, coefficient of dispersion, relative variance, or variance-to-mean ratio (vmr), like the coefficient of variation, is a normalized measure of the dispersion of a probability distribution: it is a measure used to quantify whether a set of observed occurrences are clustered or dispersed compared to a standard statistical model. it is defined as the ratio of the variance σ   {\displaystyle \sigma ^{ }} to the mean μ {\displaystyle \mu } , it is also known as the fano factor, though this term is sometimes reserved for windowed data (the mean and variance are computed over a subpopulation), where the index of dispersion is used in the special case where the window is infinite. windowing data is frequently done: the vmr is frequently computed over various intervals in time or small regions in space, which may be called ""windows"", and the resulting statistic called the fano factor. it is only defined when the mean μ {\displaystyle \mu } is non-zero, and is generally only used for positive statistics, such as count data or time between events, or where the underlying distribution is assumed to be the exponential distribution or poisson distribution. in this context, the observed dataset may consist of the times of occurrence of predefined events, such as earthquakes in a given region over a given magnitude, or of the locations in geographical space of plants of a given species. details of such occurrences are first converted into counts of the numbers of events or occurrences in each of a set of equal-sized time- or space-regions. the above defines a dispersion index for counts. a different definition applies for a dispersion index for intervals, where the quantities treated are the lengths of the time-intervals between the events. common usage is that ""index of dispersion"" means the dispersion index for counts. some distributions, most notably the poisson distribution, have equal variance and mean, giving them a vmr =  . the geometric distribution and the negative binomial distribution have vmr >  , while the binomial distribution has vmr <  , and the constant random variable has vmr =  . this yields the following table: this can be considered analogous to the classification of conic sections by eccentricity; see cumulants of particular probability distributions for details. the relevance of the index of dispersion is that it has a value of one when the probability distribution of the number of occurrences in an interval is a poisson distribution. thus the measure can be used to assess whether observed data can be modeled using a poisson process. when the coefficient of dispersion is less than  , a dataset is said to be ""under-dispersed"": this condition can relate to patterns of occurrence that are more regular than the randomness associated with a poisson process. for instance, points spread uniformly in space or regular, periodic events will be under-dispersed. if the index of dispersion is larger than  , a dataset is said to be over-dispersed: this can correspond to the existence of clusters of occurrences. clumped, concentrated data is over-dispersed. a sample-based estimate of the dispersion index can be used to construct a formal statistical hypothesis test for the adequacy of the model that a series of counts follow a poisson distribution. in terms of the interval-counts, over-dispersion corresponds to there being more intervals with low counts and more intervals with high counts, compared to a poisson distribution: in contrast, under-dispersion is characterised by there being more intervals having counts close to the mean count, compared to a poisson distribution. "
210,210,Inellipse,2,https://en.wikipedia.org/wiki/Inellipse,"in triangle geometry, an inellipse is an ellipse that touches the three sides of a triangle. the simplest example is the incircle. further important inellipses are the steiner inellipse, which touches the triangle at the midpoints of its sides, the mandart inellipse and brocard inellipse (see examples section). for any triangle there exist an infinite number of inellipses. the steiner inellipse plays a special role: its area is the greatest of all inellipses. because a non-degenerate conic section is uniquely determined by five items out of the sets of vertices and tangents, in a triangle whose three sides are given as tangents one can specify only the points of contact on two sides. the third point of contact is then uniquely determined. the inellipse of the triangle with vertices and points of contact on o a {\displaystyle oa} and o b {\displaystyle ob} respectively can by described by the rational parametric representation where a , b {\displaystyle a,b} are uniquely determined by the choice of the points of contact: the third point of contact is the center of the inellipse is the vectors "
211,211,Inequation,2,https://en.wikipedia.org/wiki/Inequation,"in mathematics, an inequation is a statement that an inequality holds between two values. it is usually written in the form of a pair of expressions denoting the values in question, with a relational sign between them indicating the specific inequality relation. some examples of inequations are: in some cases, the term ""inequation"" can be considered synonymous to the term ""inequality"", while in other cases, an inequation is reserved only for statements whose inequality relation is ""not equal to"" (≠). a shorthand notation is used for the conjunction of several inequations involving common expressions, by chaining them together. for example, the chain is shorthand for which also implies that   < b {\displaystyle  <b} and a <   {\displaystyle a< } . in rare cases, chains without such implications about distant terms are used. for example i ≠   ≠ j {\displaystyle i\neq  \neq j} is shorthand for i ≠   a n d   ≠ j {\displaystyle i\neq  ~~\mathrm {and} ~~ \neq j} , which does not imply i ≠ j . {\displaystyle i\neq j.} [citation needed] similarly, a < b > c {\displaystyle a<b>c} is shorthand for a < b a n d b > c {\displaystyle a<b~~\mathrm {and} ~~b>c} , which does not imply any order of a {\displaystyle a} and c {\displaystyle c} . similar to equation solving, inequation solving means finding what values (numbers, functions, sets, etc.) fulfill a condition stated in the form of an inequation or a conjunction of several inequations. these expressions contain one or more unknowns, which are free variables for which values are sought that cause the condition to be fulfilled. to be precise, what is sought are often not necessarily actual values, but, more in general, expressions. a solution of the inequation is an assignment of expressions to the unknowns that satisfies the inequation(s); in other words, expressions such that, when they are substituted for the unknowns, make the inequations true propositions. often, an additional objective expression (i.e., an optimization equation) is given, that is to be minimized or maximized by an optimal solution. for example, is a conjunction of inequations, partly written as chains (where ∧ {\displaystyle \land } can be read as ""and""); the set of its solutions is shown in blue in the picture (the red, green, and orange line corresponding to the  st,  nd, and  rd conjunct, respectively). for a larger example. see linear programming#example. computer support in solving inequations is described in constraint programming; in particular, the simplex algorithm finds optimal solutions of linear inequations. the programming language prolog iii also supports solving algorithms for particular classes of inequalities (and other relations) as a basic language feature. for more, see constraint logic programming. "
212,212,infimum and supremum,0,https://en.wikipedia.org/wiki/Infimum_and_supremum,"in mathematics, the infimum (abbreviated inf; plural infima) of a subset s {\displaystyle s} of a partially ordered set p {\displaystyle p} is a greatest element in p {\displaystyle p} that is less than or equal to all elements of s , {\displaystyle s,} if such an element exists. consequently, the term greatest lower bound (abbreviated as glb) is also commonly used. the supremum (abbreviated sup; plural suprema) of a subset s {\displaystyle s} of a partially ordered set p {\displaystyle p} is the least element in p {\displaystyle p} that is greater than or equal to all elements of s , {\displaystyle s,} if such an element exists. consequently, the supremum is also referred to as the least upper bound (or lub). the infimum is in a precise sense dual to the concept of a supremum. infima and suprema of real numbers are common special cases that are important in analysis, and especially in lebesgue integration. however, the general definitions remain valid in the more abstract setting of order theory where arbitrary partially ordered sets are considered. the concepts of infimum and supremum are close to minimum and maximum, but are more useful in analysis because they better characterize special sets which may have no minimum or maximum. for instance, the set of positive real numbers r + {\displaystyle \mathbb {r} ^{+}} (not including   {\displaystyle  } ) does not have a minimum, because any given element of r + {\displaystyle \mathbb {r} ^{+}} could simply be divided in half resulting in a smaller number that is still in r + . {\displaystyle \mathbb {r} ^{+}.} there is, however, exactly one infimum of the positive real numbers:   , {\displaystyle  ,} which is smaller than all the positive real numbers and greater than any other real number which could be used as a lower bound. a lower bound of a subset s {\displaystyle s} of a partially ordered set ( p , ≤ ) {\displaystyle (p,\leq )} is an element a {\displaystyle a} of p {\displaystyle p} such that a lower bound a {\displaystyle a} of s {\displaystyle s} is called an infimum (or greatest lower bound, or meet) of s {\displaystyle s} if similarly, an upper bound of a subset s {\displaystyle s} of a partially ordered set ( p , ≤ ) {\displaystyle (p,\leq )} is an element b {\displaystyle b} of p {\displaystyle p} such that an upper bound b {\displaystyle b} of s {\displaystyle s} is called a supremum (or least upper bound, or join) of s {\displaystyle s} if infima and suprema do not necessarily exist. existence of an infimum of a subset s {\displaystyle s} of p {\displaystyle p} can fail if s {\displaystyle s} has no lower bound at all, or if the set of lower bounds does not contain a greatest element. however, if an infimum or supremum does exist, it is unique. consequently, partially ordered sets for which certain infima are known to exist become especially interesting. for instance, a lattice is a partially ordered set in which all nonempty finite subsets have both a supremum and an infimum, and a complete lattice is a partially ordered set in which all subsets have both a supremum and an infimum. more information on the various classes of partially ordered sets that arise from such considerations are found in the article on completeness properties. "
213,213,onto and one-to-one functions,1,https://en.wikipedia.org/wiki/Injective_function,"in mathematics, an injective function (also known as injection, or one-to-one function) is a function f that maps distinct elements to distinct elements; that is, f(x ) = f(x ) implies x  = x . (equivalently, x  ≠ x  implies f(x ) ≠ f(x ) in the contrapositive statement.) in other words, every element of the function's codomain is the image of at most one element of its domain. the term one-to-one function must not be confused with one-to-one correspondence that refers to bijective functions, which are functions such that each element in the codomain is an image of exactly one element in the domain. an injective non-surjective function (injection, not a bijection) an injective surjective function (bijection) a non-injective surjective function (surjection, not a bijection) a non-injective non-surjective function (also not a bijection) a homomorphism between algebraic structures is a function that is compatible with the operations of the structures. for all common algebraic structures, and, in particular for vector spaces, an injective homomorphism is also called a monomorphism. however, in the more general context of category theory, the definition of a monomorphism differs from that of an injective homomorphism. this is thus a theorem that they are equivalent for algebraic structures; see homomorphism § monomorphism for more details. a function f {\displaystyle f} that is not injective is sometimes called many-to-one. let f {\displaystyle f} be a function whose domain is a set x . {\displaystyle x.} the function f {\displaystyle f} is said to be injective provided that for all a {\displaystyle a} and b {\displaystyle b} in x , {\displaystyle x,} if f ( a ) = f ( b ) , {\displaystyle f(a)=f(b),} then a = b {\displaystyle a=b} ; that is, f ( a ) = f ( b ) {\displaystyle f(a)=f(b)} implies a = b . {\displaystyle a=b.} equivalently, if a ≠ b , {\displaystyle a\neq b,} then f ( a ) ≠ f ( b ) {\displaystyle f(a)\neq f(b)} in the contrapositive statement. symbolically, more generally, when x {\displaystyle x} and y {\displaystyle y} are both the real line r , {\displaystyle \mathbb {r} ,} then an injective function f : r → r {\displaystyle f:\mathbb {r} \to \mathbb {r} } is one whose graph is never intersected by any horizontal line more than once. this principle is referred to as the horizontal line test. "
214,214,Inner product space,2,https://en.wikipedia.org/wiki/Inner_product_space,"in mathematics, an inner product space (or, rarely, a hausdorff pre-hilbert space ) is a real vector space or a complex vector space with an operation called an inner product. the inner product of two vectors in the space is a scalar, often denoted with angle brackets such as in ⟨ a , b ⟩ {\displaystyle \langle a,b\rangle } . inner products allow formal definitions of intuitive geometric notions, such as lengths, angles, and orthogonality (zero inner product) of vectors. inner product spaces generalize euclidean vector spaces, in which the inner product is the dot product or scalar product of cartesian coordinates. inner product spaces of infinite dimension are widely used in functional analysis. inner product spaces over the field of complex numbers are sometimes referred to as unitary spaces. the first usage of the concept of a vector space with an inner product is due to giuseppe peano, in     . an inner product naturally induces an associated norm, (denoted | x | {\displaystyle |x|} and | y | {\displaystyle |y|} in the picture); so, every inner product space is a normed vector space. if this normed space is also complete (that is, a banach space) then the inner product space is a hilbert space. if an inner product space h is not a hilbert space, it can be extended by completion to a hilbert space h ¯ . {\displaystyle {\overline {h}}.} this means that h {\displaystyle h} is a linear subspace of h ¯ , {\displaystyle {\overline {h}},} the inner product of h {\displaystyle h} is the restriction of that of h ¯ , {\displaystyle {\overline {h}},} and h {\displaystyle h} is dense in h ¯ {\displaystyle {\overline {h}}} for the topology defined by the norm. in this article, f denotes a field that is either the real numbers r , {\displaystyle \mathbb {r} ,} or the complex numbers c . {\displaystyle \mathbb {c} .} a scalar is thus an element of f. a bar over an expression representing a scalar denotes the complex conjugate of this scalar. a zero vector is denoted   {\displaystyle \mathbf { } } for distinguishing it from the scalar  . an inner product space is a vector space v over the field f together with an inner product, that is a map that satisfies the following three properties for all vectors x , y , z ∈ v {\displaystyle x,y,z\in v} and all scalars a , b ∈ f {\displaystyle a,b\in f} . if the positive-definiteness condition is replaced by merely requiring that ⟨ x , x ⟩ ≥   {\displaystyle \langle x,x\rangle \geq  } for all x, then one obtains the definition of positive semi-definite hermitian form. a positive semi-definite hermitian form ⟨ ⋅ , ⋅ ⟩ {\displaystyle \langle \cdot ,\cdot \rangle } is an inner product if and only if for all x, if ⟨ x , x ⟩ =   {\displaystyle \langle x,x\rangle = } then x =  . in the following properties, which result almost immediately from the definition of an inner product, x, y and z are arbitrary vectors, and a and b are arbitrary scalars. over r {\displaystyle \mathbb {r} } , conjugate-symmetry reduces to symmetry, and sesquilinearity reduces to bilinearity. hence an inner product on a real vector space is a positive-definite symmetric bilinear form. the binomial expansion of a square becomes some authors, especially in physics and matrix algebra, prefer to define inner products and sesquilinear forms with linearity in the second argument rather than the first. then the first argument becomes conjugate linear, rather than the second. among the simplest examples of inner product spaces are r {\displaystyle \mathbb {r} } and c . {\displaystyle \mathbb {c} .} the real numbers r {\displaystyle \mathbb {r} } are a vector space over r {\displaystyle \mathbb {r} } that becomes a real inner product space when endowed with standard multiplication as its real inner product: "
215,215,Integer factorization,1,https://en.wikipedia.org/wiki/Integer_factorization,"can integer factorization be solved in polynomial time on a classical computer?in number theory, integer factorization is the decomposition of a composite number into a product of smaller integers. if these factors are further restricted to prime numbers, the process is called prime factorization. when the numbers are sufficiently large, no efficient, non-quantum integer factorization algorithm is known. however, it has not been proven that no efficient algorithm exists. the presumed difficulty of this problem is at the heart of widely used algorithms in cryptography such as rsa. many areas of mathematics and computer science have been brought to bear on the problem, including elliptic curves, algebraic number theory, and quantum computing. in     , fabrice boudot, pierrick gaudry, aurore guillevic, nadia heninger, emmanuel thomé and paul zimmermann factored a    -digit (   -bit) number (rsa-   ) utilizing approximately     core-years of computing power. the researchers estimated that a     -bit rsa modulus would take about     times as long. not all numbers of a given length are equally hard to factor. the hardest instances of these problems (for currently known techniques) are semiprimes, the product of two prime numbers. when they are both large, for instance more than two thousand bits long, randomly chosen, and about the same size (but not too close, for example, to avoid efficient factorization by fermat's factorization method), even the fastest prime factorization algorithms on the fastest computers can take enough time to make the search impractical; that is, as the number of digits of the primes being factored increases, the number of operations required to perform the factorization on any computer increases drastically. many cryptographic protocols are based on the difficulty of factoring large composite integers or a related problem—for example, the rsa problem. an algorithm that efficiently factors an arbitrary integer would render rsa-based public-key cryptography insecure. by the fundamental theorem of arithmetic, every positive integer has a unique prime factorization. (by convention,   is the empty product.) testing whether the integer is prime can be done in polynomial time, for example, by the aks primality test. if composite, however, the polynomial time tests give no insight into how to obtain the factors. given a general algorithm for integer factorization, any integer can be factored into its constituent prime factors by repeated application of this algorithm. the situation is more complicated with special-purpose factorization algorithms, whose benefits may not be realized as well or even at all with the factors produced during decomposition. for example, if n =     × p × q where p < q are very large primes, trial division will quickly produce the factors   and    but will take p divisions to find the next factor. as a contrasting example, if n is the product of the primes      ,        , and            , where       ×         =            , fermat's factorization method will begin with ⌈ n ⌉ =             {\displaystyle \lceil {\sqrt {n}}\rceil =           } which immediately yields b = a   − n =   =   b {\textstyle b={\sqrt {a^{ }-n}}={\sqrt { }}= b} and hence the factors a − b =             and a + b =            . while these are easily recognized as composite and prime respectively, fermat's method will take much longer to factor the composite number because the starting value of ⌈             ⌉ =        {\textstyle \lceil {\sqrt {           }}\rceil =      } for a is nowhere near        . among the b-bit numbers, the most difficult to factor in practice using existing algorithms are those that are products of two primes of similar size. for this reason, these are the integers used in cryptographic applications. the largest such semiprime yet factored was rsa-   , a    -bit number with     decimal digits, in february     . the total computation time was roughly      core-years of computing using intel xeon gold      at  .  ghz. like all recent factorization records, this factorization was completed with a highly optimized implementation of the general number field sieve run on hundreds of machines. no algorithm has been published that can factor all integers in polynomial time, that is, that can factor a b-bit number n in time o(bk) for some constant k. neither the existence nor non-existence of such algorithms has been proved, but it is generally suspected that they do not exist and hence that the problem is not in class p. the problem is clearly in class np, but it is generally suspected that it is not np-complete, though this has not been proven. "
216,216,Integral,1,https://en.wikipedia.org/wiki/Integral,"in mathematics, an integral assigns numbers to functions in a way that describes displacement, area, volume, and other concepts that arise by combining infinitesimal data. the process of finding integrals is called integration. along with differentiation, integration is a fundamental, essential operation of calculus,[a] and serves as a tool to solve problems in mathematics and physics involving the area of an arbitrary shape, the length of a curve, and the volume of a solid, among others. the integrals enumerated here are those termed definite integrals, which can be interpreted as the signed area of the region in the plane that is bounded by the graph of a given function between two points in the real line. conventionally, areas above the horizontal axis of the plane are positive while areas below are negative. integrals also refer to the concept of an antiderivative, a function whose derivative is the given function. in this case, they are called indefinite integrals. the fundamental theorem of calculus relates definite integrals with differentiation and provides a method to compute the definite integral of a function when its antiderivative is known. although methods of calculating areas and volumes dated from ancient greek mathematics, the principles of integration were formulated independently by isaac newton and gottfried wilhelm leibniz in the late   th century, who thought of the area under a curve as an infinite sum of rectangles of infinitesimal width. bernhard riemann later gave a rigorous definition of integrals, which is based on a limiting procedure that approximates the area of a curvilinear region by breaking the region into thin vertical slabs. integrals may be generalized depending on the type of the function as well as the domain over which the integration is performed. for example, a line integral is defined for functions of two or more variables, and the interval of integration is replaced by a curve connecting the two endpoints of the interval. in a surface integral, the curve is replaced by a piece of a surface in three-dimensional space. the first documented systematic technique capable of determining integrals is the method of exhaustion of the ancient greek astronomer eudoxus (ca.     bc), which sought to find areas and volumes by breaking them up into an infinite number of divisions for which the area or volume was known. this method was further developed and employed by archimedes in the  rd century bc and used to calculate the area of a circle, the surface area and volume of a sphere, area of an ellipse, the area under a parabola, the volume of a segment of a paraboloid of revolution, the volume of a segment of a hyperboloid of revolution, and the area of a spiral. a similar method was independently developed in china around the  rd century ad by liu hui, who used it to find the area of the circle. this method was later used in the  th century by chinese father-and-son mathematicians zu chongzhi and zu geng to find the volume of a sphere. in the middle east, hasan ibn al-haytham, latinized as alhazen (c.     – c.      ad) derived a formula for the sum of fourth powers. he used the results to carry out what would now be called an integration of this function, where the formulae for the sums of integral squares and fourth powers allowed him to calculate the volume of a paraboloid. the next significant advances in integral calculus did not begin to appear until the   th century. at this time, the work of cavalieri with his method of indivisibles, and work by fermat, began to lay the foundations of modern calculus, with cavalieri computing the integrals of xn up to degree n =   in cavalieri's quadrature formula. further steps were made in the early   th century by barrow and torricelli, who provided the first hints of a connection between integration and differentiation. barrow provided the first proof of the fundamental theorem of calculus. wallis generalized cavalieri's method, computing integrals of x to a general power, including negative powers and fractional powers. the major advance in integration came in the   th century with the independent discovery of the fundamental theorem of calculus by leibniz and newton. the theorem demonstrates a connection between integration and differentiation. this connection, combined with the comparative ease of differentiation, can be exploited to calculate integrals. in particular, the fundamental theorem of calculus allows one to solve a much broader class of problems. equal in importance is the comprehensive mathematical framework that both leibniz and newton developed. given the name infinitesimal calculus, it allowed for precise analysis of functions within continuous domains. this framework eventually became modern calculus, whose notation for integrals is drawn directly from the work of leibniz. while newton and leibniz provided a systematic approach to integration, their work lacked a degree of rigour. bishop berkeley memorably attacked the vanishing increments used by newton, calling them ""ghosts of departed quantities"". calculus acquired a firmer footing with the development of limits. integration was first rigorously formalized, using limits, by riemann. although all bounded piecewise continuous functions are riemann-integrable on a bounded interval, subsequently more general functions were considered—particularly in the context of fourier analysis—to which riemann's definition does not apply, and lebesgue formulated a different definition of integral, founded in measure theory (a subfield of real analysis). other definitions of integral, extending riemann's and lebesgue's approaches, were proposed. these approaches based on the real number system are the ones most common today, but alternative approaches exist, such as a definition of integral as the standard part of an infinite riemann sum, based on the hyperreal number system. "
217,217,Integration as the inverse process of differentiation,3,https://en.wikipedia.org/wiki/Integral_of_inverse_functions,"in mathematics, integrals of inverse functions can be computed by means of a formula that expresses the antiderivatives of the inverse f −   {\displaystyle f^{- }} of a continuous and invertible function f {\displaystyle f} , in terms of f −   {\displaystyle f^{- }} and an antiderivative of f {\displaystyle f} . this formula was published in      by charles-ange laisant. let i   {\displaystyle i_{ }} and i   {\displaystyle i_{ }} be two intervals of r {\displaystyle \mathbb {r} } . assume that f : i   → i   {\displaystyle f:i_{ }\to i_{ }} is a continuous and invertible function. it follows from the intermediate value theorem that f {\displaystyle f} is strictly monotone. consequently, f {\displaystyle f} maps intervals to intervals, so is an open map and thus a homeomorphism. since f {\displaystyle f} and the inverse function f −   : i   → i   {\displaystyle f^{- }:i_{ }\to i_{ }} are continuous, they have antiderivatives by the fundamental theorem of calculus. laisant proved that if f {\displaystyle f} is an antiderivative of f {\displaystyle f} , then the antiderivatives of f −   {\displaystyle f^{- }} are: where c {\displaystyle c} is an arbitrary real number. note that it is not assumed that f −   {\displaystyle f^{- }} is differentiable. in his      article, laisant gave three proofs. first, under the additional hypothesis that f −   {\displaystyle f^{- }} is differentiable, one may differentiate the above formula, which completes the proof immediately. his second proof was geometric. if f ( a ) = c {\displaystyle f(a)=c} and f ( b ) = d {\displaystyle f(b)=d} , the theorem can be written: the figure on the right is a proof without words of this formula. laisant does not discuss the hypotheses necessary to make this proof rigorous, but this can be proved if f {\displaystyle f} is just assumed to be strictly monotone (but not necessarily continuous, let alone differentiable). in this case, both f {\displaystyle f} and f −   {\displaystyle f^{- }} are riemann integrable and the identity follows from a bijection between lower/upper darboux sums of f {\displaystyle f} and upper/lower darboux sums of f −   {\displaystyle f^{- }} . the antiderivative version of the theorem then follows from the fundamental theorem of calculus in the case when f {\displaystyle f} is also assumed to be continuous. laisant's third proof uses the additional hypothesis that f {\displaystyle f} is differentiable. beginning with f −   ( f ( x ) ) = x {\displaystyle f^{- }(f(x))=x} , one multiplies by f ′ ( x ) {\displaystyle f'(x)} and integrates both sides. the right-hand side is calculated using integration by parts to be x f ( x ) − ∫ f ( x ) d x {\textstyle xf(x)-\int f(x)\,dx} , and the formula follows. nevertheless, it can be shown that this theorem holds even if f {\displaystyle f} or f −   {\displaystyle f^{- }} is not differentiable: it suffices, for example, to use the stieltjes integral in the previous argument. on the other hand, even though general monotonic functions are differentiable almost everywhere, the proof of the general formula does not follow, unless f −   {\displaystyle f^{- }} is absolutely continuous. it is also possible to check that for every y {\displaystyle y} in i   {\displaystyle i_{ }} , the derivative of the function y ↦ y f −   ( y ) − f ( f −   ( y ) ) {\displaystyle y\mapsto yf^{- }(y)-f(f^{- }(y))} is equal to f −   ( y ) {\displaystyle f^{- }(y)} .[citation needed] in other words: to this end, it suffices to apply the mean value theorem to f {\displaystyle f} between x {\displaystyle x} and x + h {\displaystyle x+h} , taking into account that f {\displaystyle f} is monotonic. apparently, this theorem of integration was discovered for the first time in      by charles-ange laisant, who ""could hardly believe that this theorem is new"", and hoped its use would henceforth spread out among students and teachers. this result was published independently in      by an italian engineer, alberto caprilli, in an opuscule entitled ""nuove formole d'integrazione"". it was rediscovered in      by parker, and by a number of mathematicians following him. nevertheless, they all assume that f or f−  is differentiable. the general version of the theorem, free from this additional assumption, was proposed by michael spivak in     , as an exercise in the calculus, and a fairly complete proof following the same lines was published by eric key in     . this proof relies on the very definition of the darboux integral, and consists in showing that the upper darboux sums of the function f are in  -  correspondence with the lower darboux sums of f− . in     , michael bensimhoun, estimating that the general theorem was still insufficiently known, gave two other proofs: the second proof, based on the stieltjes integral and on its formulae of integration by parts and of homeomorphic change of variables, is the most suitable to establish more complex formulae. "
218,218,Integration By Parts,3,https://en.wikipedia.org/wiki/Integration_by_parts,"in calculus, and more generally in mathematical analysis, integration by parts or partial integration is a process that finds the integral of a product of functions in terms of the integral of the product of their derivative and antiderivative. it is frequently used to transform the antiderivative of a product of functions into an antiderivative for which a solution can be more easily found. the rule can be thought of as an integral version of the product rule of differentiation. the integration by parts formula states: or, letting u = u ( x ) {\displaystyle u=u(x)} and d u = u ′ ( x ) d x {\displaystyle du=u'(x)\,dx} while v = v ( x ) {\displaystyle v=v(x)} and d v = v ′ ( x ) d x {\displaystyle dv=v'(x)\,dx} , the formula can be written more compactly: mathematician brook taylor discovered integration by parts, first publishing the idea in     . more general formulations of integration by parts exist for the riemann–stieltjes and lebesgue–stieltjes integrals. the discrete analogue for sequences is called summation by parts. the theorem can be derived as follows. for two continuously differentiable functions u(x) and v(x), the product rule states: integrating both sides with respect to x, and noting that an indefinite integral is an antiderivative gives "
219,219,Integration by substitution,1,https://en.wikipedia.org/wiki/Integration_by_substitution,"in calculus, integration by substitution, also known as u-substitution or change of variables, is a method for evaluating integrals and antiderivatives. it is the counterpart to the chain rule for differentiation, and can loosely be thought of as using the chain rule ""backwards"". before stating the result rigorously, consider a simple case using indefinite integrals. compute ∫ (   x   +   )   ( x   ) d x {\displaystyle \textstyle \int ( x^{ }+ )^{ }(x^{ })\,dx} . set u =   x   +   {\displaystyle u= x^{ }+ } . this means d u d x =   x   {\displaystyle \textstyle {\frac {du}{dx}}= x^{ }} , or in differential form, d u =   x   d x {\displaystyle du= x^{ }\,dx} . now where c {\displaystyle c} is an arbitrary constant of integration. this procedure is frequently used, but not all integrals are of a form that permits its use. in any event, the result should be verified by differentiating and comparing to the original integrand. for definite integrals, the limits of integration must also be adjusted, but the procedure is mostly the same. let φ : [a, b] → i be a differentiable function with a continuous derivative, where i ⊆ r is an interval. suppose that f : i → r is a continuous function. then in leibniz notation, the substitution u = φ(x) yields working heuristically with infinitesimals yields the equation "
220,220,Intermediate Value Theorem,2,https://en.wikipedia.org/wiki/Intermediate_value_theorem,"in mathematical analysis, the intermediate value theorem states that if f is a continuous function whose domain contains the interval [a, b], then it takes on any given value between f(a) and f(b) at some point within the interval. this has two important corollaries: this captures an intuitive property of continuous functions over the real numbers: given f continuous on [ ,  ] with the known values f( ) =   and f( ) =  , then the graph of y = f(x) must pass through the horizontal line y =   while x moves from   to  . it represents the idea that the graph of a continuous function on a closed interval can be drawn without lifting a pencil from the paper. the intermediate value theorem states the following: consider an interval i = [ a , b ] {\displaystyle i=[a,b]} of real numbers r {\displaystyle \mathbb {r} } and a continuous function f : i → r {\displaystyle f\colon i\to \mathbb {r} } . then remark: version ii states that the set of function values has no gap. for any two function values c < d {\displaystyle c<d} , even if they are outside the interval between f ( a ) {\displaystyle f(a)} and f ( b ) {\displaystyle f(b)} , all points in the interval [ c , d ] {\displaystyle {\bigl [}c,d{\bigr ]}} are also function values, a subset of the real numbers with no internal gap is an interval. version i is naturally contained in version ii. the theorem depends on, and is equivalent to, the completeness of the real numbers. the intermediate value theorem does not apply to the rational numbers q because gaps exist between rational numbers; irrational numbers fill those gaps. for example, the function f ( x ) = x   −   {\displaystyle f(x)=x^{ }- } for x ∈ q {\displaystyle x\in \mathbb {q} } satisfies f (   ) = −   {\displaystyle f( )=- } and f (   ) =   {\displaystyle f( )= } . however, there is no rational number x {\displaystyle x} such that f ( x ) =   {\displaystyle f(x)= } , because   {\displaystyle {\sqrt { }}} is an irrational number. the theorem may be proven as a consequence of the completeness property of the real numbers as follows: we shall prove the first case, f ( a ) < u < f ( b ) {\displaystyle f(a)<u<f(b)} . the second case is similar. "
221,221,intersection of a circle with a straight line,2,https://en.wikipedia.org/wiki/Intersection_(Euclidean_geometry),"in geometry, an intersection is a point, line, or curve common to two or more objects (such as lines, curves, planes, and surfaces). the simplest case in euclidean geometry is the intersection of two distinct lines, which either is one point or does not exist if the lines are parallel. determination of the intersection of flats – linear geometric objects embedded in a higher-dimensional space – is a simple task of linear algebra, namely the solution of a system of linear equations. in general the determination of an intersection leads to non-linear equations, which can be solved numerically, for example using newton iteration. intersection problems between a line and a conic section (circle, ellipse, parabola, etc.) or a quadric (sphere, cylinder, hyperboloid, etc.) lead to quadratic equations that can be easily solved. intersections between quadrics lead to quartic equations that can be solved algebraically. for the determination of the intersection point of two non-parallel lines a   x + b   y = c   , a   x + b   y = c   {\displaystyle a_{ }x+b_{ }y=c_{ },\ a_{ }x+b_{ }y=c_{ }} one gets, from cramer's rule or by substituting out a variable, the coordinates of the intersection point ( x s , y s ) {\displaystyle (x_{s},y_{s})} : (if a   b   − a   b   =   {\displaystyle a_{ }b_{ }-a_{ }b_{ }= } the lines are parallel and these formulas cannot be used because they involve dividing by  .) for two non-parallel line segments ( x   , y   ) , ( x   , y   ) {\displaystyle (x_{ },y_{ }),(x_{ },y_{ })} and ( x   , y   ) , ( x   , y   ) {\displaystyle (x_{ },y_{ }),(x_{ },y_{ })} there is not necessarily an intersection point (see diagram), because the intersection point ( x   , y   ) {\displaystyle (x_{ },y_{ })} of the corresponding lines need not to be contained in the line segments. in order to check the situation one uses parametric representations of the lines: the line segments intersect only in a common point ( x   , y   ) {\displaystyle (x_{ },y_{ })} of the corresponding lines if the corresponding parameters s   , t   {\displaystyle s_{ },t_{ }} fulfill the condition   ≤ s   , t   ≤   {\displaystyle  \leq s_{ },t_{ }\leq  } . the parameters s   , t   {\displaystyle s_{ },t_{ }} are the solution of the linear system it can be solved for s and t using cramer's rule (see above). if the condition   ≤ s   , t   ≤   {\displaystyle  \leq s_{ },t_{ }\leq  } is fulfilled one inserts s   {\displaystyle s_{ }} or t   {\displaystyle t_{ }} into the corresponding parametric representation and gets the intersection point ( x   , y   ) {\displaystyle (x_{ },y_{ })} . example: for the line segments (   ,   ) , (   ,   ) {\displaystyle ( , ),( , )} and (   ,   ) , (   , −   ) {\displaystyle ( , ),( ,- )} one gets the linear system "
222,222,Sets Intersections,1,https://en.wikipedia.org/wiki/Intersection_(set_theory),"in mathematics, the intersection of two sets a {\displaystyle a} and b , {\displaystyle b,} denoted by a ∩ b , {\displaystyle a\cap b,} is the set containing all elements of a {\displaystyle a} that also belong to b {\displaystyle b} or equivalently, all elements of b {\displaystyle b} that also belong to a . {\displaystyle a.} intersection is written using the symbol "" ∩ {\displaystyle \cap } "" between the terms; that is, in infix notation. for example: for an explanation of the symbols used in this article, refer to the table of mathematical symbols. the intersection of two sets a {\displaystyle a} and b , {\displaystyle b,} denoted by a ∩ b {\displaystyle a\cap b} , is the set of all objects that are members of both the sets a {\displaystyle a} and b . {\displaystyle b.} in symbols: that is, x {\displaystyle x} is an element of the intersection a ∩ b {\displaystyle a\cap b} if and only if x {\displaystyle x} is both an element of a {\displaystyle a} and an element of b . {\displaystyle b.} for example: we say that a {\displaystyle a} intersects (meets) b {\displaystyle b} if there exists some x {\displaystyle x} that is an element of both a {\displaystyle a} and b , {\displaystyle b,} in which case we also say that a {\displaystyle a} intersects (meets) b {\displaystyle b} at x {\displaystyle x} . equivalently, a {\displaystyle a} intersects b {\displaystyle b} if their intersection a ∩ b {\displaystyle a\cap b} is an inhabited set, meaning that there exists some x {\displaystyle x} such that x ∈ a ∩ b . {\displaystyle x\in a\cap b.} we say that a {\displaystyle a} and b {\displaystyle b} are disjoint if a {\displaystyle a} does not intersect b . {\displaystyle b.} in plain language, they have no elements in common. a {\displaystyle a} and b {\displaystyle b} are disjoint if their intersection is empty, denoted a ∩ b = ∅ . {\displaystyle a\cap b=\varnothing .} for example, the sets {   ,   } {\displaystyle \{ , \}} and {   ,   } {\displaystyle \{ , \}} are disjoint, while the set of even numbers intersects the set of multiples of   at the multiples of  . binary intersection is an associative operation; that is, for any sets a , b , {\displaystyle a,b,} and c , {\displaystyle c,} one has "
223,223,Intervals,1,https://en.wikipedia.org/wiki/Interval_(mathematics),"in mathematics, a (real) interval is a set of real numbers that contains all real numbers lying between any two numbers of the set. for example, the set of numbers x satisfying   ≤ x ≤   is an interval which contains  ,  , and all numbers in between. other examples of intervals are the set of numbers such that   < x <  , the set of all real numbers r {\displaystyle \mathbb {r} } , the set of nonnegative real numbers, the set of positive real numbers, the empty set, and any singleton (set of one element). real intervals play an important role in the theory of integration, because they are the simplest sets whose ""size"" (or ""measure"" or ""length"") is easy to define. the concept of measure can then be extended to more complicated sets of real numbers, leading to the borel measure and eventually to the lebesgue measure. intervals are central to interval arithmetic, a general numerical computing technique that automatically provides guaranteed enclosures for arbitrary formulas, even in the presence of uncertainties, mathematical approximations, and arithmetic roundoff. intervals are likewise defined on an arbitrary totally ordered set, such as integers or rational numbers. the notation of integer intervals is considered in the special section below. an open interval does not include its endpoints, and is indicated with parentheses. for example, ( , ) means greater than   and less than  . this means ( , ) = {x |   < x <  }. a closed interval is an interval which includes all its limit points, and is denoted with square brackets. for example, [ , ] means greater than or equal to   and less than or equal to  . a half-open interval includes only one of its endpoints, and is denoted by mixing the notations for open and closed intervals. for example, ( , ] means greater than   and less than or equal to  , while [ , ) means greater than or equal to   and less than  . a degenerate interval is any set consisting of a single real number (i.e., an interval of the form [a,a]). some authors include the empty set in this definition. a real interval that is neither empty nor degenerate is said to be proper, and has infinitely many elements. an interval is said to be left-bounded or right-bounded, if there is some real number that is, respectively, smaller than or larger than all its elements. an interval is said to be bounded, if it is both left- and right-bounded; and is said to be unbounded otherwise. intervals that are bounded at only one end are said to be half-bounded. the empty set is bounded, and the set of all reals is the only interval that is unbounded at both ends. bounded intervals are also commonly known as finite intervals. bounded intervals are bounded sets, in the sense that their diameter (which is equal to the absolute difference between the endpoints) is finite. the diameter may be called the length, width, measure, range, or size of the interval. the size of unbounded intervals is usually defined as +∞, and the size of the empty interval may be defined as   (or left undefined). "
224,224,Inverse of a Function,2,https://en.wikipedia.org/wiki/Inverse_function," in mathematics, the inverse function of a function f (also called the inverse of f) is a function that undoes the operation of f. the inverse of f exists if and only if f is bijective, and if it exists, is denoted by f −   . {\displaystyle f^{- }.} for a function f : x → y {\displaystyle f\colon x\to y} , its inverse f −   : y → x {\displaystyle f^{- }\colon y\to x} admits an explicit description: it sends each element y ∈ y {\displaystyle y\in y} to the unique element x ∈ x {\displaystyle x\in x} such that f(x) = y. as an example, consider the real-valued function of a real variable given by f(x) =  x −  . one can think of f as the function which multiplies its input by   then subtracts   from the result. to undo this, one adds   to the input, then divides the result by  . therefore, the inverse of f is the function f −   : r → r {\displaystyle f^{- }\colon \mathbb {r} \to \mathbb {r} } defined by f −   ( y ) = ( y +   ) /  . {\displaystyle f^{- }(y)=(y+ )/ .} let f be a function whose domain is the set x, and whose codomain is the set y. then f is invertible if there exists a function g from y to x such that g ( f ( x ) ) = x {\displaystyle g(f(x))=x} for all x ∈ x {\displaystyle x\in x} and f ( g ( y ) ) = y {\displaystyle f(g(y))=y} for all y ∈ y {\displaystyle y\in y} . if f is invertible, then there is exactly one function g satisfying this property. the function g is called the inverse of f, and is usually denoted as f − , a notation introduced by john frederick william herschel in     . [nb  ] the function f is invertible if and only if it is bijective. this is because the condition g ( f ( x ) ) = x {\displaystyle g(f(x))=x} for all x ∈ x {\displaystyle x\in x} implies that f is injective, and the condition f ( g ( y ) ) = y {\displaystyle f(g(y))=y} for all y ∈ y {\displaystyle y\in y} implies that f is surjective. the inverse function f −  to f can be explicitly described as the function recall that if f is an invertible function with domain x and codomain y, then using the composition of functions, this statement can be rewritten to the following equations between functions: "
225,225,Inverse Function Theorem,3,https://en.wikipedia.org/wiki/Inverse_function_theorem,"in mathematics, specifically differential calculus, the inverse function theorem gives a sufficient condition for a function to be invertible in a neighborhood of a point in its domain: namely, that its derivative is continuous and non-zero at the point. the theorem also gives a formula for the derivative of the inverse function. in multivariable calculus, this theorem can be generalized to any continuously differentiable, vector-valued function whose jacobian determinant is nonzero at a point in its domain, giving a formula for the jacobian matrix of the inverse. there are also versions of the inverse function theorem for complex holomorphic functions, for differentiable maps between manifolds, for differentiable functions between banach spaces, and so forth. for functions of a single variable, the theorem states that if f {\displaystyle f} is a continuously differentiable function with nonzero derivative at the point a; then f {\displaystyle f} is invertible in a neighborhood of a, the inverse is continuously differentiable, and the derivative of the inverse function at b = f ( a ) {\displaystyle b=f(a)} is the reciprocal of the derivative of f {\displaystyle f} at a {\displaystyle a} : an alternate version, which assumes that f {\displaystyle f} is continuous and injective near a, and differentiable at a with a non-zero derivative, will also result in f {\displaystyle f} being invertible near a, with an inverse that's similarly continuous and injective, and where the above formula would apply as well. as a corollary, we see clearly that if f {\displaystyle f} is k {\displaystyle k} -th differentiable, with nonzero derivative at the point a, then f {\displaystyle f} is invertible in a neighborhood of a, the inverse is also k {\displaystyle k} -th differentiable. here k {\displaystyle k} is a positive integer or ∞ {\displaystyle \infty } . for functions of more than one variable, the theorem states that if f is a continuously differentiable function from an open set of r n {\displaystyle \mathbb {r} ^{n}} into r n {\displaystyle \mathbb {r} ^{n}} , and the total derivative is invertible at a point p (i.e., the jacobian determinant of f at p is non-zero), then f is invertible near p: an inverse function to f is defined on some neighborhood of q = f ( p ) {\displaystyle q=f(p)} . writing f = ( f   , … , f n ) {\displaystyle f=(f_{ },\ldots ,f_{n})} , this means that the system of n equations y i = f i ( x   , … , x n ) {\displaystyle y_{i}=f_{i}(x_{ },\dots ,x_{n})} has a unique solution for x   , … , x n {\displaystyle x_{ },\dots ,x_{n}} in terms of y   , … , y n {\displaystyle y_{ },\dots ,y_{n}} , provided that we restrict x and y to small enough neighborhoods of p and q, respectively. in the infinite dimensional case, the theorem requires the extra hypothesis that the fréchet derivative of f at p has a bounded inverse. finally, the theorem says that the inverse function f −   {\displaystyle f^{- }} is continuously differentiable, and its jacobian derivative at q = f ( p ) {\displaystyle q=f(p)} is the matrix inverse of the jacobian of f at p: the hard part of the theorem is the existence and differentiability of f −   {\displaystyle f^{- }} . assuming this, the inverse derivative formula follows from the chain rule applied to f −   ∘ f = id {\displaystyle f^{- }\circ f={\text{id}}} : consider the vector-valued function f : r   → r   {\displaystyle f:\mathbb {r} ^{ }\to \mathbb {r} ^{ }\!} defined by: the jacobian matrix is: with jacobian determinant: "
226,226,Inverse Probability,2,https://en.wikipedia.org/wiki/Inverse_probability,"in probability theory, inverse probability is an obsolete term for the probability distribution of an unobserved variable. today, the problem of determining an unobserved variable (by whatever method) is called inferential statistics, the method of inverse probability (assigning a probability distribution to an unobserved variable) is called bayesian probability, the ""distribution"" of data given the unobserved variable is rather the likelihood function (which is not a probability distribution), and the distribution of an unobserved variable, given both data and a prior distribution, is the posterior distribution. the development of the field and terminology from ""inverse probability"" to ""bayesian probability"" is described by fienberg (    ). the term ""inverse probability"" appears in an      paper of de morgan, in reference to laplace's method of probability (developed in a      paper, which independently discovered and popularized bayesian methods, and a      book), though the term ""inverse probability"" does not occur in these. fisher uses the term in fisher (    ), referring to ""the fundamental paradox of inverse probability"" as the source of the confusion between statistical terms that refer to the true value to be estimated, with the actual value arrived at by the estimation method, which is subject to error. later jeffreys uses the term in his defense of the methods of bayes and laplace, in jeffreys (    ). the term ""bayesian"", which displaced ""inverse probability"", was introduced by ronald fisher in     . inverse probability, variously interpreted, was the dominant approach to statistics until the development of frequentism in the early   th century by ronald fisher, jerzy neyman and egon pearson. following the development of frequentism, the terms frequentist and bayesian developed to contrast these approaches, and became common in the     s. in modern terms, given a probability distribution p(x|θ) for an observable quantity x conditional on an unobserved variable θ, the ""inverse probability"" is the posterior distribution p(θ|x), which depends both on the likelihood function (the inversion of the probability distribution) and a prior distribution. the distribution p(x|θ) itself is called the direct probability. the inverse probability problem (in the   th and   th centuries) was the problem of estimating a parameter from experimental data in the experimental sciences, especially astronomy and biology. a simple example would be the problem of estimating the position of a star in the sky (at a certain time on a certain date) for purposes of navigation. given the data, one must estimate the true position (probably by averaging). this problem would now be considered one of inferential statistics. the terms ""direct probability"" and ""inverse probability"" were in use until the middle part of the   th century, when the terms ""likelihood function"" and ""posterior distribution"" became prevalent. "
227,227,inverse trigonometric functions,2,https://en.wikipedia.org/wiki/Inverse_trigonometric_functions,"in mathematics, the inverse trigonometric functions (occasionally also called arcus functions, antitrigonometric functions or cyclometric functions ) are the inverse functions of the trigonometric functions (with suitably restricted domains). specifically, they are the inverses of the sine, cosine, tangent, cotangent, secant, and cosecant functions, and are used to obtain an angle from any of the angle's trigonometric ratios. inverse trigonometric functions are widely used in engineering, navigation, physics, and geometry. several notations for the inverse trigonometric functions exist. the most common convention is to name inverse trigonometric functions using an arc- prefix: arcsin(x), arccos(x), arctan(x), etc. (this convention is used throughout this article.) this notation arises from the following geometric relationships:[citation needed] when measuring in radians, an angle of θ radians will correspond to an arc whose length is rθ, where r is the radius of the circle. thus in the unit circle, ""the arc whose cosine is x"" is the same as ""the angle whose cosine is x"", because the length of the arc of the circle in radii is the same as the measurement of the angle in radians. in computer programming languages, the inverse trigonometric functions are often called by the abbreviated forms asin, acos, atan. the notations sin− (x), cos− (x), tan− (x), etc., as introduced by john herschel in     , are often used as well in english-language sources, much more than the also established sin[− ](x), cos[− ](x), tan[− ](x),—conventions consistent with the notation of an inverse function, that is useful e.g. to define the multivalued version of each inverse trigonometric function: e.g. tan −   ⁡ ( x ) = { arctan ⁡ ( x ) + π k ∣ k ∈ z } {\displaystyle \tan ^{- }(x)=\{\arctan(x)+\pi k\mid k\in \mathbb {z} \}} . however, this might appear to conflict logically with the common semantics for expressions such as sin (x) (although only sin  x, without brackets, is the really common one), which refer to numeric power rather than function composition, and therefore may result in confusion between multiplicative inverse or reciprocal and compositional inverse. the confusion is somewhat mitigated by the fact that each of the reciprocal trigonometric functions has its own name—for example, (cos(x))−  = sec(x). nevertheless, certain authors advise against using it for its ambiguity. another precarious convention used by a tiny number of authors is to use an uppercase first letter, along with a −  superscript: sin− (x), cos− (x), tan− (x), etc. although its intention is to avoid confusion with the multiplicative inverse, which should be represented by sin− (x), cos− (x), etc., or, better, by sin−  x, cos−  x, etc., it in turn creates yet another major source of ambiguity: especially in light of the fact that many popular high-level programming languages (e.g. wolfram's mathematica, and university of sydney's magma) use those very same capitalised representations for the standard trig functions; whereas others (python (ie sympy and numpy), matlab, maple etc) use lower-case. hence, since     , the iso      -  standard has specified solely the ""arc"" prefix for the inverse functions. since none of the six trigonometric functions are one-to-one, they must be restricted in order to have inverse functions. therefore, the result ranges of the inverse functions are proper (i.e. strict) subsets of the domains of the original functions. for example, using function in the sense of multivalued functions, just as the square root function y = x {\displaystyle y={\sqrt {x}}} could be defined from y   = x , {\displaystyle y^{ }=x,} the function y = arcsin ⁡ ( x ) {\displaystyle y=\arcsin(x)} is defined so that sin ⁡ ( y ) = x . {\displaystyle \sin(y)=x.} for a given real number x , {\displaystyle x,} with −   ≤ x ≤   , {\displaystyle - \leq x\leq  ,} there are multiple (in fact, countably infinitely many) numbers y {\displaystyle y} such that sin ⁡ ( y ) = x {\displaystyle \sin(y)=x} ; for example, sin ⁡ (   ) =   , {\displaystyle \sin( )= ,} but also sin ⁡ ( π ) =   , {\displaystyle \sin(\pi )= ,} sin ⁡ (   π ) =   , {\displaystyle \sin( \pi )= ,} etc. when only one value is desired, the function may be restricted to its principal branch. with this restriction, for each x {\displaystyle x} in the domain, the expression arcsin ⁡ ( x ) {\displaystyle \arcsin(x)} will evaluate only to a single value, called its principal value. these properties apply to all the inverse trigonometric functions. the principal inverses are listed in the following table. note: some authors[citation needed] define the range of arcsecant to be (   ≤ y < π   or π ≤ y <   π   ) {\textstyle ( \leq y<{\frac {\pi }{ }}{\text{ or }}\pi \leq y<{\frac { \pi }{ }})} , because the tangent function is nonnegative on this domain. this makes some computations more consistent. for example, using this range, tan ⁡ ( arcsec ⁡ ( x ) ) = x   −   , {\displaystyle \tan(\operatorname {arcsec}(x))={\sqrt {x^{ }- }},} whereas with the range (   ≤ y < π   or π   < y ≤ π ) {\textstyle ( \leq y<{\frac {\pi }{ }}{\text{ or }}{\frac {\pi }{ }}<y\leq \pi )} , we would have to write tan ⁡ ( arcsec ⁡ ( x ) ) = ± x   −   , {\displaystyle \tan(\operatorname {arcsec}(x))=\pm {\sqrt {x^{ }- }},} since tangent is nonnegative on   ≤ y < π   , {\textstyle  \leq y<{\frac {\pi }{ }},} but nonpositive on π   < y ≤ π . {\textstyle {\frac {\pi }{ }}<y\leq \pi .} for a similar reason, the same authors define the range of arccosecant to be ( − π < y ≤ − π   {\textstyle (-\pi <y\leq -{\frac {\pi }{ }}} or   < y ≤ π   ) . {\textstyle  <y\leq {\frac {\pi }{ }}).} if x {\displaystyle x} is allowed to be a complex number, then the range of y {\displaystyle y} applies only to its real part. the table below displays names and domains of the inverse trigonometric functions along with the range of their usual principal values in radians. "
228,228,Inverse of a Matrix,2,https://en.wikipedia.org/wiki/Invertible_matrix,"in linear algebra, an n-by-n square matrix a is called invertible (also nonsingular or nondegenerate), if there exists an n-by-n square matrix b such that where in denotes the n-by-n identity matrix and the multiplication used is ordinary matrix multiplication. if this is the case, then the matrix b is uniquely determined by a, and is called the (multiplicative) inverse of a, denoted by a− . matrix inversion is the process of finding the matrix b that satisfies the prior equation for a given invertible matrix a. a square matrix that is not invertible is called singular or degenerate. a square matrix is singular if and only if its determinant is zero. singular matrices are rare in the sense that if a square matrix's entries are randomly selected from any finite region on the number line or complex plane, the probability that the matrix is singular is  , that is, it will ""almost never"" be singular. non-square matrices (m-by-n matrices for which m ≠ n) do not have an inverse. however, in some cases such a matrix may have a left inverse or right inverse. if a is m-by-n and the rank of a is equal to n (n ≤ m), then a has a left inverse, an n-by-m matrix b such that ba = in. if a has rank m (m ≤ n), then it has a right inverse, an n-by-m matrix b such that ab = im. while the most common case is that of matrices over the real or complex numbers, all these definitions can be given for matrices over any ring. however, in the case of the ring being commutative, the condition for a square matrix to be invertible is that its determinant is invertible in the ring, which in general is a stricter requirement than being nonzero. for a noncommutative ring, the usual determinant is not defined. the conditions for existence of left-inverse or right-inverse are more complicated, since a notion of rank does not exist over rings. the set of n × n invertible matrices together with the operation of matrix multiplication (and entries from ring r) form a group, the general linear group of degree n, denoted gln(r). let a be a square n by n matrix over a field k (e.g., the field r of real numbers). the following statements are equivalent (i.e., they are either all true or all false for any given matrix): furthermore, the following properties hold for an invertible matrix a: the rows of the inverse matrix v of a matrix u are orthonormal to the columns of u (and vice versa interchanging rows for columns). to see this, suppose that uv = vu = i where the rows of v are denoted as v i t {\displaystyle v_{i}^{\mathrm {t} }} and the columns of u as u j {\displaystyle u_{j}} for   ≤ i , j ≤ n {\displaystyle  \leq i,j\leq n} . then clearly, the euclidean inner product of any two v i t u j = δ i , j {\displaystyle v_{i}^{\mathrm {t} }u_{j}=\delta _{i,j}} . this property can also be useful in constructing the inverse of a square matrix in some instances, where a set of orthogonal vectors (but not necessarily orthonormal vectors) to the columns of u are known. in which case, one can apply the iterative gram–schmidt process to this initial set to determine the rows of the inverse v. a matrix that is its own inverse (i.e., a matrix a such that a = a−  and a  = i), is called an involutory matrix. the adjugate of a matrix a {\displaystyle a} can be used to find the inverse of a {\displaystyle a} as follows: "
229,229,Isomorphism,2,https://en.wikipedia.org/wiki/Isomorphism,"in mathematics, an isomorphism is a structure-preserving mapping between two structures of the same type that can be reversed by an inverse mapping. two mathematical structures are isomorphic if an isomorphism exists between them. the word isomorphism is derived from the ancient greek: ἴσος isos ""equal"", and μορφή morphe ""form"" or ""shape"". the interest in isomorphisms lies in the fact that two isomorphic objects have the same properties (excluding further information such as additional structure or names of objects). thus isomorphic structures cannot be distinguished from the point of view of structure only, and may be identified. in mathematical jargon, one says that two objects are the same up to an isomorphism.[citation needed] an automorphism is an isomorphism from a structure to itself. an isomorphism between two structures is a canonical isomorphism (a canonical map that is an isomorphism) if there is only one isomorphism between the two structures (as it is the case for solutions of a universal property), or if the isomorphism is much more natural (in some sense) than other isomorphisms. for example, for every prime number p, all fields with p elements are canonically isomorphic, with a unique isomorphism. the isomorphism theorems provide canonical isomorphisms that are not unique. the term isomorphism is mainly used for algebraic structures. in this case, mappings are called homomorphisms, and a homomorphism is an isomorphism if and only if it is bijective. in various areas of mathematics, isomorphisms have received specialized names, depending on the type of structure under consideration. for example: category theory, which can be viewed as a formalization of the concept of mapping between structures, provides a language that may be used to unify the approach to these different aspects of the basic idea. let r + {\displaystyle \mathbb {r} ^{+}} be the multiplicative group of positive real numbers, and let r {\displaystyle \mathbb {r} } be the additive group of real numbers. the logarithm function log : r + → r {\displaystyle \log :\mathbb {r} ^{+}\to \mathbb {r} } satisfies log ⁡ ( x y ) = log ⁡ x + log ⁡ y {\displaystyle \log(xy)=\log x+\log y} for all x , y ∈ r + , {\displaystyle x,y\in \mathbb {r} ^{+},} so it is a group homomorphism. the exponential function exp : r → r + {\displaystyle \exp :\mathbb {r} \to \mathbb {r} ^{+}} satisfies exp ⁡ ( x + y ) = ( exp ⁡ x ) ( exp ⁡ y ) {\displaystyle \exp(x+y)=(\exp x)(\exp y)} for all x , y ∈ r , {\displaystyle x,y\in \mathbb {r} ,} so it too is a homomorphism. the identities log ⁡ exp ⁡ x = x {\displaystyle \log \exp x=x} and exp ⁡ log ⁡ y = y {\displaystyle \exp \log y=y} show that log {\displaystyle \log } and exp {\displaystyle \exp } are inverses of each other. since log {\displaystyle \log } is a homomorphism that has an inverse that is also a homomorphism, log {\displaystyle \log } is an isomorphism of groups. the log {\displaystyle \log } function is an isomorphism which translates multiplication of positive real numbers into addition of real numbers. this facility makes it possible to multiply real numbers using a ruler and a table of logarithms, or using a slide rule with a logarithmic scale. "
230,230,Jacobian Matrix,2,https://en.wikipedia.org/wiki/Jacobian_matrix_and_determinant,"in vector calculus, the jacobian matrix (/dʒəˈkoʊbiən/, /dʒɪ-, jɪ-/) of a vector-valued function of several variables is the matrix of all its first-order partial derivatives. when this matrix is square, that is, when the function takes the same number of variables as input as the number of vector components of its output, its determinant is referred to as the jacobian determinant. both the matrix and (if applicable) the determinant are often referred to simply as the jacobian in literature. suppose f : rn → rm is a function such that each of its first-order partial derivatives exist on rn. this function takes a point x ∈ rn as input and produces the vector f(x) ∈ rm as output. then the jacobian matrix of f is defined to be an m×n matrix, denoted by j, whose (i,j)th entry is j i j = ∂ f i ∂ x j {\textstyle \mathbf {j} _{ij}={\frac {\partial f_{i}}{\partial x_{j}}}} , or explicitly where ∇ t f i {\displaystyle \nabla ^{\mathrm {t} }f_{i}} is the transpose (row vector) of the gradient of the i {\displaystyle i} component. the jacobian matrix, whose entries are functions of x, is denoted in various ways; common notations include[citation needed] df, jf, ∇ f {\displaystyle \nabla \mathbf {f} } , and ∂ ( f   , . . , f m ) ∂ ( x   , . . , x n ) {\displaystyle {\frac {\partial (f_{ },..,f_{m})}{\partial (x_{ },..,x_{n})}}} . some authors define the jacobian as the transpose of the form given above. the jacobian matrix represents the differential of f at every point where f is differentiable. in detail, if h is a displacement vector represented by a column matrix, the matrix product j(x) ⋅ h is another displacement vector, that is the best linear approximation of the change of f in a neighborhood of x, if f(x) is differentiable at x.[a] this means that the function that maps y to f(x) + j(x) ⋅ (y – x) is the best linear approximation of f(y) for all points y close to x. this linear function is known as the derivative or the differential of f at x. when m = n, the jacobian matrix is square, so its determinant is a well-defined function of x, known as the jacobian determinant of f. it carries important information about the local behavior of f. in particular, the function f has locally in the neighborhood of a point x an inverse function that is differentiable if and only if the jacobian determinant is nonzero at x (see jacobian conjecture). the jacobian determinant also appears when changing the variables in multiple integrals (see substitution rule for multiple variables). when m =  , that is when f : rn → r is a scalar-valued function, the jacobian matrix reduces to the row vector ∇ t f {\displaystyle \nabla ^{\mathrm {t} }f} ; this row vector of all first-order partial derivatives of f is the transpose of the gradient of f, i.e. j f = ∇ t f {\displaystyle \mathbf {j} _{f}=\nabla ^{t}f} . specializing further, when m = n =  , that is when f : r → r is a scalar-valued function of a single variable, the jacobian matrix has a single entry; this entry is the derivative of the function f. these concepts are named after the mathematician carl gustav jacob jacobi (    –    ). the jacobian of a vector-valued function in several variables generalizes the gradient of a scalar-valued function in several variables, which in turn generalizes the derivative of a scalar-valued function of a single variable. in other words, the jacobian matrix of a scalar-valued function in several variables is (the transpose of) its gradient and the gradient of a scalar-valued function of a single variable is its derivative. at each point where a function is differentiable, its jacobian matrix can also be thought of as describing the amount of ""stretching"", ""rotating"" or ""transforming"" that the function imposes locally near that point. for example, if (x′, y′) = f(x, y) is used to smoothly transform an image, the jacobian matrix jf(x, y), describes how the image in the neighborhood of (x, y) is transformed. "
231,231,Joint Probability distribution,2,https://en.wikipedia.org/wiki/Joint_probability_distribution," given two random variables that are defined on the same probability space, the joint probability distribution is the corresponding probability distribution on all possible pairs of outputs. the joint distribution can just as well be considered for any given number of random variables. the joint distribution encodes the marginal distributions, i.e. the distributions of each of the individual random variables. it also encodes the conditional probability distributions, which deal with how the outputs of one random variable are distributed when given information on the outputs of the other random variable(s). in the formal mathematical setup of measure theory, the joint distribution is given by the pushforward measure, by the map obtained by pairing together the given random variables, of the sample space's probability measure. in the case of real-valued random variables, the joint distribution, as a particular multivariate distribution, may be expressed by a multivariate cumulative distribution function, or by a multivariate probability density function together with a multivariate probability mass function. in the special case of continuous random variables, it is sufficient to consider probability density functions, and in the case of discrete random variables, it is sufficient to consider probability mass functions. suppose each of two urns contains twice as many red balls as blue balls, and no others, and suppose one ball is randomly selected from each urn, with the two draws independent of each other. let a {\displaystyle a} and b {\displaystyle b} be discrete random variables associated with the outcomes of the draw from the first urn and second urn respectively. the probability of drawing a red ball from either of the urns is  / , and the probability of drawing a blue ball is  / . the joint probability distribution is presented in the following table: each of the four inner cells shows the probability of a particular combination of results from the two draws; these probabilities are the joint distribution. in any one cell the probability of a particular combination occurring is (since the draws are independent) the product of the probability of the specified result for a and the probability of the specified result for b. the probabilities in these four cells sum to  , as it is always true for probability distributions. moreover, the final row and the final column give the marginal probability distribution for a and the marginal probability distribution for b respectively. for example, for a the first of these cells gives the sum of the probabilities for a being red, regardless of which possibility for b in the column above the cell occurs, as  / . thus the marginal probability distribution for a {\displaystyle a} gives a {\displaystyle a} 's probabilities unconditional on b {\displaystyle b} , in a margin of the table. consider the flip of two fair coins; let a {\displaystyle a} and b {\displaystyle b} be discrete random variables associated with the outcomes of the first and second coin flips respectively. each coin flip is a bernoulli trial and has a bernoulli distribution. if a coin displays ""heads"" then the associated random variable takes the value  , and it takes the value   otherwise. the probability of each of these outcomes is  / , so the marginal (unconditional) density functions are the joint probability mass function of a {\displaystyle a} and b {\displaystyle b} defines probabilities for each pair of outcomes. all possible outcomes are since each outcome is equally likely the joint probability mass function becomes "
232,232,Jordan normal form,2,https://en.wikipedia.org/wiki/Jordan_normal_form,"in linear algebra, a jordan normal form, also known as a jordan canonical form or jcf, is an upper triangular matrix of a particular form called a jordan matrix representing a linear operator on a finite-dimensional vector space with respect to some basis. such a matrix has each non-zero off-diagonal entry equal to  , immediately above the main diagonal (on the superdiagonal), and with identical diagonal entries to the left and below them. let v be a vector space over a field k. then a basis with respect to which the matrix has the required form exists if and only if all eigenvalues of the matrix lie in k, or equivalently if the characteristic polynomial of the operator splits into linear factors over k. this condition is always satisfied if k is algebraically closed (for instance, if it is the field of complex numbers). the diagonal entries of the normal form are the eigenvalues (of the operator), and the number of times each eigenvalue occurs is called the algebraic multiplicity of the eigenvalue. if the operator is originally given by a square matrix m, then its jordan normal form is also called the jordan normal form of m. any square matrix has a jordan normal form if the field of coefficients is extended to one containing all the eigenvalues of the matrix. in spite of its name, the normal form for a given m is not entirely unique, as it is a block diagonal matrix formed of jordan blocks, the order of which is not fixed; it is conventional to group blocks for the same eigenvalue together, but no ordering is imposed among the eigenvalues, nor among the blocks for a given eigenvalue, although the latter could for instance be ordered by weakly decreasing size. the jordan–chevalley decomposition is particularly simple with respect to a basis for which the operator takes its jordan normal form. the diagonal form for diagonalizable matrices, for instance normal matrices, is a special case of the jordan normal form. the jordan normal form is named after camille jordan, who first stated the jordan decomposition theorem in     . some textbooks have the ones on the subdiagonal; that is, immediately below the main diagonal instead of on the superdiagonal. the eigenvalues are still on the main diagonal. an n × n matrix a is diagonalizable if and only if the sum of the dimensions of the eigenspaces is n. or, equivalently, if and only if a has n linearly independent eigenvectors. not all matrices are diagonalizable; matrices that are not diagonalizable are called defective matrices. consider the following matrix: including multiplicity, the eigenvalues of a are λ =  ,  ,  ,  . the dimension of the eigenspace corresponding to the eigenvalue   is   (and not  ), so a is not diagonalizable. however, there is an invertible matrix p such that j = p− ap, where the matrix j {\displaystyle j} is almost diagonal. this is the jordan normal form of a. the section example below fills in the details of the computation. in general, a square complex matrix a is similar to a block diagonal matrix "
233,233,L'Hopital's Rule,2,https://en.wikipedia.org/wiki/L%27H%C3%B4pital%27s_rule," in mathematics, more specifically calculus, l'hôpital's rule or l'hospital's rule (french: [lopital], english: /ˌloʊpiːˈtɑːl/, loh-pee-tahl), also known as bernoulli's rule, is a theorem which provides a technique to evaluate limits of indeterminate forms. application (or repeated application) of the rule often converts an indeterminate form to an expression that can be easily evaluated by substitution. the rule is named after the   th-century french mathematician guillaume de l'hôpital. although the rule is often attributed to l'hôpital, the theorem was first introduced to him in      by the swiss mathematician johann bernoulli. l'hôpital's rule states that for functions f and g which are differentiable on an open interval i except possibly at a point c contained in i, if lim x → c f ( x ) = lim x → c g ( x ) =   or ± ∞ , {\textstyle \lim _{x\to c}f(x)=\lim _{x\to c}g(x)= {\text{ or }}\pm \infty ,} and g ′ ( x ) ≠   {\textstyle g'(x)\neq  } for all x in i with x ≠ c, and lim x → c f ′ ( x ) g ′ ( x ) {\textstyle \lim _{x\to c}{\frac {f'(x)}{g'(x)}}} exists, then the differentiation of the numerator and denominator often simplifies the quotient or converts it to a limit that can be evaluated directly. guillaume de l'hôpital (also written l'hospital[a]) published this rule in his      book analyse des infiniment petits pour l'intelligence des lignes courbes (literal translation: analysis of the infinitely small for the understanding of curved lines), the first textbook on differential calculus. [b] however, it is believed that the rule was discovered by the swiss mathematician johann bernoulli. the general form of l'hôpital's rule covers many cases. let c and l be extended real numbers (i.e., real numbers, positive infinity, or negative infinity). let i be an open interval containing c (for a two-sided limit) or an open interval with endpoint c (for a one-sided limit, or a limit at infinity if c is infinite). the real valued functions f and g are assumed to be differentiable on i except possibly at c, and additionally g ′ ( x ) ≠   {\displaystyle g'(x)\neq  } on i except possibly at c. it is also assumed that lim x → c f ′ ( x ) g ′ ( x ) = l . {\textstyle \lim _{x\to c}{\frac {f'(x)}{g'(x)}}=l.} thus the rule applies to situations in which the ratio of the derivatives has a finite or infinite limit, but not to situations in which that ratio fluctuates permanently as x gets closer and closer to c. if either in the second case, the hypothesis that f diverges to infinity is not used in the proof (see note at the end of the proof section); thus, while the conditions of the rule are normally stated as above, the second sufficient condition for the rule's procedure to be valid can be more briefly stated as lim x → c | g ( x ) | = ∞ . {\textstyle \lim _{x\to c}|g(x)|=\infty .} the hypothesis that g ′ ( x ) ≠   {\displaystyle g'(x)\neq  } appears most commonly in the literature, but some authors sidestep this hypothesis by adding other hypotheses elsewhere. one method is to define the limit of a function with the additional requirement that the limiting function is defined everywhere on the relevant interval i except possibly at c.[c] another method is to require that both f and g be differentiable everywhere on an interval containing c. all four conditions for l'hôpital's rule are necessary: "
234,234,Lagrange_multiplier,0,https://en.wikipedia.org/wiki/Lagrange_multiplier,"in mathematical optimization, the method of lagrange multipliers is a strategy for finding the local maxima and minima of a function subject to equality constraints (i.e., subject to the condition that one or more equations have to be satisfied exactly by the chosen values of the variables). it is named after the mathematician joseph-louis lagrange. the basic idea is to convert a constrained problem into a form such that the derivative test of an unconstrained problem can still be applied. the relationship between the gradient of the function and gradients of the constraints rather naturally leads to a reformulation of the original problem, known as the lagrangian function. the method can be summarized as follows: in order to find the maximum or minimum of a function f ( x ) {\displaystyle f(x)} subjected to the equality constraint g ( x ) =   {\displaystyle g(x)= } , form the lagrangian function and find the stationary points[clarification needed] of l {\displaystyle {\mathcal {l}}} considered as a function of x {\displaystyle x} and the lagrange multiplier λ {\displaystyle \lambda } . the solution corresponding to the original constrained optimization is always a saddle point of the lagrangian function, which can be identified among the stationary points from the definiteness of the bordered hessian matrix. the great advantage of this method is that it allows the optimization to be solved without explicit parameterization in terms of the constraints. as a result, the method of lagrange multipliers is widely used to solve challenging constrained optimization problems. further, the method of lagrange multipliers is generalized by the karush–kuhn–tucker conditions, which can also take into account inequality constraints of the form h ( x ) ≤ c {\displaystyle h(\mathbf {x} )\leq c} for a given constant c {\displaystyle c} . the following is known as the lagrange multiplier theorem. let f : r n → r {\displaystyle f\colon \mathbb {r} ^{n}\rightarrow \mathbb {r} } be the objective function, g : r n → r c {\displaystyle g\colon \mathbb {r} ^{n}\rightarrow \mathbb {r} ^{c}} be the constraints function, both belonging to c   {\displaystyle c^{ }} (that is, having continuous first derivatives). let x ∗ {\displaystyle x^{*}} be an optimal solution to the following optimization problem such that rank d g ( x ∗ ) = c < n {\displaystyle dg(x^{*})=c<n} : (here d g ( x ∗ ) {\displaystyle dg(x^{*})} denotes the matrix of partial derivatives, [ ∂ g j ∂ x k ] {\displaystyle \left[{\frac {\partial g_{j}}{\partial x_{k}}}\right]} .) then there exists a unique lagrange multiplier λ ∗ ∈ r c {\displaystyle \lambda ^{*}\in \mathbb {r} ^{c}} such that d f ( x ∗ ) = λ ∗ t d g ( x ∗ ) {\displaystyle df(x^{*})=\lambda ^{*t}dg(x^{*})} . the lagrange multiplier theorem states that at any local maxima (or minima) of the function evaluated under the equality constraints, if constraint qualification applies (explained below), then the gradient of the function (at that point) can be expressed as a linear combination of the gradients of the constraints (at that point), with the lagrange multipliers acting as coefficients. this is equivalent to saying that any direction perpendicular to all gradients of the constraints is also perpendicular to the gradient of the function. or still, saying that the directional derivative of the function is   in every feasible direction. for the case of only one constraint and only two choice variables (as exemplified in figure  ), consider the optimization problem "
235,235,cosine rule,0,https://en.wikipedia.org/wiki/Law_of_cosines,"in trigonometry, the law of cosines (also known as the cosine formula, cosine rule, or al-kashi's theorem ) relates the lengths of the sides of a triangle to the cosine of one of its angles. using notation as in fig.  , the law of cosines states where γ denotes the angle contained between sides of lengths a and b and opposite the side of length c. for the same figure, the other two relations are analogous: the law of cosines generalizes the pythagorean theorem, which holds only for right triangles: if the angle γ is a right angle (of measure    degrees, or π/  radians), then cos γ =  , and thus the law of cosines reduces to the pythagorean theorem: the law of cosines is useful for computing the third side of a triangle when two sides and their enclosed angle are known, and in computing the angles of a triangle if all three sides are known. though the notion of the cosine was not yet developed in his time, euclid's elements, dating back to the  rd century bc, contains an early geometric theorem almost equivalent to the law of cosines. the cases of obtuse triangles and acute triangles (corresponding to the two cases of negative or positive cosine) are treated separately, in propositions    and    of book  . trigonometric functions and algebra (in particular negative numbers) being absent in euclid's time, the statement has a more geometric flavor: proposition   in obtuse-angled triangles the square on the side subtending the obtuse angle is greater than the squares on the sides containing the obtuse angle by twice the rectangle contained by one of the sides about the obtuse angle, namely that on which the perpendicular falls, and the straight line cut off outside by the perpendicular towards the obtuse angle.using notation as in fig.  , euclid's statement can be represented by the formula this formula may be transformed into the law of cosines by noting that ch = (cb) cos(π − γ) = −(cb) cos γ. proposition    contains an entirely analogous statement for acute triangles. euclid's elements paved the way for the discovery of law of cosines. in the   th century, jamshīd al-kāshī, a persian mathematician and astronomer, provided the first explicit statement of the law of cosines in a form suitable for triangulation. he provided accurate trigonometric tables and expressed the theorem in a form suitable for modern usage. as of the     s, in france, the law of cosines is still referred to as the théorème d'al-kashi. the theorem was popularized in the western world by françois viète in the   th century. at the beginning of the   th century, modern algebraic notation allowed the law of cosines to be written in its current symbolic form. "
236,236,Law of sines,0,https://en.wikipedia.org/wiki/Law_of_sines,"in trigonometry, the law of sines, sine law, sine formula, or sine rule is an equation relating the lengths of the sides of a triangle (any shape) to the sines of its angles. according to the law, the law of sines is one of two trigonometric equations commonly applied to find lengths and angles in scalene triangles, with the other being the law of cosines. the law of sines can be generalized to higher dimensions on surfaces with constant curvature. according to ubiratàn d'ambrosio and helaine selin, the spherical law of sines was discovered in the   th century. it is variously attributed to abu-mahmud khojandi, abu al-wafa' buzjani, nasir al-din al-tusi and abu nasr mansur. ibn muʿādh al-jayyānī's the book of unknown arcs of a sphere in the   th century contains the general law of sines. the plane law of sines was later stated in the   th century by nasīr al-dīn al-tūsī. in his on the sector figure, he stated the law of sines for plane and spherical triangles, and provided proofs for this law. according to glen van brummelen, ""the law of sines is really regiomontanus's foundation for his solutions of right-angled triangles in book iv, and these solutions are in turn the bases for his solutions of general triangles."" regiomontanus was a   th-century german mathematician. the area t of any triangle can be written as one half of its base times its height. selecting one side of the triangle as the base, the height of the triangle relative to that base is computed as the length of another side times the sine of the angle between the chosen side and the base. thus depending on the selection of the base, the area of the triangle can be written as any of: when using the law of sines to find a side of a triangle, an ambiguous case occurs when two separate triangles can be constructed from the data provided (i.e., there are two different possible solutions to the triangle). in the case shown below they are triangles abc and abc′. given a general triangle, the following conditions would need to be fulfilled for the case to be ambiguous: if all the above conditions are true, then each of angles β and β′ produces a valid triangle, meaning that both of the following are true: "
237,237,Law of total Probability,3,https://en.wikipedia.org/wiki/Law_of_total_probability,"in probability theory, the law (or formula) of total probability is a fundamental rule relating marginal probabilities to conditional probabilities. it expresses the total probability of an outcome which can be realized via several distinct events—hence the name. the law of total probability is a theorem that states, in its discrete case, if { b n : n =   ,   ,   , … } {\displaystyle \left\{{b_{n}:n= , , ,\ldots }\right\}} is a finite or countably infinite partition of a sample space (in other words, a set of pairwise disjoint events whose union is the entire sample space) and each event b n {\displaystyle b_{n}} is measurable, then for any event a {\displaystyle a} of the same probability space: or, alternatively, where, for any n {\displaystyle n} for which p ( b n ) =   {\displaystyle p(b_{n})= } these terms are simply omitted from the summation, because p ( a ∣ b n ) {\displaystyle p(a\mid b_{n})} is finite. the summation can be interpreted as a weighted average, and consequently the marginal probability, p ( a ) {\displaystyle p(a)} , is sometimes called ""average probability""; ""overall probability"" is sometimes used in less formal writings. the law of total probability, can also be stated for conditional probabilities. taking the b n {\displaystyle b_{n}} as above, and assuming c {\displaystyle c} is an event independent of any of the b n {\displaystyle b_{n}} : the above mathematical statement might be interpreted as follows: given an event a {\displaystyle a} , with known conditional probabilities given any of the b n {\displaystyle b_{n}} events, each with a known probability itself, what is the total probability that a {\displaystyle a} will happen? the answer to this question is given by p ( a ) {\displaystyle p(a)} . the law of total probability extends to the case of conditioning on events generated by continuous random variables. let ( ω , f , p ) {\displaystyle (\omega ,{\mathcal {f}},p)} be a probability space. suppose x {\displaystyle x} is a random variable with distribution function f x {\displaystyle f_{x}} , and a {\displaystyle a} an event on ( ω , f , p ) {\displaystyle (\omega ,{\mathcal {f}},p)} . then the law of total probability states p ( a ) = ∫ − ∞ ∞ p ( a | x = x ) d f x ( x ) . {\displaystyle p(a)=\int _{-\infty }^{\infty }p(a|x=x)df_{x}(x).} "
238,238,Leibniz notation,2,https://en.wikipedia.org/wiki/Leibniz%27s_notation,"in calculus, leibniz's notation, named in honor of the   th-century german philosopher and mathematician gottfried wilhelm leibniz, uses the symbols dx and dy to represent infinitely small (or infinitesimal) increments of x and y, respectively, just as δx and δy represent finite increments of x and y, respectively. consider y as a function of a variable x, or y = f(x). if this is the case, then the derivative of y with respect to x, which later came to be viewed as the limit was, according to leibniz, the quotient of an infinitesimal increment of y by an infinitesimal increment of x, or where the right hand side is joseph-louis lagrange's notation for the derivative of f at x. the infinitesimal increments are called differentials. related to this is the integral in which the infinitesimal increments are summed (e.g. to compute lengths, areas and volumes as sums of tiny pieces), for which leibniz also supplied a closely related notation involving the same differentials, a notation whose efficiency proved decisive in the development of continental european mathematics. leibniz's concept of infinitesimals, long considered to be too imprecise to be used as a foundation of calculus, was eventually replaced by rigorous concepts developed by weierstrass and others in the   th century. consequently, leibniz's quotient notation was re-interpreted to stand for the limit of the modern definition. however, in many instances, the symbol did seem to act as an actual quotient would and its usefulness kept it popular even in the face of several competing notations. several different formalisms were developed in the   th century that can give rigorous meaning to notions of infinitesimals and infinitesimal displacements, including nonstandard analysis, tangent space, o notation and others. the derivatives and integrals of calculus can be packaged into the modern theory of differential forms, in which the derivative is genuinely a ratio of two differentials, and the integral likewise behaves in exact accordance with leibniz notation. however, this requires that derivative and integral first be defined by other means, and as such expresses the self-consistency and computational efficacy of the leibniz notation rather than giving it a new foundation. the newton–leibniz approach to infinitesimal calculus was introduced in the   th century. while newton worked with fluxions and fluents, leibniz based his approach on generalizations of sums and differences. leibniz was the first to use the ∫ {\displaystyle \textstyle \int } character. he based the character on the latin word summa (""sum""), which he wrote ſumma with the elongated s commonly used in germany at the time. viewing differences as the inverse operation of summation, he used the symbol d, the first letter of the latin differentia, to indicate this inverse operation. leibniz was fastidious about notation; spending years experimenting, adjusting, rejecting and corresponding with other mathematicians about them. notations he used for the differential of y ranged successively from ω, l, and y/d until he finally settled on dy. his integral sign first appeared publicly in the article ""de geometria recondita et analysi indivisibilium atque infinitorum"" (""on a hidden geometry and analysis of indivisibles and infinites""), published in acta eruditorum in june     , but he had been using it in private manuscripts at least since     . leibniz first used dx in the article ""nova methodus pro maximis et minimis"" also published in acta eruditorum in     . while the symbol dx/dy does appear in private manuscripts of     , it does not appear in this form in either of the above-mentioned published works. leibniz did, however, use forms such as dy ad dx and dy : dx in print. english mathematicians were encumbered by newton's dot notation until      when robert woodhouse published a description of the continental notation. later the analytical society at cambridge university promoted the adoption of leibniz's notation. at the end of the   th century, weierstrass's followers ceased to take leibniz's notation for derivatives and integrals literally. that is, mathematicians felt that the concept of infinitesimals contained logical contradictions in its development. a number of   th century mathematicians (weierstrass and others) found logically rigorous ways to treat derivatives and integrals without infinitesimals using limits as shown above, while cauchy exploited both infinitesimals and limits (see cours d'analyse). nonetheless, leibniz's notation is still in general use. although the notation need not be taken literally, it is usually simpler than alternatives when the technique of separation of variables is used in the solution of differential equations. in physical applications, one may for example regard f(x) as measured in meters per second, and dx in seconds, so that f(x) dx is in meters, and so is the value of its definite integral. in that way the leibniz notation is in harmony with dimensional analysis. suppose a dependent variable y represents a function f of an independent variable x, that is, "
239,239,Leibnitz Integral rule,3,https://en.wikipedia.org/wiki/Leibniz_integral_rule,"in calculus, the leibniz integral rule for differentiation under the integral sign, named after gottfried leibniz, states that for an integral of the form where − ∞ < a ( x ) , b ( x ) < ∞ {\displaystyle -\infty <a(x),b(x)<\infty } , the derivative of this integral is expressible as where the partial derivative indicates that inside the integral, only the variation of f ( x , t ) {\displaystyle f(x,t)} with x {\displaystyle x} is considered in taking the derivative. notice that if a ( x ) {\displaystyle a(x)} and b ( x ) {\displaystyle b(x)} are constants rather than functions of x {\displaystyle x} , we have the special case: thus under certain conditions, one may interchange the integral and partial differential operators. this important result is particularly useful in the differentiation of integral transforms. an example of such is the moment generating function in probability theory, a variation of the laplace transform, which can be differentiated to generate the moments of a random variable. whether leibniz's integral rule applies is essentially a question about the interchange of limits. theorem — let f ( x , t ) {\displaystyle f(x,t)} be a function such that both f ( x , t ) {\displaystyle f(x,t)} and its partial derivative f x ( x , t ) {\displaystyle f_{x}(x,t)} are continuous in t {\displaystyle t} and x {\displaystyle x} in some region of the x t {\displaystyle xt} -plane, including a ( x ) ≤ t ≤ b ( x ) {\displaystyle a(x)\leq t\leq b(x)} , x   ≤ x ≤ x   {\displaystyle x_{ }\leq x\leq x_{ }} . also suppose that the functions a ( x ) {\displaystyle a(x)} and b ( x ) {\displaystyle b(x)} are both continuous and both have continuous derivatives for x   ≤ x ≤ x   {\displaystyle x_{ }\leq x\leq x_{ }} . then, for x   ≤ x ≤ x   {\displaystyle x_{ }\leq x\leq x_{ }} , this formula is the general form of the leibniz integral rule and can be derived using the fundamental theorem of calculus. the (first) fundamental theorem of calculus is just the particular case of the above formula where a ( x ) = a ∈ r {\displaystyle a(x)=a\in \mathbb {r} } , b ( x ) = x {\displaystyle b(x)=x} , and f ( x , t ) = f ( t ) {\displaystyle f(x,t)=f(t)} . if both upper and lower limits are taken as constants, then the formula takes the shape of an operator equation: the following three basic theorems on the interchange of limits are essentially equivalent: a leibniz integral rule for a two dimensional surface moving in three dimensional space is "
240,240,likelyhood function,0,https://en.wikipedia.org/wiki/Likelihood_function,"the likelihood function (often simply called the likelihood) describes the joint probability of the observed data as a function of the parameters of the chosen statistical model. for each specific parameter value θ {\displaystyle \theta } in the parameter space, the likelihood function p ( x | θ ) {\displaystyle p(x|\theta )} therefore assigns a probabilistic prediction to the observed data x {\displaystyle x} . since it is essentially the product of sampling densities, the likelihood generally encapsulates both the data-generating process as well as the missing-data mechanism that produced the observed sample. to emphasize that the likelihood is not a p.d.f. of the parameters, it is often written as l ( θ ∣ x ) {\displaystyle {\mathcal {l}}(\theta \mid x)} .[a] in maximum likelihood estimation, the likelihood function is maximized to obtain the specific value θ ^ = argmax θ ∈ θ ⁡ l ( θ ∣ x ) {\displaystyle {\hat {\theta }}=\operatorname {argmax} _{\theta \in \theta }{\mathcal {l}}(\theta \mid x)} that is ‘most likely’ to equal the parametrization of the probabilistic model that generated the observed data. meanwhile in bayesian statistics, the likelihood function serves as the conduit through which sample information influences p ( θ ∣ x ) {\displaystyle p(\theta \mid x)} , the posterior probability of the parameter. the case for using likelihood was first made by r. a. fisher, who believed it to be a self-contained framework for statistical modelling and inference. later, barnard and birnbaum led a school of thought that advocated the likelihood principle, postulating that all relevant information for inference is contained in the likelihood function. but in both frequentist and bayesian statistics, the likelihood function plays a fundamental role. the likelihood function is usually defined differently for discrete and continuous probability distributions. a general definition is also possible, as discussed below. let x {\displaystyle x} be a discrete random variable with probability mass function p {\displaystyle p} depending on a parameter θ {\displaystyle \theta } . then the function considered as a function of θ {\displaystyle \theta } , is the likelihood function, given the outcome x {\displaystyle x} of the random variable x {\displaystyle x} . sometimes the probability of ""the value x {\displaystyle x} of x {\displaystyle x} for the parameter value θ {\displaystyle \theta } "" is written as p(x = x | θ) or p(x = x; θ). the likelihood is equal to the probability that a particular outcome x {\displaystyle x} is observed when the true value of the parameter is θ {\displaystyle \theta } , it is equal to the probability density over x {\displaystyle x} , it is not a probability density over the parameter θ {\displaystyle \theta } . the likelihood, l ( θ ∣ x ) {\displaystyle {\mathcal {l}}(\theta \mid x)} , should not be confused with p ( θ ∣ x ) {\displaystyle p(\theta \mid x)} , which is the posterior probability of θ {\displaystyle \theta } given the data x {\displaystyle x} . given no event (no data), the probability and thus likelihood is  ;[citation needed] any non-trivial event will have a lower likelihood. consider a simple statistical model of a coin flip: a single parameter p h {\displaystyle p_{\text{h}}} that expresses the ""fairness"" of the coin. the parameter is the probability that a coin lands heads up (""h"") when tossed. p h {\displaystyle p_{\text{h}}} can take on any value within the range  .  to  . . for a perfectly fair coin, p h =  .  {\displaystyle p_{\text{h}}= . } . imagine flipping a fair coin twice, and observing the following data: two heads in two tosses (""hh""). assuming that each successive coin flip is i.i.d., then the probability of observing hh is hence, given the observed data hh, the likelihood that the model parameter p h {\displaystyle p_{\text{h}}} equals  .  is  .  . mathematically, this is written as "
241,241,Limit and continuity of a function,0,https://en.wikipedia.org/wiki/Limit_of_a_function,"although the function (sin x)/x is not defined at zero, as x becomes closer and closer to zero, (sin x)/x becomes arbitrarily close to  . in other words, the limit of (sin x)/x, as x approaches zero, equals  . in mathematics, the limit of a function is a fundamental concept in calculus and analysis concerning the behavior of that function near a particular input. formal definitions, first devised in the early   th century, are given below. informally, a function f assigns an output f(x) to every input x. we say that the function has a limit l at an input p, if f(x) gets closer and closer to l as x moves closer and closer to p. more specifically, when f is applied to any input sufficiently close to p, the output value is forced arbitrarily close to l. on the other hand, if some inputs very close to p are taken to outputs that stay a fixed distance apart, then we say the limit does not exist. the notion of a limit has many applications in modern calculus. in particular, the many definitions of continuity employ the concept of limit: roughly, a function is continuous if all of its limits agree with the values of the function. the concept of limit also appears in the definition of the derivative: in the calculus of one variable, this is the limiting value of the slope of secant lines to the graph of a function. although implicit in the development of calculus of the   th and   th centuries, the modern idea of the limit of a function goes back to bolzano who, in     , introduced the basics of the epsilon-delta technique to define continuous functions. however, his work was not known during his lifetime. in his      book cours d'analyse, cauchy discussed variable quantities, infinitesimals and limits, and defined continuity of y = f ( x ) {\displaystyle y=f(x)} by saying that an infinitesimal change in x necessarily produces an infinitesimal change in y, while (grabiner     ) claims that he used a rigorous epsilon-delta definition in proofs. in     , weierstrass first introduced the epsilon-delta definition of limit in the form it is usually written today. he also introduced the notations lim and limx→x . the modern notation of placing the arrow below the limit symbol is due to hardy, which is introduced in his book a course of pure mathematics in     . imagine a person walking over a landscape represented by the graph of y = f(x). their horizontal position is measured by the value of x, much like the position given by a map of the land or by a global positioning system. their altitude is given by the coordinate y. they walk toward the horizontal position given by x = p. as they get closer and closer to it, they notice that their altitude approaches l. if asked about the altitude of x = p, they would then answer l. what, then, does it mean to say, their altitude is approaching l? it means that their altitude gets nearer and nearer to l—except for a possible small error in accuracy. for example, suppose we set a particular accuracy goal for our traveler: they must get within ten meters of l. they report back that indeed, they can get within ten vertical meters of l, since they note that when they are within fifty horizontal meters of p, their altitude is always ten meters or less from l. the accuracy goal is then changed: can they get within one vertical meter? yes. if they are anywhere within seven horizontal meters of p, their altitude will always remain within one meter from the target l. in summary, to say that the traveler's altitude approaches l as their horizontal position approaches p, is to say that for every target accuracy goal, however small it may be, there is some neighbourhood of p whose altitude fulfills that accuracy goal. "
242,242,Lindeburg's condition,2,https://en.wikipedia.org/wiki/Lindeberg%27s_condition,"in probability theory, lindeberg's condition is a sufficient condition (and under certain conditions also a necessary condition) for the central limit theorem (clt) to hold for a sequence of independent random variables. unlike the classical clt, which requires that the random variables in question have finite variance and be both independent and identically distributed, lindeberg's clt only requires that they have finite variance, satisfy lindeberg's condition, and be independent. it is named after the finnish mathematician jarl waldemar lindeberg. let ( ω , f , p ) {\displaystyle (\omega ,{\mathcal {f}},\mathbb {p} )} be a probability space, and x k : ω → r , k ∈ n {\displaystyle x_{k}:\omega \to \mathbb {r} ,\,\,k\in \mathbb {n} } , be independent random variables defined on that space. assume the expected values e [ x k ] = μ k {\displaystyle \mathbb {e} \,[x_{k}]=\mu _{k}} and variances v a r [ x k ] = σ k   {\displaystyle \mathrm {var} \,[x_{k}]=\sigma _{k}^{ }} exist and are finite. also let s n   := ∑ k =   n σ k   . {\displaystyle s_{n}^{ }:=\sum _{k= }^{n}\sigma _{k}^{ }.} if this sequence of independent random variables x k {\displaystyle x_{k}} satisfies lindeberg's condition: for all ε >   {\displaystyle \varepsilon > } , where  {…} is the indicator function, then the central limit theorem holds, i.e. the random variables converge in distribution to a standard normal random variable as n → ∞ . {\displaystyle n\to \infty .} lindeberg's condition is sufficient, but not in general necessary (i.e. the inverse implication does not hold in general). however, if the sequence of independent random variables in question satisfies then lindeberg's condition is both sufficient and necessary, i.e. it holds if and only if the result of central limit theorem holds. feller's theorem can be used as an alternative method to prove that lindeberg's condition holds. letting s n := ∑ k =   n x k {\displaystyle s_{n}:=\sum _{k= }^{n}x_{k}} and for simplicity e [ x k ] =   {\displaystyle \mathbb {e} \,[x_{k}]= } , the theorem states this theorem can be used to disprove the central limit theorem holds for x k {\displaystyle x_{k}} by using proof by contradiction. this procedure involves proving that lindeberg's condition fails for x k {\displaystyle x_{k}} . because the lindeberg condition implies max k =   , … , n σ k   s n   →   {\displaystyle \max _{k= ,\ldots ,n}{\frac {\sigma _{k}^{ }}{s_{n}^{ }}}\to  } as n → ∞ {\displaystyle n\to \infty } , it guarantees that the contribution of any individual random variable x k {\displaystyle x_{k}} (   ≤ k ≤ n {\displaystyle  \leq k\leq n} ) to the variance s n   {\displaystyle s_{n}^{ }} is arbitrarily small, for sufficiently large values of n {\displaystyle n} . "
243,243,Straight Lines,1,https://en.wikipedia.org/wiki/Line_(geometry),"in geometry, the notion of line or straight line was introduced by ancient mathematicians to represent straight objects (i.e., having no curvature) with negligible width and depth. lines are an idealization of such objects, which are often described in terms of two points (e.g., a b ↔ {\displaystyle {\overleftrightarrow {ab}}} ) or referred to using a single letter (e.g., ℓ {\displaystyle \ell } ). until the   th century, lines were defined as the ""[...] first species of quantity, which has only one dimension, namely length, without any width nor depth, and is nothing else than the flow or run of the point which [...] will leave from its imaginary moving some vestige in length, exempt of any width. [...] the straight line is that which is equally extended between its points."" euclid described a line as ""breadthless length"" which ""lies equally with respect to the points on itself""; he introduced several postulates as basic unprovable properties from which he constructed all of geometry, which is now called euclidean geometry to avoid confusion with other geometries which have been introduced since the end of the   th century (such as non-euclidean, projective and affine geometry). in modern mathematics, given the multitude of geometries, the concept of a line is closely tied to the way the geometry is described. for instance, in analytic geometry, a line in the plane is often defined as the set of points whose coordinates satisfy a given linear equation, but in a more abstract setting, such as incidence geometry, a line may be an independent object, distinct from the set of points which lie on it. when a geometry is described by a set of axioms, the notion of a line is usually left undefined (a so-called primitive object). the properties of lines are then determined by the axioms which refer to them. one advantage to this approach is the flexibility it gives to users of the geometry. thus in differential geometry, a line may be interpreted as a geodesic (shortest path between points), while in some projective geometries, a line is a  -dimensional vector space (all linear combinations of two independent vectors). this flexibility also extends beyond mathematics and, for example, permits physicists to think of the path of a light ray as being a line. all definitions are ultimately circular in nature, since they depend on concepts which must themselves have definitions, a dependence which cannot be continued indefinitely without returning to the starting point. to avoid this vicious circle, certain concepts must be taken as primitive concepts; terms which are given no definition. in geometry, it is frequently the case that the concept of line is taken as a primitive. in those situations where a line is a defined concept, as in coordinate geometry, some other fundamental ideas are taken as primitives. when the line concept is a primitive, the behaviour and properties of lines are dictated by the axioms which they must satisfy. in a non-axiomatic or simplified axiomatic treatment of geometry, the concept of a primitive notion may be too abstract to be dealt with. in this circumstance, it is possible to provide a description or mental image of a primitive notion, to give a foundation to build the notion on which would formally be based on the (unstated) axioms. descriptions of this type may be referred to, by some authors, as definitions in this informal style of presentation. these are not true definitions, and could not be used in formal proofs of statements. the ""definition"" of line in euclid's elements falls into this category. even in the case where a specific geometry is being considered (for example, euclidean geometry), there is no generally accepted agreement among authors as to what an informal description of a line should be when the subject is not being treated formally. when geometry was first formalised by euclid in the elements, he defined a general line (straight or curved) to be ""breadthless length"" with a straight line being a line ""which lies evenly with the points on itself"". these definitions serve little purpose, since they use terms which are not by themselves defined. in fact, euclid himself did not use these definitions in this work, and probably included them just to make it clear to the reader what was being discussed. in modern geometry, a line is simply taken as an undefined object with properties given by axioms, but is sometimes defined as a set of points obeying a linear relationship when some other fundamental concept is left undefined. in an axiomatic formulation of euclidean geometry, such as that of hilbert (euclid's original axioms contained various flaws which have been corrected by modern mathematicians), a line is stated to have certain properties which relate it to other lines and points. for example, for any two distinct points, there is a unique line containing them, and any two distinct lines intersect in at most one point. in two dimensions (i.e., the euclidean plane), two lines which do not intersect are called parallel. in higher dimensions, two lines that do not intersect are parallel if they are contained in a plane, or skew if they are not. any collection of finitely many lines partitions the plane into convex polygons (possibly unbounded); this partition is known as an arrangement of lines. "
244,244,Line Integral,1,https://en.wikipedia.org/wiki/Line_integral,"in mathematics, a line integral is an integral where the function to be integrated is evaluated along a curve. the terms path integral, curve integral, and curvilinear integral are also used; contour integral is used as well, although that is typically reserved for line integrals in the complex plane. the function to be integrated may be a scalar field or a vector field. the value of the line integral is the sum of values of the field at all points on the curve, weighted by some scalar function on the curve (commonly arc length or, for a vector field, the scalar product of the vector field with a differential vector in the curve). this weighting distinguishes the line integral from simpler integrals defined on intervals. many simple formulae in physics, such as the definition of work as w = f ⋅ s {\displaystyle w=\mathbf {f} \cdot \mathbf {s} } , have natural continuous analogues in terms of line integrals, in this case w = ∫ l f ( s ) ⋅ d s {\displaystyle \textstyle w=\int _{l}\mathbf {f} (\mathbf {s} )\cdot d\mathbf {s} } , which computes the work done on an object moving through an electric or gravitational field f along a path l {\displaystyle l} . in qualitative terms, a line integral in vector calculus can be thought of as a measure of the total effect of a given tensor field along a given curve. for example, the line integral over a scalar field (rank   tensor) can be interpreted as the area under the field carved out by a particular curve. this can be visualized as the surface created by z = f(x,y) and a curve c in the xy plane. the line integral of f would be the area of the ""curtain"" created—when the points of the surface that are directly over c are carved out. for some scalar field f : u → r {\displaystyle f\colon u\to \mathbb {r} } where u ⊆ r n {\displaystyle u\subseteq \mathbb {r} ^{n}} , the line integral along a piecewise smooth curve c ⊂ u {\displaystyle {\mathcal {c}}\subset u} is defined as where r : [ a , b ] → c {\displaystyle \mathbf {r} \colon [a,b]\to {\mathcal {c}}} is an arbitrary bijective parametrization of the curve c {\displaystyle {\mathcal {c}}} such that r(a) and r(b) give the endpoints of c {\displaystyle {\mathcal {c}}} and a < b. here, and in the rest of the article, the absolute value bars denote the standard (euclidean) norm of a vector. the function f is called the integrand, the curve c {\displaystyle {\mathcal {c}}} is the domain of integration, and the symbol ds may be intuitively interpreted as an elementary arc length. line integrals of scalar fields over a curve c {\displaystyle {\mathcal {c}}} do not depend on the chosen parametrization r of c {\displaystyle {\mathcal {c}}} . geometrically, when the scalar field f is defined over a plane (n =  ), its graph is a surface z = f(x, y) in space, and the line integral gives the (signed) cross-sectional area bounded by the curve c {\displaystyle {\mathcal {c}}} and the graph of f. see the animation to the right. for a line integral over a scalar field, the integral can be constructed from a riemann sum using the above definitions of f, c and a parametrization r of c. this can be done by partitioning the interval [a, b] into n sub-intervals [ti− , ti] of length δt = (b − a)/n, then r(ti) denotes some point, call it a sample point, on the curve c. we can use the set of sample points {r(ti):   ≤ i ≤ n} to approximate the curve c by a polygonal path by introducing a straight line piece between each of the sample points r(ti− ) and r(ti). we then label the distance between each of the sample points on the curve as δsi. the product of f(r(ti)) and δsi can be associated with the signed area of a rectangle with a height and width of f(r(ti)) and δsi, respectively. taking the limit of the sum of the terms as the length of the partitions approaches zero gives us by the mean value theorem, the distance between subsequent points on the curve, is substituting this in the above riemann sum yields "
245,245,length of line segment,1,https://en.wikipedia.org/wiki/Line_segment,"in geometry, a line segment is a part of a line that is bounded by two distinct end points, and contains every point on the line that is between its endpoints. a closed line segment includes both endpoints, while an open line segment excludes both endpoints; a half-open line segment includes exactly one of the endpoints. in geometry, a line segment is often denoted using a line above the symbols for the two endpoints (such as a b ¯ {\displaystyle {\overline {ab}}} ). examples of line segments include the sides of a triangle or square. more generally, when both of the segment's end points are vertices of a polygon or polyhedron, the line segment is either an edge (of that polygon or polyhedron) if they are adjacent vertices, or a diagonal. when the end points both lie on a curve (such as a circle), a line segment is called a chord (of that curve). if v is a vector space over r {\displaystyle \mathbb {r} } or c {\displaystyle \mathbb {c} } , and l is a subset of v, then l is a line segment if l can be parameterized as for some vectors u , v ∈ v {\displaystyle \mathbf {u} ,\mathbf {v} \in v\,\!} . in which case, the vectors u and u + v are called the end points of l. sometimes, one needs to distinguish between ""open"" and ""closed"" line segments. in this case, one would define a closed line segment as above, and an open line segment as a subset l that can be parametrized as for some vectors u , v ∈ v {\displaystyle \mathbf {u} ,\mathbf {v} \in v\,\!} . equivalently, a line segment is the convex hull of two points. thus, the line segment can be expressed as a convex combination of the segment's two end points. in geometry, one might define point b to be between two other points a and c, if the distance ab added to the distance bc is equal to the distance ac. thus in r   {\displaystyle \mathbb {r} ^{ }} , the line segment with endpoints a = (ax, ay) and c = (cx, cy) is the following collection of points: in an axiomatic treatment of geometry, the notion of betweenness is either assumed to satisfy a certain number of axioms, or defined in terms of an isometry of a line (used as a coordinate system). segments play an important role in other theories. for example, in a convex set, the segment that joins any two points of the set is contained in the set. this is important because it transforms some of the analysis of convex sets, to the analysis of a line segment. the segment addition postulate can be used to add congruent segment or segments with equal lengths, and consequently substitute other segments into another statement to make segments congruent. "
246,246,Linear Algebra,1,https://en.wikipedia.org/wiki/Linear_algebra,"linear algebra is the branch of mathematics concerning linear equations such as: linear maps such as: and their representations in vector spaces and through matrices. linear algebra is central to almost all areas of mathematics. for instance, linear algebra is fundamental in modern presentations of geometry, including for defining basic objects such as lines, planes and rotations. also, functional analysis, a branch of mathematical analysis, may be viewed as the application of linear algebra to spaces of functions. linear algebra is also used in most sciences and fields of engineering, because it allows modeling many natural phenomena, and computing efficiently with such models. for nonlinear systems, which cannot be modeled with linear algebra, it is often used for dealing with first-order approximations, using the fact that the differential of a multivariate function at a point is the linear map that best approximates the function near that point. the procedure (using counting rods) for solving simultaneous linear equations now called gaussian elimination appears in the ancient chinese mathematical text chapter eight: rectangular arrays of the nine chapters on the mathematical art. its use is illustrated in eighteen problems, with two to five equations. systems of linear equations arose in europe with the introduction in      by rené descartes of coordinates in geometry. in fact, in this new geometry, now called cartesian geometry, lines and planes are represented by linear equations, and computing their intersections amounts to solving systems of linear equations. the first systematic methods for solving linear systems used determinants, first considered by leibniz in     . in     , gabriel cramer used them for giving explicit solutions of linear systems, now called cramer's rule. later, gauss further described the method of elimination, which was initially listed as an advancement in geodesy. in      hermann grassmann published his ""theory of extension"" which included foundational new topics of what is today called linear algebra. in     , james joseph sylvester introduced the term matrix, which is latin for womb. linear algebra grew with ideas noted in the complex plane. for instance, two numbers w and z in ℂ have a difference w – z, and the line segments wz and  (w − z) are of the same length and direction. the segments are equipollent. the four-dimensional system ℍ of quaternions was started in     . the term vector was introduced as v = xi + yj + zk representing a point in space. the quaternion difference p – q also produces a segment equipollent to pq. other hypercomplex number systems also used the idea of a linear space with a basis. "
247,247,Linear Combination,1,https://en.wikipedia.org/wiki/Linear_combination,"in mathematics, a linear combination is an expression constructed from a set of terms by multiplying each term by a constant and adding the results (e.g. a linear combination of x and y would be any expression of the form ax + by, where a and b are constants). the concept of linear combinations is central to linear algebra and related fields of mathematics. most of this article deals with linear combinations in the context of a vector space over a field, with some generalizations given at the end of the article. let v be a vector space over the field k. as usual, we call elements of v vectors and call elements of k scalars. if v ,...,vn are vectors and a ,...,an are scalars, then the linear combination of those vectors with those scalars as coefficients is there is some ambiguity in the use of the term ""linear combination"" as to whether it refers to the expression or to its value. in most cases the value is emphasized, as in the assertion ""the set of all linear combinations of v ,...,vn always forms a subspace"". however, one could also say ""two different linear combinations can have the same value"" in which case the reference is to the expression. the subtle difference between these uses is the essence of the notion of linear dependence: a family f of vectors is linearly independent precisely if any linear combination of the vectors in f (as value) is uniquely so (as expression). in any case, even when viewed as expressions, all that matters about a linear combination is the coefficient of each vi; trivial modifications such as permuting the terms or adding terms with zero coefficient do not produce distinct linear combinations. in a given situation, k and v may be specified explicitly, or they may be obvious from context. in that case, we often speak of a linear combination of the vectors v ,...,vn, with the coefficients unspecified (except that they must belong to k). or, if s is a subset of v, we may speak of a linear combination of vectors in s, where both the coefficients and the vectors are unspecified, except that the vectors must belong to the set s (and the coefficients must belong to k). finally, we may speak simply of a linear combination, where nothing is specified (except that the vectors must belong to v and the coefficients must belong to k); in this case one is probably referring to the expression, since every vector in v is certainly the value of some linear combination. note that by definition, a linear combination involves only finitely many vectors (except as described in generalizations below). however, the set s that the vectors are taken from (if one is mentioned) can still be infinite; each individual linear combination will only involve finitely many vectors. also, there is no reason that n cannot be zero; in that case, we declare by convention that the result of the linear combination is the zero vector in v. let the field k be the set r of real numbers, and let the vector space v be the euclidean space r . consider the vectors e  = ( , , ), e  = ( , , ) and e  = ( , , ). then any vector in r  is a linear combination of e , e , and e . to see that this is so, take an arbitrary vector (a ,a ,a ) in r , and write: let k be the set c of all complex numbers, and let v be the set cc(r) of all continuous functions from the real line r to the complex plane c. consider the vectors (functions) f and g defined by f(t) := eit and g(t) := e−it. (here, e is the base of the natural logarithm, about  .     ..., and i is the imaginary unit, a square root of − .) some linear combinations of f and g are: on the other hand, the constant function   is not a linear combination of f and g. to see this, suppose that   could be written as a linear combination of eit and e−it. this means that there would exist complex scalars a and b such that aeit + be−it =   for all real numbers t. setting t =   and t = π gives the equations a + b =   and a + b = − , and clearly this cannot happen. see euler's identity. let k be r, c, or any field, and let v be the set p of all polynomials with coefficients taken from the field k. consider the vectors (polynomials) p  :=  , p  := x +  , and p  := x  + x +  . "
248,248,First order equation with variable coefficients,2,https://en.wikipedia.org/wiki/Linear_differential_equation,"in mathematics, a linear differential equation is a differential equation that is defined by a linear polynomial in the unknown function and its derivatives, that is an equation of the form where a (x), ..., an(x) and b(x) are arbitrary differentiable functions that do not need to be linear, and y′, ..., y(n) are the successive derivatives of an unknown function y of the variable x. such an equation is an ordinary differential equation (ode). a linear differential equation may also be a linear partial differential equation (pde), if the unknown function depends on several variables, and the derivatives that appear in the equation are partial derivatives. a linear differential equation or a system of linear equations such that the associated homogeneous equations have constant coefficients may be solved by quadrature, which means that the solutions may be expressed in terms of integrals. this is also true for a linear equation of order one, with non-constant coefficients. an equation of order two or higher with non-constant coefficients cannot, in general, be solved by quadrature. for order two, kovacic's algorithm allows deciding whether there are solutions in terms of integrals, and computing them if any. the solutions of linear differential equations with polynomial coefficients are called holonomic functions. this class of functions is stable under sums, products, differentiation, integration, and contains many usual functions and special functions such as exponential function, logarithm, sine, cosine, inverse trigonometric functions, error function, bessel functions and hypergeometric functions. their representation by the defining differential equation and initial conditions allows making algorithmic (on these functions) most operations of calculus, such as computation of antiderivatives, limits, asymptotic expansion, and numerical evaluation to any precision, with a certified error bound. the highest order of derivation that appears in a (linear) differential equation is the order of the equation. the term b(x), which does not depend on the unknown function and its derivatives, is sometimes called the constant term of the equation (by analogy with algebraic equations), even when this term is a non-constant function. if the constant term is the zero function, then the differential equation is said to be homogeneous, as it is a homogeneous polynomial in the unknown function and its derivatives. the equation obtained by replacing, in a linear differential equation, the constant term by the zero function is the associated homogeneous equation. a differential equation has constant coefficients if only constant functions appear as coefficients in the associated homogeneous equation. a solution of a differential equation is a function that satisfies the equation. the solutions of a homogeneous linear differential equation form a vector space. in the ordinary case, this vector space has a finite dimension, equal to the order of the equation. all solutions of a linear differential equation are found by adding to a particular solution any solution of the associated homogeneous equation. a basic differential operator of order i is a mapping that maps any differentiable function to its ith derivative, or, in the case of several variables, to one of its partial derivatives of order i. it is commonly denoted in the case of univariate functions, and in the case of functions of n variables. the basic differential operators include the derivative of order  , which is the identity mapping. "
249,249,Equation of Line in Various Forms,1,https://en.wikipedia.org/wiki/Linear_equation,"in mathematics, a linear equation is an equation that may be put in the form where x   , … , x n {\displaystyle x_{ },\ldots ,x_{n}} are the variables (or unknowns), and b , a   , … , a n {\displaystyle b,a_{ },\ldots ,a_{n}} are the coefficients, which are often real numbers. the coefficients may be considered as parameters of the equation, and may be arbitrary expressions, provided they do not contain any of the variables. to yield a meaningful equation, the coefficients a   , … , a n {\displaystyle a_{ },\ldots ,a_{n}} are required to not all be zero. alternatively a linear equation can be obtained by equating to zero a linear polynomial over some field, from which the coefficients are taken. the solutions of such an equation are the values that, when substituted for the unknowns, make the equality true. in the case of just one variable, there is exactly one solution (provided that a   ≠   {\displaystyle a_{ }\neq  } ). often, the term linear equation refers implicitly to this particular case, in which the variable is sensibly called the unknown. in the case of two variables, each solution may be interpreted as the cartesian coordinates of a point of the euclidean plane. the solutions of a linear equation form a line in the euclidean plane, and, conversely, every line can be viewed as the set of all solutions of a linear equation in two variables. this is the origin of the term linear for describing this type of equations. more generally, the solutions of a linear equation in n variables form a hyperplane (a subspace of dimension n −  ) in the euclidean space of dimension n. linear equations occur frequently in all mathematics and their applications in physics and engineering, partly because non-linear systems are often well approximated by linear equations. this article considers the case of a single equation with coefficients from the field of real numbers, for which one studies the real solutions. all of its content applies to complex solutions and, more generally, for linear equations with coefficients and solutions in any field. for the case of several simultaneous linear equations, see system of linear equations. frequently the term linear equation refers implicitly to the case of just one variable. in this case, the equation can be put in the form "
250,250,Linear Map,0,https://en.wikipedia.org/wiki/Linear_map,"in mathematics, and more specifically in linear algebra, a linear map (also called a linear mapping, linear transformation, vector space homomorphism, or in some contexts linear function) is a mapping v → w {\displaystyle v\to w} between two vector spaces that preserves the operations of vector addition and scalar multiplication. the same names and the same definition are also used for the more general case of modules over a ring; see module homomorphism. if a linear map is a bijection then it is called a linear isomorphism. in the case where v = w {\displaystyle v=w} , a linear map is called a (linear) endomorphism. sometimes the term linear operator refers to this case, but the term ""linear operator"" can have different meanings for different conventions: for example, it can be used to emphasize that v {\displaystyle v} and w {\displaystyle w} are real vector spaces (not necessarily with v = w {\displaystyle v=w} ),[citation needed] or it can be used to emphasize that v {\displaystyle v} is a function space, which is a common convention in functional analysis. sometimes the term linear function has the same meaning as linear map, while in analysis it does not. a linear map from v to w always maps the origin of v to the origin of w. moreover, it maps linear subspaces in v onto linear subspaces in w (possibly of a lower dimension); for example, it maps a plane through the origin in v to either a plane through the origin in w, a line through the origin in w, or just the origin in w. linear maps can often be represented as matrices, and simple examples include rotation and reflection linear transformations. in the language of category theory, linear maps are the morphisms of vector spaces. let v {\displaystyle v} and w {\displaystyle w} be vector spaces over the same field k {\displaystyle k} . a function f : v → w {\displaystyle f:v\to w} is said to be a linear map if for any two vectors u , v ∈ v {\textstyle \mathbf {u} ,\mathbf {v} \in v} and any scalar c ∈ k {\displaystyle c\in k} the following two conditions are satisfied: thus, a linear map is said to be operation preserving. in other words, it does not matter whether the linear map is applied before (the right hand sides of the above examples) or after (the left hand sides of the examples) the operations of addition and scalar multiplication. by the associativity of the addition operation denoted as +, for any vectors u   , … , u n ∈ v {\textstyle \mathbf {u} _{ },\ldots ,\mathbf {u} _{n}\in v} and scalars c   , … , c n ∈ k , {\textstyle c_{ },\ldots ,c_{n}\in k,} the following equality holds: denoting the zero elements of the vector spaces v {\displaystyle v} and w {\displaystyle w} by   v {\textstyle \mathbf { } _{v}} and   w {\textstyle \mathbf { } _{w}} respectively, it follows that f (   v ) =   w . {\textstyle f(\mathbf { } _{v})=\mathbf { } _{w}.} let c =   {\displaystyle c= } and v ∈ v {\textstyle \mathbf {v} \in v} in the equation for homogeneity of degree  : a linear map v → k {\displaystyle v\to k} with k {\displaystyle k} viewed as a one-dimensional vector space over itself is called a linear functional. these statements generalize to any left-module r m {\textstyle {}_{r}m} over a ring r {\displaystyle r} without modification, and to any right-module upon reversing of the scalar multiplication. "
251,251,Linear programming,2,https://en.wikipedia.org/wiki/Linear_programming,"linear programming (lp, also called linear optimization) is a method to achieve the best outcome (such as maximum profit or lowest cost) in a mathematical model whose requirements are represented by linear relationships. linear programming is a special case of mathematical programming (also known as mathematical optimization). more formally, linear programming is a technique for the optimization of a linear objective function, subject to linear equality and linear inequality constraints. its feasible region is a convex polytope, which is a set defined as the intersection of finitely many half spaces, each of which is defined by a linear inequality. its objective function is a real-valued affine (linear) function defined on this polyhedron. a linear programming algorithm finds a point in the polytope where this function has the smallest (or largest) value if such a point exists. linear programs are problems that can be expressed in canonical form as here the components of x are the variables to be determined, c and b are given vectors (with c t {\displaystyle \mathbf {c} ^{t}} indicating that the coefficients of c are used as a single-row matrix for the purpose of forming the matrix product), and a is a given matrix. the function whose value is to be maximized or minimized ( x ↦ c t x {\displaystyle \mathbf {x} \mapsto \mathbf {c} ^{t}\mathbf {x} } in this case) is called the objective function. the inequalities ax ≤ b and x ≥   are the constraints which specify a convex polytope over which the objective function is to be optimized. in this context, two vectors are comparable when they have the same dimensions. if every entry in the first is less-than or equal-to the corresponding entry in the second, then it can be said that the first vector is less-than or equal-to the second vector. linear programming can be applied to various fields of study. it is widely used in mathematics, and to a lesser extent in business, economics, and for some engineering problems. industries that use linear programming models include transportation, energy, telecommunications, and manufacturing. it has proven useful in modeling diverse types of problems in planning, routing, scheduling, assignment, and design. the problem of solving a system of linear inequalities dates back at least as far as fourier, who in      published a method for solving them, and after whom the method of fourier–motzkin elimination is named. in      a linear programming formulation of a problem that is equivalent to the general linear programming problem was given by the soviet mathematician and economist leonid kantorovich, who also proposed a method for solving it. it is a way he developed, during world war ii, to plan expenditures and returns in order to reduce costs of the army and to increase losses imposed on the enemy.[citation needed] kantorovich's work was initially neglected in the ussr. about the same time as kantorovich, the dutch-american economist t. c. koopmans formulated classical economic problems as linear programs. kantorovich and koopmans later shared the      nobel prize in economics. in     , frank lauren hitchcock also formulated transportation problems as linear programs and gave a solution very similar to the later simplex method. hitchcock had died in      and the nobel prize is not awarded posthumously. during     –    , george b. dantzig independently developed general linear programming formulation to use for planning problems in the us air force. in     , dantzig also invented the simplex method that for the first time efficiently tackled the linear programming problem in most cases. when dantzig arranged a meeting with john von neumann to discuss his simplex method, neumann immediately conjectured the theory of duality by realizing that the problem he had been working in game theory was equivalent. dantzig provided formal proof in an unpublished report ""a theorem on linear inequalities"" on january  ,     . dantzig's work was made available to public in     . in the post-war years, many industries applied it in their daily planning. dantzig's original example was to find the best assignment of    people to    jobs. the computing power required to test all the permutations to select the best assignment is vast; the number of possible configurations exceeds the number of particles in the observable universe. however, it takes only a moment to find the optimum solution by posing the problem as a linear program and applying the simplex algorithm. the theory behind linear programming drastically reduces the number of possible solutions that must be checked. the linear programming problem was first shown to be solvable in polynomial time by leonid khachiyan in     , but a larger theoretical and practical breakthrough in the field came in      when narendra karmarkar introduced a new interior-point method for solving linear-programming problems. "
252,252,Linear Span,0,https://en.wikipedia.org/wiki/Linear_span,"in mathematics, the linear span (also called the linear hull or just span) of a set s of vectors (from a vector space), denoted span(s), is the smallest linear subspace that contains the set. it can be characterized either as the intersection of all linear subspaces that contain s, or as the set of linear combinations of elements of s. the linear span of a set of vectors is therefore a vector space. spans can be generalized to matroids and modules. for expressing that a vector space v is a span of a set s, one commonly uses the following phrases: s spans v; s generates v; v is spanned by s; v is generated by s; s is a spanning set of v; s is a generating set of v. given a vector space v over a field k, the span of a set s of vectors (not necessarily infinite) is defined to be the intersection w of all subspaces of v that contain s. w is referred to as the subspace spanned by s, or by the vectors in s. conversely, s is called a spanning set of w, and we say that s spans w. alternatively, the span of s may be defined as the set of all finite linear combinations of elements (vectors) of s, which follows from the above definition. in the case of infinite s, infinite linear combinations (i.e. where a combination may involve an infinite sum, assuming that such sums are defined somehow as in, say, a banach space) are excluded by the definition; a generalization that allows these is not equivalent. the real vector space r  has {(− ,  ,  ), ( ,  ,  ), ( ,  ,  )} as a spanning set. this particular spanning set is also a basis. if (− ,  ,  ) were replaced by ( ,  ,  ), it would also form the canonical basis of r . another spanning set for the same space is given by {( ,  ,  ), ( ,  ,  ), (− ,  ⁄ ,  ), ( ,  ,  )}, but this set is not a basis, because it is linearly dependent. the set {( ,  ,  ), ( ,  ,  ), ( ,  ,  )} is not a spanning set of r , since its span is the space of all vectors in r  whose last component is zero. that space is also spanned by the set {( ,  ,  ), ( ,  ,  )}, as ( ,  ,  ) is a linear combination of ( ,  ,  ) and ( ,  ,  ). it does, however, span r .(when interpreted as a subset of r ). the empty set is a spanning set of {( ,  ,  )}, since the empty set is a subset of all possible vector spaces in r , and {( ,  ,  )} is the intersection of all of these vector spaces. "
253,253,Linear Subspace,2,https://en.wikipedia.org/wiki/Linear_subspace,"in mathematics, and more specifically in linear algebra, a linear subspace, also known as a vector subspace [note  ] is a vector space that is a subset of some larger vector space. a linear subspace is usually simply called a subspace when the context serves to distinguish it from other types of subspaces. if v is a vector space over a field k and if w is a subset of v, then w is a linear subspace of v if under the operations of v, w is a vector space over k. equivalently, a nonempty subset w is a subspace of v if, whenever w , w  are elements of w and α, β are elements of k, it follows that αw  + βw  is in w. as a corollary, all vector spaces are equipped with at least two (possibly different) linear subspaces: the zero vector space consisting of the zero vector alone and the entire vector space itself. these are called the trivial subspaces of the vector space. let the field k be the set r of real numbers, and let the vector space v be the real coordinate space r . take w to be the set of all vectors in v whose last component is  . then w is a subspace of v. proof: let the field be r again, but now let the vector space v be the cartesian plane r . take w to be the set of points (x, y) of r  such that x = y. then w is a subspace of r . proof: in general, any subset of the real coordinate space rn that is defined by a system of homogeneous linear equations will yield a subspace. (the equation in example i was z =  , and the equation in example ii was x = y.) geometrically, these subspaces are points, lines, planes and spaces that pass through the point  . again take the field to be r, but now let the vector space v be the set rr of all functions from r to r. let c(r) be the subset consisting of continuous functions. then c(r) is a subspace of rr. proof: "
254,254,Linearization,2,https://en.wikipedia.org/wiki/Linearization,"in mathematics, linearization is finding the linear approximation to a function at a given point. the linear approximation of a function is the first order taylor expansion around the point of interest. in the study of dynamical systems, linearization is a method for assessing the local stability of an equilibrium point of a system of nonlinear differential equations or discrete dynamical systems. this method is used in fields such as engineering, physics, economics, and ecology. linearizations of a function are lines—usually lines that can be used for purposes of calculation. linearization is an effective method for approximating the output of a function y = f ( x ) {\displaystyle y=f(x)} at any x = a {\displaystyle x=a} based on the value and slope of the function at x = b {\displaystyle x=b} , given that f ( x ) {\displaystyle f(x)} is differentiable on [ a , b ] {\displaystyle [a,b]} (or [ b , a ] {\displaystyle [b,a]} ) and that a {\displaystyle a} is close to b {\displaystyle b} . in short, linearization approximates the output of a function near x = a {\displaystyle x=a} . for example,   =   {\displaystyle {\sqrt { }}= } . however, what would be a good approximation of  .    =   + .    {\displaystyle {\sqrt { .   }}={\sqrt { +.   }}} ? for any given function y = f ( x ) {\displaystyle y=f(x)} , f ( x ) {\displaystyle f(x)} can be approximated if it is near a known differentiable point. the most basic requisite is that l a ( a ) = f ( a ) {\displaystyle l_{a}(a)=f(a)} , where l a ( x ) {\displaystyle l_{a}(x)} is the linearization of f ( x ) {\displaystyle f(x)} at x = a {\displaystyle x=a} . the point-slope form of an equation forms an equation of a line, given a point ( h , k ) {\displaystyle (h,k)} and slope m {\displaystyle m} . the general form of this equation is: y − k = m ( x − h ) {\displaystyle y-k=m(x-h)} . using the point ( a , f ( a ) ) {\displaystyle (a,f(a))} , l a ( x ) {\displaystyle l_{a}(x)} becomes y = f ( a ) + m ( x − a ) {\displaystyle y=f(a)+m(x-a)} . because differentiable functions are locally linear, the best slope to substitute in would be the slope of the line tangent to f ( x ) {\displaystyle f(x)} at x = a {\displaystyle x=a} . while the concept of local linearity applies the most to points arbitrarily close to x = a {\displaystyle x=a} , those relatively close work relatively well for linear approximations. the slope m {\displaystyle m} should be, most accurately, the slope of the tangent line at x = a {\displaystyle x=a} . visually, the accompanying diagram shows the tangent line of f ( x ) {\displaystyle f(x)} at x {\displaystyle x} . at f ( x + h ) {\displaystyle f(x+h)} , where h {\displaystyle h} is any small positive or negative value, f ( x + h ) {\displaystyle f(x+h)} is very nearly the value of the tangent line at the point ( x + h , l ( x + h ) ) {\displaystyle (x+h,l(x+h))} . the final equation for the linearization of a function at x = a {\displaystyle x=a} is: y = ( f ( a ) + f ′ ( a ) ( x − a ) ) {\displaystyle y=(f(a)+f'(a)(x-a))} for x = a {\displaystyle x=a} , f ( a ) = f ( x ) {\displaystyle f(a)=f(x)} . the derivative of f ( x ) {\displaystyle f(x)} is f ′ ( x ) {\displaystyle f'(x)} , and the slope of f ( x ) {\displaystyle f(x)} at a {\displaystyle a} is f ′ ( a ) {\displaystyle f'(a)} . "
255,255,definite integrals and their properties,3,https://en.wikipedia.org/wiki/List_of_definite_integrals," in mathematics, the definite integral is the area of the region in the xy-plane bounded by the graph of f, the x-axis, and the lines x = a and x = b, such that area above the x-axis adds to the total, and that below the x-axis subtracts from the total. the fundamental theorem of calculus establishes the relationship between indefinite and definite integrals and introduces a technique for evaluating definite integrals. if the interval is infinite the definite integral is called an improper integral and defined by using appropriate limiting procedures. for example: a constant, such pi, that may be defined by the integral of an algebraic function over an algebraic domain is known as a period. the following is a list of the most common or interesting definite integrals. for a list of indefinite integrals see list of indefinite integrals. "
256,256,list of formulas in elementarty geometry,1,https://en.wikipedia.org/wiki/List_of_formulas_in_elementary_geometry," this is a short list of some common mathematical shapes and figures and the formulas that describe them. the basic quantities describing a sphere (meaning a  -sphere, a  -dimensional surface inside  -dimensional space) will be denoted by the following variables surface area: s =   π r   =   π c   = π (   v )     {\displaystyle {\begin{alignedat}{ }s&= \pi r^{ }\\[ . ex]&={\frac { }{\pi }}c^{ }\\[ . ex]&={\sqrt[{ }]{\pi ( v)^{ }}}\\[ . ex]\end{alignedat}}} volume: v =     π r   =     π   c   =     π s   /   {\displaystyle {\begin{alignedat}{ }v&={\frac { }{ }}\pi r^{ }\\[ . ex]&={\frac { }{ \pi ^{ }}}c^{ }\\[ . ex]&={\frac { }{ {\sqrt {\pi }}}}s^{ / }\\[ . ex]\end{alignedat}}} radius: r =     π c =     π s =     π v   {\displaystyle {\begin{alignedat}{ }r&={\frac { }{ \pi }}c\\[ . ex]&={\sqrt {{\frac { }{ \pi }}s}}\\[ . ex]&={\sqrt[{ }]{{\frac { }{ \pi }}v}}\\[ . ex]\end{alignedat}}} circumference: c =   π r = π s = π     v   {\displaystyle {\begin{alignedat}{ }c&= \pi r\\[ . ex]&={\sqrt {\pi s}}\\[ . ex]&={\sqrt[{ }]{\pi ^{ } v}}\\[ . ex]\end{alignedat}}} "
257,257,List Of Integrals of Exponential Functions,1,https://en.wikipedia.org/wiki/List_of_integrals_of_exponential_functions,"the following is a list of integrals of exponential functions. for a complete list of integral functions, please see the list of integrals. indefinite integrals are antiderivative functions. a constant (the constant of integration) may be added to the right hand side of any of these formulas, but has been suppressed here in the interest of brevity. in the following formulas, erf is the error function and ei is the exponential integral. the below formulae was proved by toyesh the last expression is the logarithmic mean. ∫ − ∞ ∞ e − ( a x   + b x + c ) d x = π a e b     a − c ( a >   ) {\displaystyle \int _{-\infty }^{\infty }e^{-(ax^{ }+bx+c)}\,dx={\sqrt {\pi \over a}}e^{{\tfrac {b^{ }}{ a}}-c}\quad (a> )} (the operator ! ! {\displaystyle !!} is the double factorial) where li s ⁡ ( z ) {\displaystyle \operatorname {li} _{s}(z)} is the polylogarithm. where γ {\displaystyle \gamma } is the euler–mascheroni constant which equals the value of a number of definite integrals. finally, a well known result, "
258,258,List of Integrals of Inverse Trigonometric Functions,2,https://en.wikipedia.org/wiki/List_of_integrals_of_inverse_trigonometric_functions," the following is a list of indefinite integrals (antiderivatives) of expressions involving the inverse trigonometric functions. for a complete list of integral formulas, see lists of integrals. "
259,259,List of Integrals of Irrational Functions,2,https://en.wikipedia.org/wiki/List_of_integrals_of_irrational_functions," the following is a list of integrals (antiderivative functions) of irrational functions. for a complete list of integral functions, see lists of integrals. throughout this article the constant of integration is omitted for brevity. assume x  > a  (for x  < a , see next section): here ln ⁡ | x + s a | = sgn ⁡ ( x ) arcosh ⁡ | x a | =     ln ⁡ ( x + s x − s ) {\displaystyle \ln \left|{\frac {x+s}{a}}\right|=\operatorname {sgn} (x)\,\operatorname {arcosh} \left|{\frac {x}{a}}\right|={\frac { }{ }}\ln \left({\frac {x+s}{x-s}}\right)} , where the positive value of arcosh ⁡ | x a | {\displaystyle \operatorname {arcosh} \left|{\frac {x}{a}}\right|} is to be taken. assume (ax  + bx + c) cannot be reduced to the following expression (px + q)  for some p and q. "
260,260,List Of Integrals of Logarithmic Functions,2,https://en.wikipedia.org/wiki/List_of_integrals_of_logarithmic_functions," the following is a list of integrals (antiderivative functions) of logarithmic functions. for a complete list of integral functions, see list of integrals. note: x >   is assumed throughout this article, and the constant of integration is omitted for simplicity. these generalization was given by toyesh for n {\displaystyle n} consecutive integrations, the formula generalizes to "
261,261,List Of Integrals of Trigonometric Functions,2,https://en.wikipedia.org/wiki/List_of_integrals_of_trigonometric_functions," the following is a list of integrals (antiderivative functions) of trigonometric functions. for antiderivatives involving both exponential and trigonometric functions, see list of integrals of exponential functions. for a complete list of antiderivative functions, see lists of integrals. for the special antiderivatives involving trigonometric functions, see trigonometric integral. generally, if the function sin ⁡ x {\displaystyle \sin x} is any trigonometric function, and cos ⁡ x {\displaystyle \cos x} is its derivative, in all formulas the constant a is assumed to be nonzero, and c denotes the constant of integration. an integral that is a rational function of the sine and cosine can be evaluated using bioche's rules. "
262,262,Logarithm Identities,1,https://en.wikipedia.org/wiki/List_of_logarithmic_identities,"in mathematics, many logarithmic identities exist. the following is a compilation of the notable of these, many of which are used for computational purposes. logarithms and exponentials with the same base cancel each other. this is true because logarithms and exponentials are inverse operations—much like the same way multiplication and division are inverse operations, and addition and subtraction are inverse operations. both of the above are derived from the following two equations that define a logarithm: substituting c in the left equation gives blogb(x) = x, and substituting x in the right gives logb(bc) = c. finally, replace c with x. logarithms can be used to make calculations easier. for example, two numbers can be multiplied just by using a logarithm table and adding. these are often known as logarithmic properties, which are documented in the table below. the first three operations below assume that x = bc and/or y = bd, so that logb(x) = c and logb(y) = d. derivations also use the log definitions x = blogb(x) and x = logb(bx). where b {\displaystyle b} , x {\displaystyle x} , and y {\displaystyle y} are positive real numbers and b ≠   {\displaystyle b\neq  } , and c {\displaystyle c} and d {\displaystyle d} are real numbers. the laws result from canceling exponentials and the appropriate law of indices. starting with the first law: x y = b log b ⁡ ( x ) b log b ⁡ ( y ) = b log b ⁡ ( x ) + log b ⁡ ( y ) ⇒ log b ⁡ ( x y ) = log b ⁡ ( b log b ⁡ ( x ) + log b ⁡ ( y ) ) = log b ⁡ ( x ) + log b ⁡ ( y ) {\displaystyle xy=b^{\log _{b}(x)}b^{\log _{b}(y)}=b^{\log _{b}(x)+\log _{b}(y)}\rightarrow \log _{b}(xy)=\log _{b}(b^{\log _{b}(x)+\log _{b}(y)})=\log _{b}(x)+\log _{b}(y)} the law for powers exploits another of the laws of indices: x y = ( b log b ⁡ ( x ) ) y = b y log b ⁡ ( x ) ⇒ log b ⁡ ( x y ) = y log b ⁡ ( x ) {\displaystyle x^{y}=(b^{\log _{b}(x)})^{y}=b^{y\log _{b}(x)}\rightarrow \log _{b}(x^{y})=y\log _{b}(x)} "
263,263,Half-angle formula,2,https://en.wikipedia.org/wiki/List_of_trigonometric_identities,"in mathematics, trigonometric identities are equalities that involve trigonometric functions and are true for every value of the occurring variables for which both sides of the equality are defined. geometrically, these are identities involving certain functions of one or more angles. they are distinct from triangle identities, which are identities potentially involving angles but also involving side lengths or other lengths of a triangle. these identities are useful whenever expressions involving trigonometric functions need to be simplified. an important application is the integration of non-trigonometric functions: a common technique involves first using the substitution rule with a trigonometric function, and then simplifying the resulting integral with a trigonometric identity. the basic relationship between the sine and cosine is given by the pythagorean identity: where sin   ⁡ θ {\displaystyle \sin ^{ }\theta } means ( sin ⁡ θ )   {\displaystyle (\sin \theta )^{ }} and cos   ⁡ θ {\displaystyle \cos ^{ }\theta } means ( cos ⁡ θ )   . {\displaystyle (\cos \theta )^{ }.} this can be viewed as a version of the pythagorean theorem, and follows from the equation x   + y   =   {\displaystyle x^{ }+y^{ }= } for the unit circle. this equation can be solved for either the sine or the cosine: where the sign depends on the quadrant of θ . {\displaystyle \theta .} dividing this identity by sin   ⁡ θ {\displaystyle \sin ^{ }\theta } , cos   ⁡ θ {\displaystyle \cos ^{ }\theta } , or both yields the following identities: using these identities, it is possible to express any trigonometric function in terms of any other (up to a plus or minus sign): by examining the unit circle, one can establish the following properties of the trigonometric functions. when the direction of a euclidean vector is represented by an angle θ , {\displaystyle \theta ,} this is the angle determined by the free vector (starting at the origin) and the positive x {\displaystyle x} -unit vector. the same concept may also be applied to lines in a euclidean space, where the angle is that determined by a parallel to the given line through the origin and the positive x {\displaystyle x} -axis. if a line (vector) with direction θ {\displaystyle \theta } is reflected about a line with direction α , {\displaystyle \alpha ,} then the direction angle θ ′ {\displaystyle \theta ^{\prime }} of this reflected line (vector) has the value "
264,264,half-angle formula,1,https://en.wikipedia.org/wiki/List_of_trigonometric_identitiess,
265,265,List of Integrals,1,https://en.wikipedia.org/wiki/Lists_of_integrals,"integration is the basic operation in integral calculus. while differentiation has straightforward rules by which the derivative of a complicated function can be found by differentiating its simpler component functions, integration does not, so tables of known integrals are often useful. this page lists some of the most common antiderivatives. a compilation of a list of integrals (integraltafeln) and techniques of integral calculus was published by the german mathematician meier hirsch [de] (aka meyer hirsch [de]) in     . these tables were republished in the united kingdom in     . more extensive tables were compiled in      by the dutch mathematician david bierens de haan for his tables d'intégrales définies, supplemented by supplément aux tables d'intégrales définies in ca.     . a new edition was published in      under the title nouvelles tables d'intégrales définies. these tables, which contain mainly integrals of elementary functions, remained in use until the middle of the   th century. they were then replaced by the much more extensive tables of gradshteyn and ryzhik. in gradshteyn and ryzhik, integrals originating from the book by bierens de haan are denoted by bi. not all closed-form expressions have closed-form antiderivatives; this study forms the subject of differential galois theory, which was initially developed by joseph liouville in the     s and     s, leading to liouville's theorem which classifies which expressions have closed form antiderivatives. a simple example of a function without a closed form antiderivative is e−x , whose antiderivative is (up to constants) the error function. since      there is the risch algorithm for determining indefinite integrals that can be expressed in term of elementary functions, typically using a computer algebra system. integrals that cannot be expressed using elementary functions can be manipulated symbolically using general functions such as the meijer g-function. more detail may be found on the following pages for the lists of integrals: gradshteyn, ryzhik, geronimus, tseytlin, jeffrey, zwillinger, and moll's (gr) table of integrals, series, and products contains a large collection of results. an even larger, multivolume table is the integrals and series by prudnikov, brychkov, and marichev (with volumes  –  listing integrals and series of elementary and special functions, volume  –  are tables of laplace transforms). more compact collections can be found in e.g. brychkov, marichev, prudnikov's tables of indefinite integrals, or as chapters in zwillinger's crc standard mathematical tables and formulae or bronshtein and semendyayev's guide book to mathematics, handbook of mathematics or users' guide to mathematics, and other mathematical handbooks. other useful resources include abramowitz and stegun and the bateman manuscript project. both works contain many identities concerning specific integrals, which are organized with the most relevant topic instead of being collected into a separate table. two volumes of the bateman manuscript are specific to integral transforms. there are several web sites which have tables of integrals and integrals on demand. wolfram alpha can show results, and for some simpler expressions, also the intermediate steps of the integration. wolfram research also operates another online service, the wolfram mathematica online integrator. c is used for an arbitrary constant of integration that can only be determined if something about the value of the integral at some point is known. thus, each function has an infinite number of antiderivatives. these formulas only state in another form the assertions in the table of derivatives. "
266,266,Locus problems,0,https://en.wikipedia.org/wiki/Locus,"locus (plural loci) is latin for ""place"". it may refer to: "
267,267,Locus (mathematics),2,https://en.wikipedia.org/wiki/Locus_(mathematics),"in geometry, a locus (plural: loci) (latin word for ""place"", ""location"") is a set of all points (commonly, a line, a line segment, a curve or a surface), whose location satisfies or is determined by one or more specified conditions. in other words, the set of the points that satisfy some property is often called the locus of a point satisfying this property. the use of the singular in this formulation is a witness that, until the end of the   th century, mathematicians did not consider infinite sets. instead of viewing lines and curves as sets of points, they viewed them as places where a point may be located or may move. until the beginning of the   th century, a geometrical shape (for example a curve) was not considered as an infinite set of points; rather, it was considered as an entity on which a point may be located or on which it moves. thus a circle in the euclidean plane was defined as the locus of a point that is at a given distance of a fixed point, the center of the circle. in modern mathematics, similar concepts are more frequently reformulated by describing shapes as sets; for instance, one says that the circle is the set of points that are at a given distance from the center. in contrast to the set-theoretic view, the old formulation avoids considering infinite collections, as avoiding the actual infinite was an important philosophical position of earlier mathematicians. once set theory became the universal basis over which the whole mathematics is built, the term of locus became rather old-fashioned. nevertheless, the word is still widely used, mainly for a concise formulation, for example: more recently, techniques such as the theory of schemes, and the use of category theory instead of set theory to give a foundation to mathematics, have returned to notions more like the original definition of a locus as an object in itself rather than as a set of points. examples from plane geometry include: other examples of loci appear in various areas of mathematics. for example, in complex dynamics, the mandelbrot set is a subset of the complex plane that may be characterized as the connectedness locus of a family of polynomial maps. to prove a geometric shape is the correct locus for a given set of conditions, one generally divides the proof into two stages: find the locus of a point p that has a given ratio of distances k = d /d  to two given points. "
268,268,Log-normal distribution,2,https://en.wikipedia.org/wiki/Log-normal_distribution,"in probability theory, a log-normal (or lognormal) distribution is a continuous probability distribution of a random variable whose logarithm is normally distributed. thus, if the random variable x is log-normally distributed, then y = ln(x) has a normal distribution. equivalently, if y has a normal distribution, then the exponential function of y, x = exp(y), has a log-normal distribution. a random variable which is log-normally distributed takes only positive real values. it is a convenient and useful model for measurements in exact and engineering sciences, as well as medicine, economics and other topics (e.g., energies, concentrations, lengths, prices of financial instruments, and other metrics). the distribution is occasionally referred to as the galton distribution or galton's distribution, after francis galton. the log-normal distribution has also been associated with other names, such as mcalister, gibrat and cobb–douglas. a log-normal process is the statistical realization of the multiplicative product of many independent random variables, each of which is positive. this is justified by considering the central limit theorem in the log domain (sometimes called gibrat's law). the log-normal distribution is the maximum entropy probability distribution for a random variate x—for which the mean and variance of ln(x) are specified. let z {\displaystyle z} be a standard normal variable, and let μ {\displaystyle \mu } and σ >   {\displaystyle \sigma > } be two real numbers. then, the distribution of the random variable is called the log-normal distribution with parameters μ {\displaystyle \mu } and σ {\displaystyle \sigma } . these are the expected value (or mean) and standard deviation of the variable's natural logarithm, not the expectation and standard deviation of x {\displaystyle x} itself. this relationship is true regardless of the base of the logarithmic or exponential function: if log a ⁡ ( x ) {\displaystyle \log _{a}(x)} is normally distributed, then so is log b ⁡ ( x ) {\displaystyle \log _{b}(x)} for any two positive numbers a , b ≠   {\displaystyle a,b\neq  } . likewise, if e y {\displaystyle e^{y}} is log-normally distributed, then so is a y {\displaystyle a^{y}} , where   < a ≠   {\displaystyle  <a\neq  } . in order to produce a distribution with desired mean μ x {\displaystyle \mu _{x}} and variance σ x   {\displaystyle \sigma _{x}^{ }} , one uses μ = ln ⁡ ( μ x   μ x   + σ x   ) {\displaystyle \mu =\ln \left({\frac {\mu _{x}^{ }}{\sqrt {\mu _{x}^{ }+\sigma _{x}^{ }}}}\right)} and σ   = ln ⁡ (   + σ x   μ x   ) . {\displaystyle \sigma ^{ }=\ln \left( +{\frac {\sigma _{x}^{ }}{\mu _{x}^{ }}}\right).} alternatively, the ""multiplicative"" or ""geometric"" parameters μ ∗ = e μ {\displaystyle \mu ^{*}=e^{\mu }} and σ ∗ = e σ {\displaystyle \sigma ^{*}=e^{\sigma }} can be used. they have a more direct interpretation: μ ∗ {\displaystyle \mu ^{*}} is the median of the distribution, and σ ∗ {\displaystyle \sigma ^{*}} is useful for determining ""scatter"" intervals, see below. a positive random variable x is log-normally distributed (i.e., x ∼ lognormal ⁡ ( μ x , σ x   ) {\displaystyle x\sim \operatorname {lognormal} (\mu _{x},\sigma _{x}^{ })} ), if the natural logarithm of x is normally distributed with mean μ {\displaystyle \mu } and variance σ   {\displaystyle \sigma ^{ }} : let φ {\displaystyle \phi } and φ {\displaystyle \varphi } be respectively the cumulative probability distribution function and the probability density function of the n( , ) distribution, then we have that "
269,269,Log sum inequality,0,https://en.wikipedia.org/wiki/Log_sum_inequality,"the log sum inequality is used for proving theorems in information theory. let a   , … , a n {\displaystyle a_{ },\ldots ,a_{n}} and b   , … , b n {\displaystyle b_{ },\ldots ,b_{n}} be nonnegative numbers. denote the sum of all a i {\displaystyle a_{i}} s by a {\displaystyle a} and the sum of all b i {\displaystyle b_{i}} s by b {\displaystyle b} . the log sum inequality states that with equality if and only if a i b i {\displaystyle {\frac {a_{i}}{b_{i}}}} are equal for all i {\displaystyle i} , in other words a i = c b i {\displaystyle a_{i}=cb_{i}} for all i {\displaystyle i} . (take a i log ⁡ a i b i {\displaystyle a_{i}\log {\frac {a_{i}}{b_{i}}}} to be   {\displaystyle  } if a i =   {\displaystyle a_{i}= } and ∞ {\displaystyle \infty } if a i >   , b i =   {\displaystyle a_{i}> ,b_{i}= } . these are the limiting values obtained as the relevant number tends to   {\displaystyle  } .) notice that after setting f ( x ) = x log ⁡ x {\displaystyle f(x)=x\log x} we have where the inequality follows from jensen's inequality since b i b ≥   {\displaystyle {\frac {b_{i}}{b}}\geq  } , ∑ i =   n b i b =   {\displaystyle \sum _{i= }^{n}{\frac {b_{i}}{b}}= } , and f {\displaystyle f} is convex. the inequality remains valid for n = ∞ {\displaystyle n=\infty } provided that a < ∞ {\displaystyle a<\infty } and b < ∞ {\displaystyle b<\infty } .[citation needed] the proof above holds for any function g {\displaystyle g} such that f ( x ) = x g ( x ) {\displaystyle f(x)=xg(x)} is convex, such as all continuous non-decreasing functions. generalizations to non-decreasing functions other than the logarithm is given in csiszár,     . the log sum inequality can be used to prove inequalities in information theory. gibbs' inequality states that the kullback-leibler divergence is non-negative, and equal to zero precisely if its arguments are equal. one proof uses the log sum inequality. with equality if and only if p i = q i {\displaystyle p_{i}=q_{i}} for all i (as both p {\displaystyle p} and q {\displaystyle q} sum to  ). the inequality can also prove convexity of kullback-leibler divergence. "
270,270,Logarithm and Their Properties,1,https://en.wikipedia.org/wiki/Logarithm," in mathematics, the logarithm is the inverse function to exponentiation. that means the logarithm of a given number x is the exponent to which another fixed number, the base b, must be raised, to produce that number x. in the simplest case, the logarithm counts the number of occurrences of the same factor in repeated multiplication; e.g. since      =    ×    ×    =    , the ""logarithm base   "" of      is  , or log   (    ) =  . the logarithm of x to base b is denoted as logb (x), or without parentheses, logb x, or even without the explicit base, log x, when no confusion is possible, or when the base does not matter such as in big o notation. the logarithm base    (that is b =   ) is called the decimal or common logarithm and is commonly used in science and engineering. the natural logarithm has the number e (that is b ≈  .   ) as its base; its use is widespread in mathematics and physics, because of its simpler integral and derivative. the binary logarithm uses base   (that is b =  ) and is frequently used in computer science. logarithms were introduced by john napier in      as a means of simplifying calculations. they were rapidly adopted by navigators, scientists, engineers, surveyors and others to perform high-accuracy computations more easily. using logarithm tables, tedious multi-digit multiplication steps can be replaced by table look-ups and simpler addition. this is possible because of the fact—important in its own right—that the logarithm of a product is the sum of the logarithms of the factors: provided that b, x and y are all positive and b ≠  . the slide rule, also based on logarithms, allows quick calculations without tables, but at lower precision. the present-day notion of logarithms comes from leonhard euler, who connected them to the exponential function in the   th century, and who also introduced the letter e as the base of natural logarithms. logarithmic scales reduce wide-ranging quantities to smaller scopes. for example, the decibel (db) is a unit used to express ratio as logarithms, mostly for signal power and amplitude (of which sound pressure is a common example). in chemistry, ph is a logarithmic measure for the acidity of an aqueous solution. logarithms are commonplace in scientific formulae, and in measurements of the complexity of algorithms and of geometric objects called fractals. they help to describe frequency ratios of musical intervals, appear in formulas counting prime numbers or approximating factorials, inform some models in psychophysics, and can aid in forensic accounting. the concept of logarithm as the inverse of exponentiation extends to other mathematical structures as well. however, in general settings, the logarithm tends to be a multi-valued function. for example, the complex logarithm is the multi-valued inverse of the complex exponential function. similarly, the discrete logarithm is the multi-valued inverse of the exponential function in finite groups; it has uses in public-key cryptography. addition, multiplication, and exponentiation are three of the most fundamental arithmetic operations. the inverse of addition is subtraction, and the inverse of multiplication is division. similarly, a logarithm is the inverse operation of exponentiation. exponentiation is when a number b, the base is raised to a certain power y, the exponent for giving a value x; this denoted for example, raising   to the power of   gives  :     =   {\displaystyle  ^{ }= } the logarithm of base b is the inverse operation, that provides the output y from the input x. that is, y = log b ⁡ x {\displaystyle y=\log _{b}x} is equivalent to x = b y {\displaystyle x=b^{y}} if b is a positive real number. (if b is not a positive real number, both exponentiation and logarithm can be defined but may take several values, which makes definitions much more complicated.) "
271,271,Logarithm of a matrix,1,https://en.wikipedia.org/wiki/Logarithm_of_a_matrix," in mathematics, a logarithm of a matrix is another matrix such that the matrix exponential of the latter matrix equals the original matrix. it is thus a generalization of the scalar logarithm and in some sense an inverse function of the matrix exponential. not all matrices have a logarithm and those matrices that do have a logarithm may have more than one logarithm. the study of logarithms of matrices leads to lie theory since when a matrix has a logarithm then it is in an element of a lie group and the logarithm is the corresponding element of the vector space of the lie algebra. the exponential of a matrix a is defined by given a matrix b, another matrix a is said to be a matrix logarithm of b if ea = b. because the exponential function is not bijective for complex numbers (e.g. e π i = e   π i = −   {\displaystyle e^{\pi i}=e^{ \pi i}=- } ), numbers can have multiple complex logarithms, and as a consequence of this, some matrices may have more than one logarithm, as explained below. if b is sufficiently close to the identity matrix, then a logarithm of b may be computed by means of the following power series: specifically, if ‖ b − i ‖ <   {\displaystyle \left\|b-i\right\|< } , then the preceding series converges and e log ⁡ ( b ) = b {\displaystyle e^{\log(b)}=b} . the rotations in the plane give a simple example. a rotation of angle α around the origin is represented by the  × -matrix for any integer n, the matrix is a logarithm of a. l o g ( a ) = b n {\displaystyle log(a)=b_{n}~} ⇔ e b n = a {\displaystyle ~~e^{b_{n}}=a} "
272,272,Logarithmic Derivative,1,https://en.wikipedia.org/wiki/Logarithmic_derivative,"in mathematics, specifically in calculus and complex analysis, the logarithmic derivative of a function f is defined by the formula when f is a function f(x) of a real variable x, and takes real, strictly positive values, this is equal to the derivative of ln(f), or the natural logarithm of f. this follows directly from the chain rule: many properties of the real logarithm also apply to the logarithmic derivative, even when the function does not take values in the positive reals. for example, since the logarithm of a product is the sum of the logarithms of the factors, we have a corollary to this is that the logarithmic derivative of the reciprocal of a function is the negation of the logarithmic derivative of the function: more generally, the logarithmic derivative of a quotient is the difference of the logarithmic derivatives of the dividend and the divisor: generalising in another direction, the logarithmic derivative of a power (with constant real exponent) is the product of the exponent and the logarithmic derivative of the base: in summary, both derivatives and logarithms have a product rule, a reciprocal rule, a quotient rule, and a power rule (compare the list of logarithmic identities); each pair of rules is related through the logarithmic derivative. [verification needed] logarithmic derivatives can simplify the computation of derivatives requiring the product rule while producing the same result. the procedure is as follows: suppose that f(x) = u(x)v(x) and that we wish to compute f′(x). instead of computing it directly as f′ = u' v + v' u, we compute its logarithmic derivative. that is, we compute: multiplying through by ƒ computes f′: this technique is most useful when ƒ is a product of a large number of factors. this technique makes it possible to compute f′ by computing the logarithmic derivative of each factor, summing, and multiplying by f. [verification needed] "
273,273,Logarithmic Differentiation,1,https://en.wikipedia.org/wiki/Logarithmic_differentiation,"in calculus, logarithmic differentiation or differentiation by taking logarithms is a method used to differentiate functions by employing the logarithmic derivative of a function f, the technique is often performed in cases where it is easier to differentiate the logarithm of a function rather than the function itself. this usually occurs in cases where the function of interest is composed of a product of a number of parts, so that a logarithmic transformation will turn it into a sum of separate parts (which is much easier to differentiate). it can also be useful when applied to functions raised to the power of variables or functions. logarithmic differentiation relies on the chain rule as well as properties of logarithms (in particular, the natural logarithm, or the logarithm to the base e) to transform products into sums and divisions into subtractions. the principle can be implemented, at least in part, in the differentiation of almost all differentiable functions, providing that these functions are non-zero. for a function logarithmic differentiation typically begins by taking the natural logarithm, or the logarithm to the base e, on both sides, remembering to take absolute values: after implicit differentiation: multiplication by y is then done to eliminate  /y and leave only dy/dx on the left-hand side: the method is used because the properties of logarithms provide avenues to quickly simplify complicated functions to be differentiated. these properties can be manipulated after the taking of natural logarithms on both sides and before the preliminary differentiation. the most commonly used logarithm laws are using capital pi notation, application of natural logarithms results in (with capital sigma notation) and after differentiation, "
274,274,logical operation,0,https://en.wikipedia.org/wiki/Logical_connective,"in logic, a logical connective (also called a logical operator, sentential connective, or sentential operator) is a logical constant. they can be used to connect logical formulas. for instance in the syntax of propositional logic, the binary connective ∨ {\displaystyle \lor } can be used to join the two atomic formulas p {\displaystyle p} and q {\displaystyle q} , rendering the complex formula p ∨ q {\displaystyle p\lor q} . common connectives include negation, disjunction, conjunction, and implication. in standard systems of classical logic, these connectives are interpreted as truth functions, though they receive a variety of alternative interpretations in nonclassical logics. their classical interpretations are similar to the meanings of natural language expressions such as english ""not"", ""or"", ""and"", and ""if"", but not identical. discrepancies between natural language connectives and those of classical logic have motivated nonclassical approaches to natural language meaning as well as approaches which pair a classical compositional semantics with a robust pragmatics. a logical connective is similar to, but not equivalent to, a syntax commonly used in programming languages called a conditional operator. [better source needed] in formal languages, truth functions are represented by unambiguous symbols. this allows logical statements to not be understood in an ambiguous way. these symbols are called logical connectives, logical operators, propositional operators, or, in classical logic, truth-functional connectives. for the rules which allow new well-formed formulas to be constructed by joining other well-formed formulas using truth-functional connectives, see well-formed formula. logical connectives can be used to link zero or more statements, so one can speak about n-ary logical connectives. the boolean constants true and false can be thought of as zero-ary operators. negation is a  -ary connective, and so on. commonly used logical connectives include: alternative names for biconditional are iff, xnor, and bi-implication. for example, the meaning of the statements it is raining (denoted by p) and i am indoors (denoted by q) is transformed, when the two are combined with logical connectives: it is also common to consider the always true formula and the always false formula to be connective: some authors used letters for connectives at some time of the history: u. for conjunction (german's ""und"" for ""and"") and o. for disjunction (german's ""oder"" for ""or"") in earlier works by hilbert (    ); np for negation, kpq for conjunction, dpq for alternative denial, apq for disjunction, xpq for joint denial, cpq for implication, epq for biconditional in łukasiewicz (    ); cf. polish notation. "
275,275,Logical reasoning,3,https://en.wikipedia.org/wiki/Logical_reasoning,"two kinds of logical reasoning are often distinguished in addition to formal deduction: induction and abduction. given a precondition or premise, a conclusion or logical consequence and a rule or material conditional that implies the conclusion given the precondition, one can explain the following. within the context of a mathematical model, these three kinds of reasoning can be described as follows. the construction/creation of the structure of the model is abduction. assigning values (or probability distributions) to the parameters of the model is induction. executing/running the model is deduction. other kinds of reasoning beside the three common categories above are: see non-demonstrative reasoning for a comparison of these other kinds of reasoning. "
276,276,Logistic Distribution,2,https://en.wikipedia.org/wiki/Logistic_distribution,"in probability theory and statistics, the logistic distribution is a continuous probability distribution. its cumulative distribution function is the logistic function, which appears in logistic regression and feedforward neural networks. it resembles the normal distribution in shape but has heavier tails (higher kurtosis). the logistic distribution is a special case of the tukey lambda distribution. when the location parameter μ is   and the scale parameter s is  , then the probability density function of the logistic distribution is given by thus in general the density is: because this function can be expressed in terms of the square of the hyperbolic secant function ""sech"", it is sometimes referred to as the sech-square(d) distribution. (see also: hyperbolic secant distribution). the logistic distribution receives its name from its cumulative distribution function, which is an instance of the family of logistic functions. the cumulative distribution function of the logistic distribution is also a scaled version of the hyperbolic tangent. in this equation, x is the random variable, μ is the mean, and s is a scale parameter proportional to the standard deviation. the inverse cumulative distribution function (quantile function) of the logistic distribution is a generalization of the logit function. its derivative is called the quantile density function. they are defined as follows: an alternative parameterization of the logistic distribution can be derived by expressing the scale parameter, s {\displaystyle s} , in terms of the standard deviation, σ {\displaystyle \sigma } , using the substitution s = q σ {\displaystyle s\,=\,q\,\sigma } , where q =   / π =  .          … {\displaystyle q\,=\,{\sqrt { }}/{\pi }\,=\, .         \ldots } . the alternative forms of the above functions are reasonably straightforward. the logistic distribution—and the s-shaped pattern of its cumulative distribution function (the logistic function) and quantile function (the logit function)—have been extensively used in many different areas. one of the most common applications is in logistic regression, which is used for modeling categorical dependent variables (e.g., yes-no choices or a choice of   or   possibilities), much as standard linear regression is used for modeling continuous variables (e.g., income or population). specifically, logistic regression models can be phrased as latent variable models with error variables following a logistic distribution. this phrasing is common in the theory of discrete choice models, where the logistic distribution plays the same role in logistic regression as the normal distribution does in probit regression. indeed, the logistic and normal distributions have a quite similar shape. however, the logistic distribution has heavier tails, which often increases the robustness of analyses based on it compared with using the normal distribution. "
277,277,Markov's Inequality,2,https://en.wikipedia.org/wiki/Markov%27s_inequality,"in probability theory, markov's inequality gives an upper bound for the probability that a non-negative function of a random variable is greater than or equal to some positive constant. it is named after the russian mathematician andrey markov, although it appeared earlier in the work of pafnuty chebyshev (markov's teacher), and many sources, especially in analysis, refer to it as chebyshev's inequality (sometimes, calling it the first chebyshev inequality, while referring to chebyshev's inequality as the second chebyshev inequality) or bienaymé's inequality. markov's inequality (and other similar inequalities) relate probabilities to expectations, and provide (frequently loose but still useful) bounds for the cumulative distribution function of a random variable. if x is a nonnegative random variable and a >  , then the probability that x is at least a is at most the expectation of x divided by a: let a = a ~ ⋅ e ⁡ ( x ) {\displaystyle a={\tilde {a}}\cdot \operatorname {e} (x)} (where a ~ >   {\displaystyle {\tilde {a}}> } ); then we can rewrite the previous inequality as in the language of measure theory, markov's inequality states that if (x, σ, μ) is a measure space, f {\displaystyle f} is a measurable extended real-valued function, and ε >  , then this measure-theoretic definition is sometimes referred to as chebyshev's inequality. if φ is a monotonically increasing nonnegative function for the nonnegative reals, x is a random variable, a ≥  , and φ(a) >  , then an immediate corollary, using higher moments of x supported on values larger than  , is we separate the case in which the measure space is a probability space from the more general case because the probability case is more accessible for the general reader. e ⁡ ( x ) = p ⁡ ( x < a ) ⋅ e ⁡ ( x | x < a ) + p ⁡ ( x ≥ a ) ⋅ e ⁡ ( x | x ≥ a ) {\displaystyle \operatorname {e} (x)=\operatorname {p} (x<a)\cdot \operatorname {e} (x|x<a)+\operatorname {p} (x\geq a)\cdot \operatorname {e} (x|x\geq a)} where e ⁡ ( x | x < a ) {\displaystyle \operatorname {e} (x|x<a)} is larger than   as r.v. x {\displaystyle x} is non-negative and e ⁡ ( x | x ≥ a ) {\displaystyle \operatorname {e} (x|x\geq a)} is larger than a {\displaystyle a} because the conditional expectation only takes into account of values larger than a {\displaystyle a} which r.v. x {\displaystyle x} can take. "
278,278,Markov chain,0,https://en.wikipedia.org/wiki/Markov_chain,"a markov chain or markov process is a stochastic model describing a sequence of possible events in which the probability of each event depends only on the state attained in the previous event. a countably infinite sequence, in which the chain moves state at discrete time steps, gives a discrete-time markov chain (dtmc). a continuous-time process is called a continuous-time markov chain (ctmc). it is named after the russian mathematician andrey markov. markov chains have many applications as statistical models of real-world processes, such as studying cruise control systems in motor vehicles, queues or lines of customers arriving at an airport, currency exchange rates and animal population dynamics. markov processes are the basis for general stochastic simulation methods known as markov chain monte carlo, which are used for simulating sampling from complex probability distributions, and have found application in bayesian statistics, thermodynamics, statistical mechanics, physics, chemistry, economics, finance, signal processing, information theory and speech processing. the adjectives markovian and markov are used to describe something that is related to a markov process. a markov process is a stochastic process that satisfies the markov property (sometimes characterized as ""memorylessness""). in simpler terms, it is a process for which predictions can be made regarding future outcomes based solely on its present state and—most importantly—such predictions are just as good as the ones that could be made knowing the process's full history. in other words, conditional on the present state of the system, its future and past states are independent. a markov chain is a type of markov process that has either a discrete state space or a discrete index set (often representing time), but the precise definition of a markov chain varies. for example, it is common to define a markov chain as a markov process in either discrete or continuous time with a countable state space (thus regardless of the nature of time), but it is also common to define a markov chain as having discrete time in either countable or continuous state space (thus regardless of the state space). the system's state space and time parameter index need to be specified. the following table gives an overview of the different instances of markov processes for different levels of state space generality and for discrete time v. continuous time: note that there is no definitive agreement in the literature on the use of some of the terms that signify special cases of markov processes. usually the term ""markov chain"" is reserved for a process with a discrete set of times, that is, a discrete-time markov chain (dtmc), but a few authors use the term ""markov process"" to refer to a continuous-time markov chain (ctmc) without explicit mention. in addition, there are other extensions of markov processes that are referred to as such but do not necessarily fall within any of these four categories (see markov model). moreover, the time index need not necessarily be real-valued; like with the state space, there are conceivable processes that move through index sets with other mathematical constructs. notice that the general state space continuous-time markov chain is general to such a degree that it has no designated term. while the time parameter is usually discrete, the state space of a markov chain does not have any generally agreed-on restrictions: the term may refer to a process on an arbitrary state space. however, many applications of markov chains employ finite or countably infinite state spaces, which have a more straightforward statistical analysis. besides time-index and state-space parameters, there are many other variations, extensions and generalizations (see variations). for simplicity, most of this article concentrates on the discrete-time, discrete state-space case, unless mentioned otherwise. the changes of state of the system are called transitions. the probabilities associated with various state changes are called transition probabilities. the process is characterized by a state space, a transition matrix describing the probabilities of particular transitions, and an initial state (or initial distribution) across the state space. by convention, we assume all possible states and transitions have been included in the definition of the process, so there is always a next state, and the process does not terminate. "
279,279,Mathematical logic,0,https://en.wikipedia.org/wiki/Mathematical_logic," mathematical logic is the study of formal logic within mathematics. major subareas include model theory, proof theory, set theory, and recursion theory. research in mathematical logic commonly addresses the mathematical properties of formal systems of logic such as their expressive or deductive power. however, it can also include uses of logic to characterize correct mathematical reasoning or to establish foundations of mathematics. since its inception, mathematical logic has both contributed to, and has been motivated by, the study of foundations of mathematics. this study began in the late   th century with the development of axiomatic frameworks for geometry, arithmetic, and analysis. in the early   th century it was shaped by david hilbert's program to prove the consistency of foundational theories. results of kurt gödel, gerhard gentzen, and others provided partial resolution to the program, and clarified the issues involved in proving consistency. work in set theory showed that almost all ordinary mathematics can be formalized in terms of sets, although there are some theorems that cannot be proven in common axiom systems for set theory. contemporary work in the foundations of mathematics often focuses on establishing which parts of mathematics can be formalized in particular formal systems (as in reverse mathematics) rather than trying to find theories in which all of mathematics can be developed. the handbook of mathematical logic in      makes a rough division of contemporary mathematical logic into four areas: additionally, sometimes the field of computational complexity theory is also included as part of mathematical logic. each area has a distinct focus, although many techniques and results are shared among multiple areas. the borderlines amongst these fields, and the lines separating mathematical logic and other fields of mathematics, are not always sharp. gödel's incompleteness theorem marks not only a milestone in recursion theory and proof theory, but has also led to löb's theorem in modal logic. the method of forcing is employed in set theory, model theory, and recursion theory, as well as in the study of intuitionistic mathematics. the mathematical field of category theory uses many formal axiomatic methods, and includes the study of categorical logic, but category theory is not ordinarily considered a subfield of mathematical logic. because of its applicability in diverse fields of mathematics, mathematicians including saunders mac lane have proposed category theory as a foundational system for mathematics, independent of set theory. these foundations use toposes, which resemble generalized models of set theory that may employ classical or nonclassical logic. mathematical logic emerged in the mid-  th century as a subfield of mathematics, reflecting the confluence of two traditions: formal philosophical logic and mathematics. ""mathematical logic, also called 'logistic', 'symbolic logic', the 'algebra of logic', and, more recently, simply 'formal logic', is the set of logical theories elaborated in the course of the last [nineteenth] century with the aid of an artificial notation and a rigorously deductive method."" before this emergence, logic was studied with rhetoric, with calculationes, through the syllogism, and with philosophy. the first half of the   th century saw an explosion of fundamental results, accompanied by vigorous debate over the foundations of mathematics. theories of logic were developed in many cultures in history, including china, india, greece and the islamic world. greek methods, particularly aristotelian logic (or term logic) as found in the organon, found wide application and acceptance in western science and mathematics for millennia. the stoics, especially chrysippus, began the development of predicate logic. in   th-century europe, attempts to treat the operations of formal logic in a symbolic or algebraic way had been made by philosophical mathematicians including leibniz and lambert, but their labors remained isolated and little known. in the middle of the nineteenth century, george boole and then augustus de morgan presented systematic mathematical treatments of logic. their work, building on work by algebraists such as george peacock, extended the traditional aristotelian doctrine of logic into a sufficient framework for the study of foundations of mathematics. charles sanders peirce later built upon the work of boole to develop a logical system for relations and quantifiers, which he published in several papers from      to     . gottlob frege presented an independent development of logic with quantifiers in his begriffsschrift, published in     , a work generally considered as marking a turning point in the history of logic. frege's work remained obscure, however, until bertrand russell began to promote it near the turn of the century. the two-dimensional notation frege developed was never widely adopted and is unused in contemporary texts. "
280,280,Mathematical proof,3,https://en.wikipedia.org/wiki/Mathematical_proof," a mathematical proof is an inferential argument for a mathematical statement, showing that the stated assumptions logically guarantee the conclusion. the argument may use other previously established statements, such as theorems; but every proof can, in principle, be constructed using only certain basic or original assumptions known as axioms, along with the accepted rules of inference. proofs are examples of exhaustive deductive reasoning which establish logical certainty, to be distinguished from empirical arguments or non-exhaustive inductive reasoning which establish ""reasonable expectation"". presenting many cases in which the statement holds is not enough for a proof, which must demonstrate that the statement is true in all possible cases. a proposition that has not been proved but is believed to be true is known as a conjecture, or a hypothesis if frequently used as an assumption for further mathematical work. proofs employ logic expressed in mathematical symbols, along with natural language which usually admits some ambiguity. in most mathematical literature, proofs are written in terms of rigorous informal logic. purely formal proofs, written fully in symbolic language without the involvement of natural language, are considered in proof theory. the distinction between formal and informal proofs has led to much examination of current and historical mathematical practice, quasi-empiricism in mathematics, and so-called folk mathematics, oral traditions in the mainstream mathematical community or in other cultures. the philosophy of mathematics is concerned with the role of language and logic in proofs, and mathematics as a language. the word ""proof"" comes from the latin probare (to test). related modern words are english ""probe"", ""probation"", and ""probability"", spanish probar (to smell or taste, or sometimes touch or test), italian provare (to try), and german probieren (to try). the legal term ""probity"" means authority or credibility, the power of testimony to prove facts when given by persons of reputation or status. plausibility arguments using heuristic devices such as pictures and analogies preceded strict mathematical proof. it is likely that the idea of demonstrating a conclusion first arose in connection with geometry, which originated in practical problems of land measurement. the development of mathematical proof is primarily the product of ancient greek mathematics, and one of its greatest achievements. thales (   –    bce) and hippocrates of chios (c.    –    bce) gave some of the first known proofs of theorems in geometry. eudoxus (   –    bce) and theaetetus (   –    bce) formulated theorems but did not prove them. aristotle (   –    bce) said definitions should describe the concept being defined in terms of other concepts already known. mathematical proof was revolutionized by euclid (    bce), who introduced the axiomatic method still in use today. it starts with undefined terms and axioms, propositions concerning the undefined terms which are assumed to be self-evidently true (from greek ""axios"", something worthy). from this basis, the method proves theorems using deductive logic. euclid's book, the elements, was read by anyone who was considered educated in the west until the middle of the   th century. in addition to theorems of geometry, such as the pythagorean theorem, the elements also covers number theory, including a proof that the square root of two is irrational and a proof that there are infinitely many prime numbers. further advances also took place in medieval islamic mathematics. while earlier greek proofs were largely geometric demonstrations, the development of arithmetic and algebra by islamic mathematicians allowed more general proofs with no dependence on geometric intuition. in the   th century ce, the iraqi mathematician al-hashimi worked with numbers as such, called ""lines"" but not necessarily considered as measurements of geometric objects, to prove algebraic propositions concerning multiplication, division, etc., including the existence of irrational numbers. an inductive proof for arithmetic sequences was introduced in the al-fakhri (    ) by al-karaji, who used it to prove the binomial theorem and properties of pascal's triangle. alhazen also developed the method of proof by contradiction, as the first attempt at proving the euclidean parallel postulate. modern proof theory treats proofs as inductively defined data structures, not requiring an assumption that axioms are ""true"" in any sense. this allows parallel mathematical theories as formal models of a given intuitive concept, based on alternate sets of axioms, for example axiomatic set theory and non-euclidean geometry. as practiced, a proof is expressed in natural language and is a rigorous argument intended to convince the audience of the truth of a statement. the standard of rigor is not absolute and has varied throughout history. a proof can be presented differently depending on the intended audience. in order to gain acceptance, a proof has to meet communal standards of rigor; an argument considered vague or incomplete may be rejected. the concept of proof is formalized in the field of mathematical logic. a formal proof is written in a formal language instead of natural language. a formal proof is a sequence of formulas in a formal language, starting with an assumption, and with each subsequent formula a logical consequence of the preceding ones. this definition makes the concept of proof amenable to study. indeed, the field of proof theory studies formal proofs and their properties, the most famous and surprising being that almost all axiomatic systems can generate certain undecidable statements not provable within the system. "
281,281,Mathmatics,1,https://en.wikipedia.org/wiki/Mathematics," mathematics (from ancient greek μάθημα (máthēma) 'knowledge, study, learning') is an area of knowledge, which includes the study of such topics as numbers (arithmetic and number theory), formulas and related structures (algebra), shapes and spaces in which they are contained (geometry), and quantities and their changes (calculus and analysis). there is no general consensus about its exact scope or epistemological status. most of mathematical activity consists of discovering and proving (by pure reasoning) properties of abstract objects. these objects are either abstractions from nature (such as natural numbers or lines), or (in modern mathematics) abstract entities of which certain properties, called axioms, are stipulated. a proof consists of a succession of applications of some deductive rules to already known results, including previously proved theorems, axioms and (in case of abstraction from nature) some basic properties that are considered as true starting points of the theory under consideration. the result of a proof is called a theorem. mathematics is widely used in science for modeling phenomena. this enables the extraction of quantitative predictions from experimental laws. for example, the movement of planets can be predicted with high accuracy using newton's law of gravitation combined with mathematical computation. the independence of mathematical truth from any experimentation implies that the accuracy of such predictions depends only on the adequacy of the model for describing the reality. so when some inaccurate predictions arise, it means that the model must be improved or changed, not that the mathematics is wrong. for example, the perihelion precession of mercury cannot be explained by newton's law of gravitation, but is accurately explained by einstein's general relativity. this experimental validation of einstein's theory shows that newton's law of gravitation is only an approximation (which still is very accurate in everyday life). mathematics is essential in many fields, including natural sciences, engineering, medicine, finance, computer science and social sciences. some areas of mathematics, such as statistics and game theory, are developed in direct correlation with their applications, and are often grouped under the name of applied mathematics. other mathematical areas are developed independently from any application (and are therefore called pure mathematics), but practical applications are often discovered later. a fitting example is the problem of integer factorization, which goes back to euclid, but which had no practical application before its use in the rsa cryptosystem (for the security of computer networks). mathematics has been a human activity from as far back as written records exist. however, the concept of a ""proof"" and its associated ""mathematical rigour"" first appeared in greek mathematics, most notably in euclid's elements. mathematics developed at a relatively slow pace until the renaissance, when algebra and infinitesimal calculus were added to arithmetic and geometry as main areas of mathematics. since then the interaction between mathematical innovations and scientific discoveries have led to a rapid increase in the rate of mathematical discoveries. at the end of the   th century, the foundational crisis of mathematics led to the systematization of the axiomatic method. this, in turn, gave rise to a dramatic increase in the number of mathematics areas and their fields of applications; a witness of this is the mathematics subject classification, which lists more than sixty first-level areas of mathematics. before the renaissance, mathematics was divided into two main areas: arithmetic, devoted to the manipulation of numbers, and geometry, devoted to the study of shapes. there was also some pseudoscience, such as numerology and astrology, that were not clearly distinguished from mathematics. around the renaissance, two new main areas appeared. the introduction of mathematical notation led to algebra, which, roughly speaking, consists of the study and the manipulation of formulas. calculus, a shorthand of infinitesimal calculus and integral calculus, is the study of continuous functions, which model the change of, and the relationship between varying quantities (variables). this division into four main areas remained valid until the end of the   th century, although some areas, such as celestial mechanics and solid mechanics, which were often considered as mathematics, are now considered as belonging to physics. also, some subjects developed during this period predate mathematics (being divided into different) areas, such as probability theory and combinatorics, which only later became regarded as autonomous areas of their own. at the end of the   th century, the foundational crisis in mathematics and the resulting systematization of the axiomatic method led to an explosion in the amount of areas of mathematics. the mathematics subject classification contains more than    first-level areas. some of these areas correspond to the older division in four main areas. this is the case of number theory (the modern name for higher arithmetic) and geometry. however, there are several other first-level areas that have ""geometry"" in their name or are commonly considered as belonging to geometry. algebra and calculus do not appear as first-level areas, but are each split into several first-level areas. other first-level areas did not exist at all before the   th century (for example category theory; homological algebra, and computer science) or were not considered before as mathematics, such as   :mathematical logic and foundations (including model theory, computability theory, set theory, proof theory, and algebraic logic). number theory started with the manipulation of numbers, that is, natural numbers ( n ) , {\displaystyle (\mathbb {n} ),} and later expanded to integers ( z ) {\displaystyle (\mathbb {z} )} and rational numbers ( q ) . {\displaystyle (\mathbb {q} ).} number theory was formerly called arithmetic, but nowadays this term is mostly used for the methods of calculation with numbers. "
282,282,Matrix theory,2,https://en.wikipedia.org/wiki/Matrix_(mathematics),"in mathematics, a matrix (plural matrices) is a rectangular array or table of numbers, symbols, or expressions, arranged in rows and columns, which is used to represent a mathematical object or a property of such an object. for example, without further specifications, matrices represent linear maps, and allow explicit computations in linear algebra. therefore, the study of matrices is a large part of linear algebra, and most properties and operations of abstract linear algebra can be expressed in terms of matrices. for example, matrix multiplication represents composition of linear maps. not all matrices are related to linear algebra. this is, in particular, the case in graph theory, of incidence matrices, and adjacency matrices. this article focuses on matrices related to linear algebra, and, unless otherwise specified, all matrices represent linear maps or may be viewed as such. square matrices, matrices with the same number of rows and columns, play a major role in matrix theory. square matrices of a given dimension form a noncommutative ring, which is one of the most common examples of a noncommutative ring. the determinant of a square matrix is a number associated to the matrix, which is fundamental for the study of a square matrix; for example, a square matrix is invertible if and only if it has a nonzero determinant, and the eigenvalues of a square matrix are the roots of a polynomial determinant. in geometry, matrices are widely used for specifying and representing geometric transformations (for example rotations) and coordinate changes. in numerical analysis, many computational problems are solved by reducing them to a matrix computation, and this involves often to compute with matrices of huge dimension. matrices are used in most areas of mathematics and most scientific fields, either directly, or through their use in geometry and numerical analysis. a matrix is a rectangular array of numbers (or other mathematical objects), called the entries of the matrix. matrices are subject to standard operations such as addition and multiplication. most commonly, a matrix over a field f is a rectangular array of elements of f. a real matrix and a complex matrix are matrices whose entries are respectively real numbers or complex numbers. more general types of entries are discussed below. for instance, this is a real matrix: the numbers, symbols, or expressions in the matrix are called its entries or its elements. the horizontal and vertical lines of entries in a matrix are called rows and columns, respectively. the size of a matrix is defined by the number of rows and columns it contains. there is no limit to the numbers of rows and columns a matrix (in the usual sense) can have as long as they are positive integers. a matrix with m rows and n columns is called an m × n matrix, or m-by-n matrix, while m and n are called its dimensions. for example, the matrix a above is a   ×   matrix. matrices with a single row are called row vectors, and those with a single column are called column vectors. a matrix with the same number of rows and columns is called a square matrix. a matrix with an infinite number of rows or columns (or both) is called an infinite matrix. in some contexts, such as computer algebra programs, it is useful to consider a matrix with no rows or no columns, called an empty matrix. "
283,283,Matrix Determinant Lemma,2,https://en.wikipedia.org/wiki/Matrix_determinant_lemma,"in mathematics, in particular linear algebra, the matrix determinant lemma computes the determinant of the sum of an invertible matrix a and the dyadic product, u vt, of a column vector u and a row vector vt. suppose a is an invertible square matrix and u, v are column vectors. then the matrix determinant lemma states that here, uvt is the outer product of two vectors u and v. the theorem can also be stated in terms of the adjugate matrix of a: in which case it applies whether or not the square matrix a is invertible. first the proof of the special case a = i follows from the equality: the determinant of the left hand side is the product of the determinants of the three matrices. since the first and third matrix are triangular matrices with unit diagonal, their determinants are just  . the determinant of the middle matrix is our desired value. the determinant of the right hand side is simply (  + vtu). so we have the result: then the general case can be found as: if the determinant and inverse of a are already known, the formula provides a numerically cheap way to compute the determinant of a corrected by the matrix uvt. the computation is relatively cheap because the determinant of a + uvt does not have to be computed from scratch (which in general is expensive). using unit vectors for u and/or v, individual columns, rows or elements of a may be manipulated and a correspondingly updated determinant computed relatively cheaply in this way. when the matrix determinant lemma is used in conjunction with the sherman–morrison formula, both the inverse and determinant may be conveniently updated together. "
284,284,equality of matrices,0,https://en.wikipedia.org/wiki/Matrix_equivalence,"in linear algebra, two rectangular m-by-n matrices a and b are called equivalent if for some invertible n-by-n matrix p and some invertible m-by-m matrix q. equivalent matrices represent the same linear transformation v → w under two different choices of a pair of bases of v and w, with p and q being the change of basis matrices in v and w respectively. the notion of equivalence should not be confused with that of similarity, which is only defined for square matrices, and is much more restrictive (similar matrices are certainly equivalent, but equivalent square matrices need not be similar). that notion corresponds to matrices representing the same endomorphism v → v under two different choices of a single basis of v, used both for initial vectors and their images. matrix equivalence is an equivalence relation on the space of rectangular matrices. for two rectangular matrices of the same size, their equivalence can also be characterized by the following conditions the rank property yields an intuitive canonical form for matrices of the equivalence class of rank k {\displaystyle k} as (       ⋯         ⋯       ⋱   ⋮   ⋮   ⋱   ⋯   ) {\displaystyle {\begin{pmatrix} & & &&\cdots && \\ & & &&\cdots && \\ & &\ddots &&&& \\\vdots &&& &&&\vdots \\&&&& &&\\&&&&&\ddots &\\ &&&\cdots &&& \end{pmatrix}}} , where the number of   {\displaystyle  } s on the diagonal is equal to k {\displaystyle k} . this is a special case of the smith normal form, which generalizes this concept on vector spaces to free modules over principal ideal domains. "
285,285,multiplication by a scalar and product of matrices,1,https://en.wikipedia.org/wiki/Matrix_multiplication,"in mathematics, particularly in linear algebra, matrix multiplication is a binary operation that produces a matrix from two matrices. for matrix multiplication, the number of columns in the first matrix must be equal to the number of rows in the second matrix. the resulting matrix, known as the matrix product, has the number of rows of the first and the number of columns of the second matrix. the product of matrices a and b is denoted as ab. matrix multiplication was first described by the french mathematician jacques philippe marie binet in     , to represent the composition of linear maps that are represented by matrices. matrix multiplication is thus a basic tool of linear algebra, and as such has numerous applications in many areas of mathematics, as well as in applied mathematics, statistics, physics, economics, and engineering. computing matrix products is a central operation in all computational applications of linear algebra. this article will use the following notational conventions: matrices are represented by capital letters in bold, e.g. a; vectors in lowercase bold, e.g. a; and entries of vectors and matrices are italic (they are numbers from a field), e.g. a and a. index notation is often the clearest way to express definitions, and is used as standard in the literature. the entry in row i, column j of matrix a is indicated by (a)ij, aij or aij. in contrast, a single subscript, e.g. a , a , is used to select a matrix (not a matrix entry) from a collection of matrices. if a is an m × n matrix and b is an n × p matrix, the matrix product c = ab (denoted without multiplication signs or dots) is defined to be the m × p matrix such that for i =  , ..., m and j =  , ..., p. that is, the entry c i j {\displaystyle c_{ij}} of the product is obtained by multiplying term-by-term the entries of the ith row of a and the jth column of b, and summing these n products. in other words, c i j {\displaystyle c_{ij}} is the dot product of the ith row of a and the jth column of b. therefore, ab can also be written as thus the product ab is defined if and only if the number of columns in a equals the number of rows in b, in this case n. many scientific calculator manuals do not mention this condition correctly.[citation needed] "
286,286,Maxima and minima,1,https://en.wikipedia.org/wiki/Maxima_and_minima,"in mathematical analysis, the maxima and minima (the respective plurals of maximum and minimum) of a function, known collectively as extrema (the plural of extremum), are the largest and smallest value of the function, either within a given range (the local or relative extrema), or on the entire domain (the global or absolute extrema). pierre de fermat was one of the first mathematicians to propose a general technique, adequality, for finding the maxima and minima of functions. as defined in set theory, the maximum and minimum of a set are the greatest and least elements in the set, respectively. unbounded infinite sets, such as the set of real numbers, have no minimum or maximum. a real-valued function f defined on a domain x has a global (or absolute) maximum point at x∗, if f(x∗) ≥ f(x) for all x in x. similarly, the function has a global (or absolute) minimum point at x∗, if f(x∗) ≤ f(x) for all x in x. the value of the function at a maximum point is called the maximum value of the function, denoted max ( f ( x ) ) {\displaystyle \max(f(x))} , and the value of the function at a minimum point is called the minimum value of the function. symbolically, this can be written as follows: the definition of global minimum point also proceeds similarly. if the domain x is a metric space, then f is said to have a local (or relative) maximum point at the point x∗, if there exists some ε >   such that f(x∗) ≥ f(x) for all x in x within distance ε of x∗. similarly, the function has a local minimum point at x∗, if f(x∗) ≤ f(x) for all x in x within distance ε of x∗. a similar definition can be used when x is a topological space, since the definition just given can be rephrased in terms of neighbourhoods. mathematically, the given definition is written as follows: the definition of local minimum point can also proceed similarly. in both the global and local cases, the concept of a strict extremum can be defined. for example, x∗ is a strict global maximum point if for all x in x with x ≠ x∗, we have f(x∗) > f(x), and x∗ is a strict local maximum point if there exists some ε >   such that, for all x in x within distance ε of x∗ with x ≠ x∗, we have f(x∗) > f(x). note that a point is a strict global maximum point if and only if it is the unique global maximum point, and similarly for minimum points. a continuous real-valued function with a compact domain always has a maximum point and a minimum point. an important example is a function whose domain is a closed and bounded interval of real numbers (see the graph above). finding global maxima and minima is the goal of mathematical optimization. if a function is continuous on a closed interval, then by the extreme value theorem, global maxima and minima exist. furthermore, a global maximum (or minimum) either must be a local maximum (or minimum) in the interior of the domain, or must lie on the boundary of the domain. so a method of finding a global maximum (or minimum) is to look at all the local maxima (or minima) in the interior, and also look at the maxima (or minima) of the points on the boundary, and take the largest (or smallest) one. for differentiable functions, fermat's theorem states that local extrema in the interior of a domain must occur at critical points (or points where the derivative equals zero). however, not all critical points are extrema. one can distinguish whether a critical point is a local maximum or local minimum by using the first derivative test, second derivative test, or higher-order derivative test, given sufficient differentiability. "
287,287,Maximum Likelihood Estimation,0,https://en.wikipedia.org/wiki/Maximum_likelihood_estimation,"in statistics, maximum likelihood estimation (mle) is a method of estimating the parameters of an assumed probability distribution, given some observed data. this is achieved by maximizing a likelihood function so that, under the assumed statistical model, the observed data is most probable. the point in the parameter space that maximizes the likelihood function is called the maximum likelihood estimate. the logic of maximum likelihood is both intuitive and flexible, and as such the method has become a dominant means of statistical inference. if the likelihood function is differentiable, the derivative test for determining maxima can be applied. in some cases, the first-order conditions of the likelihood function can be solved explicitly; for instance, the ordinary least squares estimator maximizes the likelihood of the linear regression model. under most circumstances, however, numerical methods will be necessary to find the maximum of the likelihood function. the likelihood function can also be non-convex with multiple local minima requiring the use of heuristic global optimization techniques. from the vantage point of bayesian inference, mle is a special case of maximum a posteriori estimation (map) that assumes a uniform prior distribution of the parameters. in frequentist inference, mle is a special case of an extremum estimator, with the objective function being the likelihood. from a statistical standpoint, a given set of observations is a random sample from an unknown population. the goal of maximum likelihood estimation is to make inferences about the population that is most likely to have generated the sample, specifically the joint probability distribution of the random variables { y   , y   , … } {\displaystyle \left\{y_{ },y_{ },\ldots \right\}} , not necessarily independent and identically distributed. associated with each probability distribution is a unique vector θ = [ θ   , θ   , … , θ k ] t {\displaystyle \theta =\left[\theta _{ },\,\theta _{ },\,\ldots ,\,\theta _{k}\right]^{\mathsf {t}}} of parameters that index the probability distribution within a parametric family { f ( ⋅ ; θ ) ∣ θ ∈ θ } {\displaystyle \{f(\cdot \,;\theta )\mid \theta \in \theta \}} , where θ {\displaystyle \theta } is called the parameter space, a finite-dimensional subset of euclidean space. evaluating the joint density at the observed data sample y = ( y   , y   , … , y n ) {\displaystyle \mathbf {y} =(y_{ },y_{ },\ldots ,y_{n})} gives a real-valued function, which is called the likelihood function. for independent and identically distributed random variables, f n ( y ; θ ) {\displaystyle f_{n}(\mathbf {y} ;\theta )} will be the product of univariate density functions. the goal of maximum likelihood estimation is to find the values of the model parameters that maximize the likelihood function over the parameter space, that is intuitively, this selects the parameter values that make the observed data most probable. the specific value θ ^ = θ ^ n ( y ) ∈ θ {\displaystyle {\hat {\theta }}={\hat {\theta }}_{n}(\mathbf {y} )\in \theta } that maximizes the likelihood function l n {\displaystyle l_{n}} is called the maximum likelihood estimate. further, if the function θ ^ n : r n → θ {\displaystyle {\hat {\theta }}_{n}:\mathbb {r} ^{n}\to \theta } so defined is measurable, then it is called the maximum likelihood estimator. it is generally a function defined over the sample space, i.e. taking a given sample as its argument. a sufficient but not necessary condition for its existence is for the likelihood function to be continuous over a parameter space θ {\displaystyle \theta } that is compact. for an open θ {\displaystyle \theta } the likelihood function may increase without ever reaching a supremum value. in practice, it is often convenient to work with the natural logarithm of the likelihood function, called the log-likelihood: since the logarithm is a monotonic function, the maximum of ℓ ( θ ; y ) {\displaystyle \ell (\theta \,;\mathbf {y} )} occurs at the same value of θ {\displaystyle \theta } as does the maximum of l n {\displaystyle l_{n}} . if ℓ ( θ ; y ) {\displaystyle \ell (\theta \,;\mathbf {y} )} is differentiable in θ {\displaystyle \theta } , the necessary conditions for the occurrence of a maximum (or a minimum) are known as the likelihood equations. for some models, these equations can be explicitly solved for θ ^ {\displaystyle {\widehat {\theta \,}}} , but in general no closed-form solution to the maximization problem is known or available, and an mle can only be found via numerical optimization. another problem is that in finite samples, there may exist multiple roots for the likelihood equations. whether the identified root θ ^ {\displaystyle {\widehat {\theta \,}}} of the likelihood equations is indeed a (local) maximum depends on whether the matrix of second-order partial and cross-partial derivatives, the so-called hessian matrix "
288,288,mean,1,https://en.wikipedia.org/wiki/Mean,"there are several kinds of mean in mathematics, especially in statistics. for a data set, the arithmetic mean, also known as arithmetic average, is a central value of a finite set of numbers: specifically, the sum of the values divided by the number of values. the arithmetic mean of a set of numbers x , x , ..., xn is typically denoted by x ¯ {\displaystyle {\bar {x}}} [note  ]. if the data set were based on a series of observations obtained by sampling from a statistical population, the arithmetic mean is the sample mean (denoted x ¯ {\displaystyle {\bar {x}}} ) to distinguish it from the mean, or expected value, of the underlying distribution, the population mean (denoted μ {\displaystyle \mu } or μ x {\displaystyle \mu _{x}} [note  ]). outside probability and statistics, a wide range of other notions of mean are often used in geometry and mathematical analysis; examples are given below. the arithmetic mean (or simply mean) of a list of numbers, is the sum of all of the numbers divided by the number of numbers. similarly, the mean of a sample x   , x   , … , x n {\displaystyle x_{ },x_{ },\ldots ,x_{n}} , usually denoted by x ¯ {\displaystyle {\bar {x}}} , is the sum of the sampled values divided by the number of items in the sample for example, the arithmetic mean of five values:  ,   ,   ,   ,    is: the geometric mean is an average that is useful for sets of positive numbers, that are interpreted according to their product (as is the case with rates of growth) and not their sum (as is the case with the arithmetic mean): for example, the geometric mean of five values:  ,   ,   ,   ,    is: the harmonic mean is an average which is useful for sets of numbers which are defined in relation to some unit, as in the case of speed (i.e., distance per unit of time): for example, the harmonic mean of the five values:  ,   ,   ,   ,    is am, gm, and hm satisfy these inequalities: "
289,289,Cauchy's Mean Value Theorem,1,https://en.wikipedia.org/wiki/Mean_value_theorem,"in mathematics, the mean value theorem states, roughly, that for a given planar arc between two endpoints, there is at least one point at which the tangent to the arc is parallel to the secant through its endpoints. it is one of the most important results in real analysis. this theorem is used to prove statements about a function on an interval starting from local hypotheses about derivatives at points of the interval. more precisely, the theorem states that if f {\displaystyle f} is a continuous function on the closed interval [ a , b ] {\displaystyle [a,b]} and differentiable on the open interval ( a , b ) {\displaystyle (a,b)} , then there exists a point c {\displaystyle c} in ( a , b ) {\displaystyle (a,b)} such that the tangent at c {\displaystyle c} is parallel to the secant line through the endpoints ( a , f ( a ) ) {\displaystyle {\big (}a,f(a){\big )}} and ( b , f ( b ) ) {\displaystyle {\big (}b,f(b){\big )}} , that is, a special case of this theorem was first described by parameshvara (    –    ), from the kerala school of astronomy and mathematics in india, in his commentaries on govindasvāmi and bhāskara ii. a restricted form of the theorem was proved by michel rolle in     ; the result was what is now known as rolle's theorem, and was proved only for polynomials, without the techniques of calculus. the mean value theorem in its modern form was stated and proved by augustin louis cauchy in     . many variations of this theorem have been proved since then. let f : [ a , b ] → r {\displaystyle f:[a,b]\to \mathbb {r} } be a continuous function on the closed interval [ a , b ] {\displaystyle [a,b]} , and differentiable on the open interval ( a , b ) {\displaystyle (a,b)} , where a < b {\displaystyle a<b} . then there exists some c {\displaystyle c} in ( a , b ) {\displaystyle (a,b)} such that the mean value theorem is a generalization of rolle's theorem, which assumes f ( a ) = f ( b ) {\displaystyle f(a)=f(b)} , so that the right-hand side above is zero. the mean value theorem is still valid in a slightly more general setting. one only needs to assume that f : [ a , b ] → r {\displaystyle f:[a,b]\to \mathbb {r} } is continuous on [ a , b ] {\displaystyle [a,b]} , and that for every x {\displaystyle x} in ( a , b ) {\displaystyle (a,b)} the limit exists as a finite number or equals ∞ {\displaystyle \infty } or − ∞ {\displaystyle -\infty } . if finite, that limit equals f ′ ( x ) {\displaystyle f'(x)} . an example where this version of the theorem applies is given by the real-valued cube root function mapping x ↦ x   /   {\displaystyle x\mapsto x^{ / }} , whose derivative tends to infinity at the origin. note that the theorem, as stated, is false if a differentiable function is complex-valued instead of real-valued. for example, define f ( x ) = e x i {\displaystyle f(x)=e^{xi}} for all real x {\displaystyle x} . then while f ′ ( x ) ≠   {\displaystyle f'(x)\neq  } for any real x {\displaystyle x} . these formal statements are also known as lagrange's mean value theorem. "
290,290,median,1,https://en.wikipedia.org/wiki/Median,"in statistics and probability theory, the median is the value separating the higher half from the lower half of a data sample, a population, or a probability distribution. for a data set, it may be thought of as ""the middle"" value. the basic feature of the median in describing data compared to the mean (often simply described as the ""average"") is that it is not skewed by a small proportion of extremely large or small values, and therefore provides a better representation of a ""typical"" value. median income, for example, may be a better way to suggest what a ""typical"" income is, because income distribution can be very skewed. the median is of central importance in robust statistics, as it is the most resistant statistic, having a breakdown point of   %: so long as no more than half the data are contaminated, the median is not an arbitrarily large or small result. the median of a finite list of numbers is the ""middle"" number, when those numbers are listed in order from smallest to greatest. if the data set has an odd number of observations, the middle one is selected. for example, the following list of seven numbers, has the median of  , which is the fourth value. if the data set has an even number of observations, there is no distinct middle value and the median is usually defined to be the arithmetic mean of the two middle values. for example, this data set of   numbers has a median value of  . , that is (   +   ) /   {\displaystyle ( + )/ } . (in more technical terms, this interprets the median as the fully trimmed mid-range). in general, with this convention, the median can be defined as follows: for a data set x {\displaystyle x} of n {\displaystyle n} elements, ordered from smallest to greatest, formally, a median of a population is any value such that at most half of the population is less than the proposed median and at most half is greater than the proposed median. as seen above, medians may not be unique. if each set contains less than half the population, then some of the population is exactly equal to the unique median. the median is well-defined for any ordered (one-dimensional) data, and is independent of any distance metric. the median can thus be applied to classes which are ranked but not numerical (e.g. working out a median grade when students are graded from a to f), although the result might be halfway between classes if there is an even number of cases. a geometric median, on the other hand, is defined in any number of dimensions. a related concept, in which the outcome is forced to correspond to a member of the sample, is the medoid. "
291,291,Menger's Theorem,2,https://en.wikipedia.org/wiki/Menger%27s_theorem,"in the mathematical discipline of graph theory, menger's theorem says that in a finite graph, the size of a minimum cut set is equal to the maximum number of disjoint paths that can be found between any pair of vertices. proved by karl menger in     , it characterizes the connectivity of a graph. it is generalized by the max-flow min-cut theorem, which is a weighted, edge version, and which in turn is a special case of the strong duality theorem for linear programs. the edge-connectivity version of menger's theorem is as follows: the vertex-connectivity statement of menger's theorem is as follows: all these statements (in both edge and vertex versions) remain true in directed graphs (when considering directed paths). most direct proofs consider a more general statement to allow proving it by induction. it is also convenient to use definitions that include some degenerate cases. the following proof for undirected graphs works without change for directed graphs or multi-graphs, provided we take path to mean directed path. for sets of vertices a,b ⊂ g (not necessarily disjoint), an ab-path is a path in g with a starting vertex in a, a final vertex in b, and no internal vertices in a or b. we allow a path with a single vertex in a ∩ b and zero edges. an ab-separator of size k is a set s of k vertices (which may intersect a and b) such that g−s contains no ab-path. an ab-connector of size k is a union of k vertex-disjoint ab-paths. in other words, if no k−  vertices disconnect a from b, then there exist k disjoint paths from a to b. this variant implies the above vertex-connectivity statement: for x,y ∈ g in the previous section, apply the current theorem to g−{x,y} with a = n(x), b = n(y), the neighboring vertices of x,y. then a set of vertices disconnecting x and y is the same thing as an ab-separator, and removing the end vertices in a set of independent xy-paths gives an ab-connector. proof of the theorem: induction on the number of edges in g. for g with no edges, the minimum ab-separator is a ∩ b, which is itself an ab-connector consisting of single-vertex paths. for g having an edge e, we may assume by induction that the theorem holds for g−e. if g−e has a minimal ab-separator of size k, then there is an ab-connector of size k in g−e, and hence in g. otherwise, let s be a ab-separator of g−e of size less than k, so that every ab-path in g contains a vertex of s or the edge e. the size of s must be k- , since if it was less, s together with either endpoint of e would be a better ab-separator of g. in g−s there is an ab-path through e, since s alone is too small to be an ab-separator of g. let v  be the earlier and v  be the later vertex of e on such a path. then v  is reachable from a but not from b in g−s−e, while v  is reachable from b but not from a. "
292,292,Minimax,0,https://en.wikipedia.org/wiki/Minimax,"minimax (sometimes minmax, mm or saddle point ) is a decision rule used in artificial intelligence, decision theory, game theory, statistics, and philosophy for minimizing the possible loss for a worst case (maximum loss) scenario. when dealing with gains, it is referred to as ""maximin""—to maximize the minimum gain. originally formulated for n-player zero-sum game theory, covering both the cases where players take alternate moves and those where they make simultaneous moves, it has also been extended to more complex games and to general decision-making in the presence of uncertainty. the maximin value is the highest value that the player can be sure to get without knowing the actions of the other players; equivalently, it is the lowest value the other players can force the player to receive when they know the player's action. its formal definition is: where: calculating the maximin value of a player is done in a worst-case approach: for each possible action of the player, we check all possible actions of the other players and determine the worst possible combination of actions—the one that gives player i the smallest value. then, we determine which action player i can take in order to make sure that this smallest value is the highest possible. for example, consider the following game for two players, where the first player (""row player"") may choose any of three moves, labelled t, m, or b, and the second player (""column"" player) may choose either of two moves, l or r. the result of the combination of both moves is expressed in a payoff table: (where the first number in each cell is the pay-out of the row player and the second number is the pay-out of the column player). for the sake of example, we consider only pure strategies. check each player in turn: if both players play their respective maximin strategies ( t , l ) {\displaystyle (t,l)} , the payoff vector is (   ,   ) {\displaystyle ( , )} . the minimax value of a player is the smallest value that the other players can force the player to receive, without knowing the player's actions; equivalently, it is the largest value the player can be sure to get when they know the actions of the other players. its formal definition is: the definition is very similar to that of the maximin value—only the order of the maximum and minimum operators is inverse. in the above example: "
293,293,Minkowski inequality,0,https://en.wikipedia.org/wiki/Minkowski_inequality,"in mathematical analysis, the minkowski inequality establishes that the lp spaces are normed vector spaces. let s be a measure space, let   ≤ p < ∞ and let f and g be elements of lp(s). then f + g is in lp(s), and we have the triangle inequality with equality for   < p < ∞ if and only if f and g are positively linearly dependent, i.e., f = λg for some λ ≥   or g =  . here, the norm is given by: if p < ∞, or in the case p = ∞ by the essential supremum the minkowski inequality is the triangle inequality in lp(s). in fact, it is a special case of the more general fact where it is easy to see that the right-hand side satisfies the triangular inequality. like hölder's inequality, the minkowski inequality can be specialized to sequences and vectors by using the counting measure: for all real (or complex) numbers x , ..., xn, y , ..., yn and where n is the cardinality of s (the number of elements in s). the inequality is named after the german mathematician hermann minkowski. first, we prove that f+g has finite p-norm if f and g both do, which follows by indeed, here we use the fact that h ( x ) = | x | p {\displaystyle h(x)=|x|^{p}} is convex over r+ (for p >  ) and so, by the definition of convexity, "
294,294,mode,1,https://en.wikipedia.org/wiki/Mode_(statistics),"the mode is the value that appears most often in a set of data values. if x is a discrete random variable, the mode is the value x (i.e, x = x) at which the probability mass function takes its maximum value. in other words, it is the value that is most likely to be sampled. like the statistical mean and median, the mode is a way of expressing, in a (usually) single number, important information about a random variable or a population. the numerical value of the mode is the same as that of the mean and median in a normal distribution, and it may be very different in highly skewed distributions. the mode is not necessarily unique to a given discrete distribution, since the probability mass function may take the same maximum value at several points x , x , etc. the most extreme case occurs in uniform distributions, where all values occur equally frequently. when the probability density function of a continuous distribution has multiple local maxima it is common to refer to all of the local maxima as modes of the distribution. such a continuous distribution is called multimodal (as opposed to unimodal). a mode of a continuous probability distribution is often considered to be any value x at which its probability density function has a locally maximum value, so any peak is a mode. in symmetric unimodal distributions, such as the normal distribution, the mean (if defined), median and mode all coincide. for samples, if it is known that they are drawn from a symmetric unimodal distribution, the sample mean can be used as an estimate of the population mode. the mode of a sample is the element that occurs most often in the collection. for example, the mode of the sample [ ,  ,  ,  ,  ,  ,  ,  ,   ,   ,   ] is  . given the list of data [ ,  ,  ,  ,  ] its mode is not unique. a dataset, in such a case, is said to be bimodal, while a set with more than two modes may be described as multimodal. for a sample from a continuous distribution, such as [ .   ...,  .   ...,  .   ...,  .   ...,  .   ...], the concept is unusable in its raw form, since no two values will be exactly the same, so each value will occur precisely once. in order to estimate the mode of the underlying distribution, the usual practice is to discretize the data by assigning frequency values to intervals of equal distance, as for making a histogram, effectively replacing the values by the midpoints of the intervals they are assigned to. the mode is then the value where the histogram reaches its peak. for small or middle-sized samples the outcome of this procedure is sensitive to the choice of interval width if chosen too narrow or too wide; typically one should have a sizable fraction of the data concentrated in a relatively small number of intervals (  to   ), while the fraction of the data falling outside these intervals is also sizable. an alternate approach is kernel density estimation, which essentially blurs point samples to produce a continuous estimate of the probability density function which can provide an estimate of the mode. the following matlab (or octave) code example computes the mode of a sample: the algorithm requires as a first step to sort the sample in ascending order. it then computes the discrete derivative of the sorted list, and finds the indices where this derivative is positive. next it computes the discrete derivative of this set of indices, locating the maximum of this derivative of indices, and finally evaluates the sorted sample at the point where that maximum occurs, which corresponds to the last member of the stretch of repeated values. unlike mean and median, the concept of mode also makes sense for ""nominal data"" (i.e., not consisting of numerical values in the case of mean, or even of ordered values in the case of median). for example, taking a sample of korean family names, one might find that ""kim"" occurs more often than any other name. then ""kim"" would be the mode of the sample. in any voting system where a plurality determines victory, a single modal value determines the victor, while a multi-modal outcome would require some tie-breaking procedure to take place. "
295,295,model theory,2,https://en.wikipedia.org/wiki/Model_theory,"in mathematical logic, model theory is the study of the relationship between formal theories (a collection of sentences in a formal language expressing statements about a mathematical structure), and their models (those structures in which the statements of the theory hold). the aspects investigated include the number and size of models of a theory, the relationship of different models to each other, and their interaction with the formal language itself. in particular, model theorists also investigate the sets that can be defined in a model of a theory, and the relationship of such definable sets to each other. as a separate discipline, model theory goes back to alfred tarski, who first used the term ""theory of models"" in publication in     . since the     s, the subject has been shaped decisively by saharon shelah's stability theory. compared to other areas of mathematical logic such as proof theory, model theory is often less concerned with formal rigour and closer in spirit to classical mathematics. this has prompted the comment that ""if proof theory is about the sacred, then model theory is about the profane"". the applications of model theory to algebraic and diophantine geometry reflect this proximity to classical mathematics, as they often involve an integration of algebraic and model-theoretic results and techniques. the most prominent scholarly organization in the field of model theory is the association for symbolic logic. this page focuses on finitary first order model theory of infinite structures. the relative emphasis placed on the class of models of a theory as opposed to the class of definable sets within a model fluctuated in the history of the subject, and the two directions are summarised by the pithy characterisations from      and      respectively: where universal algebra stands for mathematical structures and logic for logical theories; and where logical formulas are to definable sets what equations are to varieties over a field. nonetheless, the interplay of classes of models and the sets definable in them has been crucial to the development of model theory throughout its history. for instance, while stability was originally introduced to classify theories by their numbers of models in a given cardinality, stability theory proved crucial to understanding the geometry of definable sets. a first-order formula is built out of atomic formulas such as r(f(x,y),z) or y = x +   by means of the boolean connectives ¬ , ∧ , ∨ , → {\displaystyle \neg ,\land ,\lor ,\rightarrow } and prefixing of quantifiers ∀ v {\displaystyle \forall v} or ∃ v {\displaystyle \exists v} . a sentence is a formula in which each occurrence of a variable is in the scope of a corresponding quantifier. examples for formulas are φ (or φ(x) to mark the fact that at most x is an unbound variable in φ) and ψ defined as follows: (note that the equality symbol has a double meaning here.) it is intuitively clear how to translate such formulas into mathematical meaning. in the σsmr-structure n {\displaystyle {\mathcal {n}}} of the natural numbers, for example, an element n satisfies the formula φ if and only if n is a prime number. the formula ψ similarly defines irreducibility. tarski gave a rigorous definition, sometimes called ""tarski's definition of truth"", for the satisfaction relation ⊨ {\displaystyle \models } , so that one easily proves: "
296,296,Moment-generating function,2,https://en.wikipedia.org/wiki/Moment-generating_function,"in probability theory and statistics, the moment-generating function of a real-valued random variable is an alternative specification of its probability distribution. thus, it provides the basis of an alternative route to analytical results compared with working directly with probability density functions or cumulative distribution functions. there are particularly simple results for the moment-generating functions of distributions defined by the weighted sums of random variables. however, not all random variables have moment-generating functions. as its name implies, the moment generating function can be used to compute a distribution’s moments: the nth moment about   is the nth derivative of the moment-generating function, evaluated at  . in addition to real-valued distributions (univariate distributions), moment-generating functions can be defined for vector- or matrix-valued random variables, and can even be extended to more general cases. the moment-generating function of a real-valued distribution does not always exist, unlike the characteristic function. there are relations between the behavior of the moment-generating function of a distribution and properties of the distribution, such as the existence of moments. let x {\displaystyle x} be a random variable with cdf f x {\displaystyle f_{x}} . the moment generating function (mgf) of x {\displaystyle x} (or f x {\displaystyle f_{x}} ), denoted by m x ( t ) {\displaystyle m_{x}(t)} , is provided this expectation exists for t {\displaystyle t} in some neighborhood of  . that is, there is an h >   {\displaystyle h> } such that for all t {\displaystyle t} in − h < t < h {\displaystyle -h<t<h} , e ⁡ [ e t x ] {\displaystyle \operatorname {e} \left[e^{tx}\right]} exists. if the expectation does not exist in a neighborhood of  , we say that the moment generating function does not exist. in other words, the moment-generating function of x is the expectation of the random variable e t x {\displaystyle e^{tx}} . more generally, when x = ( x   , … , x n ) t {\displaystyle \mathbf {x} =(x_{ },\ldots ,x_{n})^{\mathrm {t} }} , an n {\displaystyle n} -dimensional random vector, and t {\displaystyle \mathbf {t} } is a fixed vector, one uses t ⋅ x = t t x {\displaystyle \mathbf {t} \cdot \mathbf {x} =\mathbf {t} ^{\mathrm {t} }\mathbf {x} } instead of t x {\displaystyle tx} : m x (   ) {\displaystyle m_{x}( )} always exists and is equal to  . however, a key problem with moment-generating functions is that moments and the moment-generating function may not exist, as the integrals need not converge absolutely. by contrast, the characteristic function or fourier transform always exists (because it is the integral of a bounded function on a space of finite measure), and for some purposes may be used instead. the moment-generating function is so named because it can be used to find the moments of the distribution. the series expansion of e t x {\displaystyle e^{tx}} is hence "
297,297,Monotonic function,1,https://en.wikipedia.org/wiki/Monotonic_function,"in mathematics, a monotonic function (or monotone function) is a function between ordered sets that preserves or reverses the given order. this concept first arose in calculus, and was later generalized to the more abstract setting of order theory. in calculus, a function f {\displaystyle f} defined on a subset of the real numbers with real values is called monotonic if and only if it is either entirely non-increasing, or entirely non-decreasing. that is, as per fig.  , a function that increases monotonically does not exclusively have to increase, it simply must not decrease. a function is called monotonically increasing (also increasing or non-decreasing), if for all x {\displaystyle x} and y {\displaystyle y} such that x ≤ y {\displaystyle x\leq y} one has f ( x ) ≤ f ( y ) {\displaystyle f\!\left(x\right)\leq f\!\left(y\right)} , so f {\displaystyle f} preserves the order (see figure  ). likewise, a function is called monotonically decreasing (also decreasing or non-increasing) if, whenever x ≤ y {\displaystyle x\leq y} , then f ( x ) ≥ f ( y ) {\displaystyle f\!\left(x\right)\geq f\!\left(y\right)} , so it reverses the order (see figure  ). if the order ≤ {\displaystyle \leq } in the definition of monotonicity is replaced by the strict order < {\displaystyle <} , then one obtains a stronger requirement. a function with this property is called strictly increasing (also increasing). again, by inverting the order symbol, one finds a corresponding concept called strictly decreasing (also decreasing). a function may be called strictly monotone if it is either strictly increasing or strictly decreasing. functions that are strictly monotone are one-to-one (because for x {\displaystyle x} not equal to y {\displaystyle y} , either x < y {\displaystyle x<y} or x > y {\displaystyle x>y} and so, by monotonicity, either f ( x ) < f ( y ) {\displaystyle f\!\left(x\right)<f\!\left(y\right)} or f ( x ) > f ( y ) {\displaystyle f\!\left(x\right)>f\!\left(y\right)} , thus f ( x ) ≠ f ( y ) {\displaystyle f\!\left(x\right)\neq f\!\left(y\right)} .) if it is not clear that ""increasing"" and ""decreasing"" are taken to include the possibility of repeating the same value at successive arguments, one may use the terms weakly monotone, weakly increasing and weakly decreasing to stress this possibility. the terms ""non-decreasing"" and ""non-increasing"" should not be confused with the (much weaker) negative qualifications ""not decreasing"" and ""not increasing"". for example, the function of figure   first falls, then rises, then falls again. it is therefore not decreasing and not increasing, but it is neither non-decreasing nor non-increasing. a function f ( x ) {\displaystyle f\!\left(x\right)} is said to be absolutely monotonic over an interval ( a , b ) {\displaystyle \left(a,b\right)} if the derivatives of all orders of f {\displaystyle f} are nonnegative or all nonpositive at all points on the interval. a function that is monotonic, but not strictly monotonic, and thus constant on an interval, doesn't have an inverse. this is because in order for a function to have an inverse, there needs to be a one-to-one mapping from the range to the domain of the function. since a monotonic function has some values that are constant in its domain, this means that there would be more than one value in the range that maps to this constant value. however, a function y = g(x) that is strictly monotonic, has an inverse function such that x = h(y) because there is guaranteed to always be a one-to-one mapping from range to domain of the function. also, a function can be said to be strictly monotonic on a range of values, and thus have an inverse on that range of value. for example, if y = g(x) is strictly monotonic on the range [a, b], then it has an inverse x = h(y) on the range [g(a), g(b)], but we cannot say the entire range of the function has an inverse. note, some textbooks[which?] mistakenly state that an inverse exists for a monotonic function, when they really mean that an inverse exists for a strictly monotonic function. "
298,298,Multinomial Theorm,2,https://en.wikipedia.org/wiki/Multinomial_theorem,"in mathematics, the multinomial theorem describes how to expand a power of a sum in terms of powers of the terms in that sum. it is the generalization of the binomial theorem from binomials to multinomials. for any positive integer m and any non-negative integer n, the multinomial formula describes how a sum with m terms expands when raised to an arbitrary power n: where is a multinomial coefficient. the sum is taken over all combinations of nonnegative integer indices k  through km such that the sum of all ki is n. that is, for each term in the expansion, the exponents of the xi must add up to n. also, as with the binomial theorem, quantities of the form x  that appear are taken to equal   (even when x equals zero). in the case m =  , this statement reduces to that of the binomial theorem. the third power of the trinomial a + b + c is given by this can be computed by hand using the distributive property of multiplication over addition, but it can also be done (perhaps more easily) with the multinomial theorem. it is possible to ""read off"" the multinomial coefficients from the terms by using the multinomial coefficient formula. for example: the statement of the theorem can be written concisely using multiindices: where and "
299,299,sub-multiple angles,2,https://en.wikipedia.org/wiki/Multiple_(mathematics),"in science, a multiple is the product of any quantity and an integer. in other words, for the quantities a and b, it can be said that b is a multiple of a if b = na for some integer n, which is called the multiplier. if a is not zero, this is equivalent to saying that b a {\displaystyle {\frac {b}{a}}} is an integer. in mathematics, when a and b are both integers, and b is a multiple of a, then a is called a divisor of b. one says also that a divides b. if a and b are not integers, mathematicians prefer generally to use integer multiple instead of multiple, for clarification. in fact, multiple is used for other kinds of product; for example, a polynomial p is a multiple of another polynomial q if there exists third polynomial r such that p = qr. in some texts, ""a is a submultiple of b"" has the meaning of ""a being a unit fraction of b"" or, equivalently, ""b being an integer multiple of a"". this terminology is also used with units of measurement (for example by the bipm and nist ), where a submultiple of a main unit is a unit, named by prefixing the main unit, defined as the quotient of the main unit by an integer, mostly a power of    . for example, a millimetre is the     -fold submultiple of a metre. as another example, one inch may be considered as a   -fold submultiple of a foot, or a   -fold submultiple of a yard.   ,   , –   and   are multiples of  , whereas   and –  are not. this is because there are integers that   may be multiplied by to reach the values of   ,   ,   and –  , while there are no such integers for   and – . each of the products listed below, and in particular, the products for   and – , is the only way that the relevant number can be written as a product of   and another real number: "
300,300,Multi integral,3,https://en.wikipedia.org/wiki/Multiple_integral,"in mathematics (specifically multivariable calculus), a multiple integral is a definite integral of a function of several real variables, for instance, f(x, y) or f(x, y, z). integrals of a function of two variables over a region in r   {\displaystyle \mathbb {r} ^{ }} (the real-number plane) are called double integrals, and integrals of a function of three variables over a region in r   {\displaystyle \mathbb {r} ^{ }} (real-number  d space) are called triple integrals. for multiple integrals of a single-variable function, see the cauchy formula for repeated integration. just as the definite integral of a positive function of one variable represents the area of the region between the graph of the function and the x-axis, the double integral of a positive function of two variables represents the volume of the region between the surface defined by the function (on the three-dimensional cartesian plane where z = f(x, y)) and the plane which contains its domain. if there are more variables, a multiple integral will yield hypervolumes of multidimensional functions. multiple integration of a function in n variables: f(x , x , ..., xn) over a domain d is most commonly represented by nested integral signs in the reverse order of execution (the leftmost integral sign is computed last), followed by the function and integrand arguments in proper order (the integral with respect to the rightmost argument is computed last). the domain of integration is either represented symbolically for every argument over each integral sign, or is abbreviated by a variable at the rightmost integral sign: since the concept of an antiderivative is only defined for functions of a single real variable, the usual definition of the indefinite integral does not immediately extend to the multiple integral. for n >  , consider a so-called ""half-open"" n-dimensional hyperrectangular domain t, defined as: partition each interval [aj, bj) into a finite family ij of non-overlapping subintervals ijα, with each subinterval closed at the left end, and open at the right end. then the finite family of subrectangles c given by is a partition of t; that is, the subrectangles ck are non-overlapping and their union is t. let f : t → r be a function defined on t. consider a partition c of t as defined above, such that c is a family of m subrectangles cm and we can approximate the total (n +  )th-dimensional volume bounded below by the n-dimensional hyperrectangle t and above by the n-dimensional graph of f with the following riemann sum: "
301,301,Multiplication theory,3,https://en.wikipedia.org/wiki/Multiplication_theorem,"in mathematics, the multiplication theorem is a certain type of identity obeyed by many special functions related to the gamma function. for the explicit case of the gamma function, the identity is a product of values; thus the name. the various relations all stem from the same underlying principle; that is, the relation for one special function can be derived from that for the others, and is simply a manifestation of the same identity in different guises. the multiplication theorem takes two common forms. in the first case, a finite number of terms are added or multiplied to give the relation. in the second case, an infinite number of terms are added or multiplied. the finite form typically occurs only for the gamma and related functions, for which the identity follows from a p-adic relation over a finite field. for example, the multiplication theorem for the gamma function follows from the chowla–selberg formula, which follows from the theory of complex multiplication. the infinite sums are much more common, and follow from characteristic zero relations on the hypergeometric series. the following tabulates the various appearances of the multiplication theorem for finite characteristic; the characteristic zero relations are given further down. in all cases, n and k are non-negative integers. for the special case of n =  , the theorem is commonly referred to as the duplication formula. the duplication formula and the multiplication theorem for the gamma function are the prototypical examples. the duplication formula for the gamma function is it is also called the legendre duplication formula or legendre relation, in honor of adrien-marie legendre. the multiplication theorem is for integer k ≥  , and is sometimes called gauss's multiplication formula, in honour of carl friedrich gauss. the multiplication theorem for the gamma functions can be understood to be a special case, for the trivial dirichlet character, of the chowla–selberg formula. the polygamma function is the logarithmic derivative of the gamma function, and thus, the multiplication theorem becomes additive, instead of multiplicative: for m >   {\displaystyle m> } , and, for m =   {\displaystyle m= } , one has the digamma function: the polygamma identities can be used to obtain a multiplication theorem for harmonic numbers. for the hurwitz zeta function generalizes the polygamma function to non-integer orders, and thus obeys a very similar multiplication theorem: "
302,302,Multiplicative Inverse,1,https://en.wikipedia.org/wiki/Multiplicative_inverse,"in mathematics, a multiplicative inverse or reciprocal for a number x, denoted by  /x or x− , is a number which when multiplied by x yields the multiplicative identity,  . the multiplicative inverse of a fraction a/b is b/a. for the multiplicative inverse of a real number, divide   by the number. for example, the reciprocal of   is one fifth ( /  or  . ), and the reciprocal of  .   is   divided by  .  , or  . the reciprocal function, the function f(x) that maps x to  /x, is one of the simplest examples of a function which is its own inverse (an involution). multiplying by a number is the same as dividing by its reciprocal and vice versa. for example, multiplication by  /  (or  . ) will give the same result as division by  /  (or  .  ). therefore, multiplication by a number followed by multiplication by its reciprocal yields the original number (since the product of the number and its reciprocal is  ). the term reciprocal was in common use at least as far back as the third edition of encyclopædia britannica (    ) to describe two numbers whose product is  ; geometrical quantities in inverse proportion are described as reciprocall in a      translation of euclid's elements. in the phrase multiplicative inverse, the qualifier multiplicative is often omitted and then tacitly understood (in contrast to the additive inverse). multiplicative inverses can be defined over many mathematical domains as well as numbers. in these cases it can happen that ab ≠ ba; then ""inverse"" typically implies that an element is both a left and right inverse. the notation f −  is sometimes also used for the inverse function of the function f, which is not in general equal to the multiplicative inverse. for example, the multiplicative inverse  /(sin x) = (sin x)−  is the cosecant of x, and not the inverse sine of x denoted by sin−  x or arcsin x. only for linear maps are they strongly related (see below). the terminology difference reciprocal versus inverse is not sufficient to make this distinction, since many authors prefer the opposite naming convention, probably for historical reasons (for example in french, the inverse function is preferably called the bijection réciproque). in the real numbers, zero does not have a reciprocal because no real number multiplied by   produces   (the product of any number with zero is zero). with the exception of zero, reciprocals of every real number are real, reciprocals of every rational number are rational, and reciprocals of every complex number are complex. the property that every element other than zero has a multiplicative inverse is part of the definition of a field, of which these are all examples. on the other hand, no integer other than   and −  has an integer reciprocal, and so the integers are not a field. in modular arithmetic, the modular multiplicative inverse of a is also defined: it is the number x such that ax ≡   (mod n). this multiplicative inverse exists if and only if a and n are coprime. for example, the inverse of   modulo    is   because   ⋅   ≡   (mod   ). the extended euclidean algorithm may be used to compute it. the sedenions are an algebra in which every nonzero element has a multiplicative inverse, but which nonetheless has divisors of zero, that is, nonzero elements x, y such that xy =  . a square matrix has an inverse if and only if its determinant has an inverse in the coefficient ring. the linear map that has the matrix a−  with respect to some base is then the reciprocal function of the map having a as matrix in the same base. thus, the two distinct notions of the inverse of a function are strongly related in this case, while they must be carefully distinguished in the general case (as noted above). the trigonometric functions are related by the reciprocal identity: the cotangent is the reciprocal of the tangent; the secant is the reciprocal of the cosine; the cosecant is the reciprocal of the sine. "
303,303,Multivariable calculus,2,https://en.wikipedia.org/wiki/Multivariable_calculus,"multivariable calculus (also known as multivariate calculus) is the extension of calculus in one variable to calculus with functions of several variables: the differentiation and integration of functions involving several variables, rather than just one. a study of limits and continuity in multivariable calculus yields many counterintuitive results not demonstrated by single-variable functions. :   –   for example, there are scalar functions of two variables with points in their domain which give different limits when approached along different paths. e.g., the function. approaches zero whenever the point (   ,   ) {\displaystyle ( , )} is approached along lines through the origin ( y = k x {\displaystyle y=kx} ). however, when the origin is approached along a parabola y = ± x   {\displaystyle y=\pm x^{ }} , the function value has a limit of ±   /   {\displaystyle \pm  / } . since taking different paths toward the same point yields different limit values, a general limit does not exist there. continuity in each argument not being sufficient for multivariate continuity can also be seen from the following example. :   –   in particular, for a real-valued function with two real-valued parameters, f ( x , y ) {\displaystyle f(x,y)} , continuity of f {\displaystyle f} in x {\displaystyle x} for fixed y {\displaystyle y} and continuity of f {\displaystyle f} in y {\displaystyle y} for fixed x {\displaystyle x} does not imply continuity of f {\displaystyle f} . consider it is easy to verify that this function is zero by definition on the boundary and outside of the quadrangle (   ,   ) × (   ,   ) {\displaystyle ( , )\times ( , )} . furthermore, the functions defined for constant x {\displaystyle x} and y {\displaystyle y} and   ≤ a ≤   {\displaystyle  \leq a\leq  } by are continuous. specifically, however, the sequence f (   n ,   n ) {\displaystyle f\left({\tfrac { }{n}},{\tfrac { }{n}}\right)} (for natural n {\displaystyle n} ) converges to lim n → ∞ f (   n ,   n ) =   {\displaystyle \lim _{n\to \infty }f\left({\tfrac { }{n}},{\tfrac { }{n}}\right)= } , rendering the function as discontinuous at (   ,   ) {\displaystyle ( , )} . approaching the origin not along parallels to the x {\displaystyle x} - and y {\displaystyle y} -axis reveals this discontinuity. if f ( x , y ) {\displaystyle f(x,y)} is continuous at ( a , b ) , {\displaystyle (a,b),} and g {\displaystyle g} is a single variable function continuous at f ( a , b ) , {\displaystyle f(a,b),} then the composite function h = g ∘ f {\displaystyle h=g\circ f} defined by h ( x , y ) = g ( f ( x , y ) ) {\displaystyle h(x,y)=g(f(x,y))} is continuous at ( a , b ) . {\displaystyle (a,b).} for examples, exp ⁡ ( x − y ) {\displaystyle \exp(x-y)} and ln ⁡ (   + x y −   x +    y ) . {\displaystyle \ln( +xy- x+  y).} "
304,304,Mutually Exclusive Events,1,https://en.wikipedia.org/wiki/Mutual_exclusivity,"in logic and probability theory, two events (or propositions) are mutually exclusive or disjoint if they cannot both occur at the same time. a clear example is the set of outcomes of a single coin toss, which can result in either heads or tails, but not both. in the coin-tossing example, both outcomes are, in theory, collectively exhaustive, which means that at least one of the outcomes must happen, so these two possibilities together exhaust all the possibilities. however, not all mutually exclusive events are collectively exhaustive. for example, the outcomes   and   of a single roll of a six-sided die are mutually exclusive (both cannot happen at the same time) but not collectively exhaustive (there are other possible outcomes;  , , , ). in logic, two mutually exclusive propositions are propositions that logically cannot be true in the same sense at the same time. to say that more than two propositions are mutually exclusive, depending on the context, means that one cannot be true if the other one is true, or at least one of them cannot be true. the term pairwise mutually exclusive always means that two of them cannot be true simultaneously. in probability theory, events e , e , ..., en are said to be mutually exclusive if the occurrence of any one of them implies the non-occurrence of the remaining n −   events. therefore, two mutually exclusive events cannot both occur. formally said, the intersection of each two of them is empty (the null event): a ∩ b = ∅. in consequence, mutually exclusive events have the property: p(a ∩ b) =  . for example, in a standard   -card deck with two colors it is impossible to draw a card that is both red and a club because clubs are always black. if just one card is drawn from the deck, either a red card (heart or diamond) or a black card (club or spade) will be drawn. when a and b are mutually exclusive, p(a ∪ b) = p(a) + p(b). to find the probability of drawing a red card or a club, for example, add together the probability of drawing a red card and the probability of drawing a club. in a standard   -card deck, there are twenty-six red cards and thirteen clubs:   /   +   /   =   /   or  / . one would have to draw at least two cards in order to draw both a red card and a club. the probability of doing so in two draws depends on whether the first card drawn was replaced before the second drawing since without replacement there is one fewer card after the first card was drawn. the probabilities of the individual events (red, and club) are multiplied rather than added. the probability of drawing a red and a club in two drawings without replacement is then   /   ×   /   ×   =    /    , or   /  . with replacement, the probability would be   /   ×   /   ×   =    /    , or   /  . in probability theory, the word or allows for the possibility of both events happening. the probability of one or both events occurring is denoted p(a ∪ b) and in general, it equals p(a) + p(b) – p(a ∩ b). therefore, in the case of drawing a red card or a king, drawing any of a red king, a red non-king, or a black king is considered a success. in a standard   -card deck, there are twenty-six red cards and four kings, two of which are red, so the probability of drawing a red or a king is   /   +  /   –  /   =   /  . events are collectively exhaustive if all the possibilities for outcomes are exhausted by those possible events, so at least one of those outcomes must occur. the probability that at least one of the events will occur is equal to one. for example, there are theoretically only two possibilities for flipping a coin. flipping a head and flipping a tail are collectively exhaustive events, and there is a probability of one of flipping either a head or a tail. events can be both mutually exclusive and collectively exhaustive. in the case of flipping a coin, flipping a head and flipping a tail are also mutually exclusive events. both outcomes cannot occur for a single trial (i.e., when a coin is flipped only once). the probability of flipping a head and the probability of flipping a tail can be added to yield a probability of  :  /  +  /  = . in statistics and regression analysis, an independent variable that can take on only two possible values is called a dummy variable. for example, it may take on the value   if an observation is of a white subject or   if the observation is of a black subject. the two possible categories associated with the two possible values are mutually exclusive, so that no observation falls into more than one category, and the categories are exhaustive, so that every observation falls into some category. sometimes there are three or more possible categories, which are pairwise mutually exclusive and are collectively exhaustive — for example, under    years of age,    to    years of age, and age    or above. in this case a set of dummy variables is constructed, each dummy variable having two mutually exclusive and jointly exhaustive categories — in this example, one dummy variable (called d ) would equal   if age is less than   , and would equal   otherwise; a second dummy variable (called d ) would equal   if age is in the range   -  , and   otherwise. in this set-up, the dummy variable pairs (d , d ) can have the values ( , ) (under   ), ( , ) (between    and   ), or ( , ) (   or older) (but not ( , ), which would nonsensically imply that an observed subject is both under    and between    and   ). then the dummy variables can be included as independent (explanatory) variables in a regression. note that the number of dummy variables is always one less than the number of categories: with the two categories black and white there is a single dummy variable to distinguish them, while with the three age categories two dummy variables are needed to distinguish them. such qualitative data can also be used for dependent variables. for example, a researcher might want to predict whether someone gets arrested or not, using family income or race, as explanatory variables. here the variable to be explained is a dummy variable that equals   if the observed subject does not get arrested and equals   if the subject does get arrested. in such a situation, ordinary least squares (the basic regression technique) is widely seen as inadequate; instead probit regression or logistic regression is used. further, sometimes there are three or more categories for the dependent variable — for example, no charges, charges, and death sentences. in this case, the multinomial probit or multinomial logit technique is used. "
305,305,Natural logarithm,2,https://en.wikipedia.org/wiki/Natural_logarithm," the natural logarithm of a number is its logarithm to the base of the mathematical constant e, which is an irrational and transcendental number approximately equal to  .            . the natural logarithm of x is generally written as ln x, loge x, or sometimes, if the base e is implicit, simply log x. parentheses are sometimes added for clarity, giving ln(x), loge(x), or log(x). this is done particularly when the argument to the logarithm is not a single symbol, so as to prevent ambiguity. the natural logarithm of x is the power to which e would have to be raised to equal x. for example, ln  .  is  .    ..., because e .    ... =  . . the natural logarithm of e itself, ln e, is  , because e  = e, while the natural logarithm of   is  , since e  =  . the natural logarithm can be defined for any positive real number a as the area under the curve y =  /x from   to a (with the area being negative when   < a <  ). the simplicity of this definition, which is matched in many other formulas involving the natural logarithm, leads to the term ""natural"". the definition of the natural logarithm can then be extended to give logarithm values for negative numbers and for all non-zero complex numbers, although this leads to a multi-valued function: see complex logarithm for more. the natural logarithm function, if considered as a real-valued function of a real variable, is the inverse function of the exponential function, leading to the identities: like all logarithms, the natural logarithm maps multiplication of positive numbers into addition: logarithms can be defined for any positive base other than  , not only e. however, logarithms in other bases differ only by a constant multiplier from the natural logarithm, and can be defined in terms of the latter. for instance, the base-  logarithm (also called the binary logarithm) is equal to the natural logarithm divided by ln( ), the natural logarithm of  , or equivalently, multiplied by log (e). logarithms are useful for solving equations in which the unknown appears as the exponent of some other quantity. for example, logarithms are used to solve for the half-life, decay constant, or unknown time in exponential decay problems. they are important in many branches of mathematics and scientific disciplines, and are used to solve problems involving compound interest. the concept of the natural logarithm was worked out by gregoire de saint-vincent and alphonse antonio de sarasa before     . their work involved quadrature of the hyperbola with equation xy =  , by determination of the area of hyperbolic sectors. their solution generated the requisite ""hyperbolic logarithm"" function, which had the properties now associated with the natural logarithm. an early mention of the natural logarithm was by nicholas mercator in his work logarithmotechnia, published in     , although the mathematics teacher john speidell had already compiled a table of what in fact were effectively natural logarithms in     . it has been said that speidell's logarithms were to the base e, but this is not entirely true due to complications with the values being expressed as integers. :     "
306,306,Natural Number,1,https://en.wikipedia.org/wiki/Natural_number," in mathematics, the natural numbers are those numbers used for counting (as in ""there are six coins on the table"") and ordering (as in ""this is the third largest city in the country""). in common language, numbers used for counting colloquially called ""cardinal numbers"", and words used for ordering are called ""ordinal numbers"". the natural numbers can, at times, appear as a convenient set of codes (labels or ""names""), that is, as what linguists call nominal numbers, forgoing many or all of the properties of being a number in a mathematical sense. some definitions, including the standard iso      - , [a] begin the natural numbers with  , corresponding to the non-negative integers  ,  ,  ,  , ..., whereas others start with  , corresponding to the positive integers  ,  ,  , ... [b] texts that exclude zero from the natural numbers sometimes refer to the natural numbers together with zero as the whole numbers, while in other writings, that term is used instead for the integers (including negative integers). the natural numbers are a basis from which many other number sets may be built by extension: the integers, by including (if not yet in) the neutral element   and an additive inverse (−n) for each nonzero natural number n; the rational numbers, by including a multiplicative inverse (   n {\displaystyle {\tfrac { }{n}}} ) for each nonzero integer n (and also the product of these inverses by integers); the real numbers by including with the rationals the limits of (converging) cauchy sequences of rationals; the complex numbers, by including with the real numbers the unresolved square root of minus one (and also the sums and products thereof); and so on.[c][d] this chain of extensions make the natural numbers canonically embedded (identified) in the other number systems. properties of the natural numbers, such as divisibility and the distribution of prime numbers, are studied in number theory. problems concerning counting and ordering, such as partitioning and enumerations, are studied in combinatorics. in common language, particularly in primary school education, natural numbers may be called counting numbers to intuitively exclude the negative integers and zero, and also to contrast the discreteness of counting to the continuity of measurement—a hallmark characteristic of real numbers. the most primitive method of representing a natural number is to put down a mark for each object. later, a set of objects could be tested for equality, excess or shortage—by striking out a mark and removing an object from the set. the first major advance in abstraction was the use of numerals to represent numbers. this allowed systems to be developed for recording large numbers. the ancient egyptians developed a powerful system of numerals with distinct hieroglyphs for  ,   , and all powers of    up to over   million. a stone carving from karnak, dating back from around      bce and now at the louvre in paris, depicts     as   hundreds,   tens, and   ones; and similarly for the number  ,   . the babylonians had a place-value system based essentially on the numerals for   and   , using base sixty, so that the symbol for sixty was the same as the symbol for one—its value being determined from context. a much later advance was the development of the idea that   can be considered as a number, with its own numeral. the use of a   digit in place-value notation (within other numbers) dates back as early as     bce by the babylonians, who omitted such a digit when it would have been the last symbol in the number.[e] the olmec and maya civilizations used   as a separate number as early as the  st century bce, but this usage did not spread beyond mesoamerica. the use of a numeral   in modern times originated with the indian mathematician brahmagupta in     ce. however,   had been used as a number in the medieval computus (the calculation of the date of easter), beginning with dionysius exiguus in     ce, without being denoted by a numeral (standard roman numerals do not have a symbol for  ). instead, nulla (or the genitive form nullae) from nullus, the latin word for ""none"", was employed to denote a   value. the first systematic study of numbers as abstractions is usually credited to the greek philosophers pythagoras and archimedes. some greek mathematicians treated the number   differently than larger numbers, sometimes even not as a number at all.[f] euclid, for example, defined a unit first and then a number as a multitude of units, thus by his definition, a unit is not a number and there are no unique numbers (e.g., any two units from indefinitely many units is a  ). "
307,307,Negative Binomial Distribution,2,https://en.wikipedia.org/wiki/Negative_binomial_distribution,"in probability theory and statistics, the negative binomial distribution is a discrete probability distribution that models the number of successes in a sequence of independent and identically distributed bernoulli trials before a specified (non-random) number of failures (denoted r) occurs. for example, we can define rolling a   on a dice as a failure, and rolling any other number as a success, and ask how many successful rolls will occur before we see the third failure (r =  ). in such a case, the probability distribution of the number of non- s that appear will be a negative binomial distribution. we could similarly use the negative binomial distribution to model the number of days a certain machine works before it breaks down (r =  ). ""success"" and ""failure"" are arbitrary terms that are sometimes swapped. we could just as easily say that the negative binomial distribution is the distribution of the number of failures before r successes. when applied to real-world problems, outcomes of success and failure may or may not be outcomes we ordinarily view as good and bad, respectively. this article is inconsistent in its use of these terms, so the reader should be careful to identify which outcome can vary in number of occurrences and which outcome stops the sequence of trials. the article may also use p (the probability of one of the outcomes in any given bernoulli trial) inconsistently. the pascal distribution (after blaise pascal) and polya distribution (for george pólya) are special cases of the negative binomial distribution. a convention among engineers, climatologists, and others is to use ""negative binomial"" or ""pascal"" for the case of an integer-valued stopping-time parameter r, and use ""polya"" for the real-valued case. for occurrences of associated discrete events, like tornado outbreaks, the polya distributions can be used to give more accurate models than the poisson distribution by allowing the mean and variance to be different, unlike the poisson. the negative binomial distribution has a variance μ (   + μ / r ) {\displaystyle \mu ( +\mu /r)} , with the distribution becoming identical to poisson in the limit r → ∞ {\displaystyle r\to \infty } for a given mean μ {\displaystyle \mu } . this can make the distribution a useful overdispersed alternative to the poisson distribution, for example for a robust modification of poisson regression. in epidemiology it has been used to model disease transmission for infectious diseases where the likely number of onward infections may vary considerably from individual to individual and from setting to setting. more generally it may be appropriate where events have positively correlated occurrences causing a larger variance than if the occurrences were independent, due to a positive covariance term. the term ""negative binomial"" is likely due to the fact that a certain binomial coefficient that appears in the formula for the probability mass function of the distribution can be written more simply with negative numbers. unlike the introduction and infobox, this section uses ""success"" to refer to the limiting outcome. in other words, this section deals with a sequence of trials that stops after r successes. this terminology may be inconsistent with other parts of the article. suppose there is a sequence of independent bernoulli trials. each trial has two potential outcomes called ""success"" and ""failure"". in each trial the probability of success is p and of failure is (  − p). we observe this sequence until a predefined number r of successes have occurred. then the random number of failures we have seen, x, will have the negative binomial (or pascal) distribution: the probability mass function of the negative binomial distribution is[clarification needed] where r is the number of successes, k is the number of failures, and p is the probability of success. note that this formulation is an alternative formulation to the sidebar; in this formulation, the mean is (   − p ) r / p {\displaystyle {( -p)r}/{p}} and the variance is (   − p ) r / p   {\displaystyle {( -p)r}/{p^{ }}} . here, the quantity in parentheses is the binomial coefficient, and is equal to "
308,308,negative hypergeometric distribution,0,https://en.wikipedia.org/wiki/Negative_hypergeometric_distribution," n ∈ {   ,   ,   , … } {\displaystyle n\in \left\{ , , ,\dots \right\}} - total number of elements k ∈ {   ,   ,   , … , n } {\displaystyle k\in \left\{ , , ,\dots ,n\right\}} - total number of 'success' elements in probability theory and statistics, the negative hypergeometric distribution describes probabilities for when sampling from a finite population without replacement in which each sample can be classified into two mutually exclusive categories like pass/fail, male/female or employed/unemployed. as random selections are made from the population, each subsequent draw decreases the population causing the probability of success to change with each draw. unlike the standard hypergeometric distribution, which describes the number of successes in a fixed sample size, in the negative hypergeometric distribution, samples are drawn until r {\displaystyle r} failures have been found, and the distribution describes the probability of finding k {\displaystyle k} successes in such a sample. in other words, the negative hypergeometric distribution describes the likelihood of k {\displaystyle k} successes in a sample with exactly r {\displaystyle r} failures. there are n {\displaystyle n} elements, of which k {\displaystyle k} are defined as ""successes"" and the rest are ""failures"". elements are drawn one after the other, without replacements, until r {\displaystyle r} failures are encountered. then, the drawing stops and the number k {\displaystyle k} of successes is counted. the negative hypergeometric distribution, n h g n , k , r ( k ) {\displaystyle nhg_{n,k,r}(k)} is the discrete distribution of this k {\displaystyle k} . the negative hypergeometric distribution is a special case of the beta-binomial distribution with parameters α = r {\displaystyle \alpha =r} and β = n − k − r +   {\displaystyle \beta =n-k-r+ } both being integers (and n = k {\displaystyle n=k} ). the outcome requires that we observe k {\displaystyle k} successes in ( k + r −   ) {\displaystyle (k+r- )} draws and the ( k + r ) -th {\displaystyle (k+r){\text{-th}}} bit must be a failure. the probability of the former can be found by the direct application of the hypergeometric distribution ( h g n , k , k + r −   ( k ) ) {\displaystyle (hg_{n,k,k+r- }(k))} and the probability of the latter is simply the number of failures remaining ( = n − k − ( r −   ) ) {\displaystyle (=n-k-(r- ))} divided by the size of the remaining population ( = n − ( k + r −   ) {\displaystyle (=n-(k+r- )} . the probability of having exactly k {\displaystyle k} successes up to the r -th {\displaystyle r{\text{-th}}} failure (i.e. the drawing stops as soon as the sample includes the predefined number of r {\displaystyle r} failures) is then the product of these two probabilities: ( k k ) ( n − k k + r −   − k ) ( n k + r −   ) ⋅ n − k − ( r −   ) n − ( k + r −   ) = ( k + r −   k ) ( n − r − k k − k ) ( n k ) . {\displaystyle {\frac {{\binom {k}{k}}{\binom {n-k}{k+r- -k}}}{\binom {n}{k+r- }}}\cdot {\frac {n-k-(r- )}{n-(k+r- )}}={\frac {{{k+r- } \choose {k}}{{n-r-k} \choose {k-k}}}{n \choose k}}.} therefore, a random variable follows the negative hypergeometric distribution if its probability mass function (pmf) is given by f ( k ; n , k , r ) ≡ pr ( x = k ) = ( k + r −   k ) ( n − r − k k − k ) ( n k ) for k =   ,   ,   , … , k {\displaystyle f(k;n,k,r)\equiv \pr(x=k)={\frac {{{k+r- } \choose {k}}{{n-r-k} \choose {k-k}}}{n \choose k}}\quad {\text{for }}k= , , ,\dotsc ,k} "
309,309,Newton's laws of motion,1,https://en.wikipedia.org/wiki/Newton%27s_laws_of_motion," newton's laws of motion are three laws of classical mechanics that describe the relationship between the motion of an object and the forces acting on it. these laws can be paraphrased as follows: law  . a body remains at rest, or in motion at a constant speed in a straight line, unless acted upon by a force. law  . when a body is acted upon by a force, the time rate of change of its momentum equals the force. law  . if two bodies exert forces on each other, these forces have the same magnitude but opposite directions. the three laws of motion were first stated by isaac newton in his philosophiæ naturalis principia mathematica (mathematical principles of natural philosophy), first published in     . newton used them to explain and investigate the motion of many physical objects and systems, which laid the foundation for newtonian mechanics. newton's first law, also called the ""law of inertia"", states that an object at rest remains at rest, and an object that is moving will continue to move straight and with constant velocity, if and only if there is no net force acting on that object. :     if any number of different external forces f   , f   , … {\displaystyle \mathbf {f} _{ },\mathbf {f} _{ },\ldots } are being applied to an object, then the net force f net {\displaystyle f_{\text{net}}} is the vector sum of those forces, so f net = f   + f   + ⋯ {\displaystyle f_{\text{net}}=\mathbf {f} _{ }+\mathbf {f} _{ }+\cdots } . if that net force is zero, then the object's velocity must not be changing. conversely, if the object's velocity is not changing, then it must have a net force of zero. :     mathematically, "
310,310,Nine Point Circle,3,https://en.wikipedia.org/wiki/Nine-point_circle,"in geometry, the nine-point circle is a circle that can be constructed for any given triangle. it is so named because it passes through nine significant concyclic points defined from the triangle. these nine points are: the nine-point circle is also known as feuerbach's circle, euler's circle, terquem's circle, the six-points circle, the twelve-points circle, the n-point circle, the medioscribed circle, the mid circle or the circum-midcircle. its center is the nine-point center of the triangle. the diagram above shows the nine significant points of the nine-point circle. points d, e, and f are the midpoints of the three sides of the triangle. points g, h, and i are the feet of the altitudes of the triangle. points j, k, and l are the midpoints of the line segments between each altitude's vertex intersection (points a, b, and c) and the triangle's orthocenter (point s). for an acute triangle, six of the points (the midpoints and altitude feet) lie on the triangle itself; for an obtuse triangle two of the altitudes have feet outside the triangle, but these feet still belong to the nine-point circle. although he is credited for its discovery, karl wilhelm feuerbach did not entirely discover the nine-point circle, but rather the six-point circle, recognizing the significance of the midpoints of the three sides of the triangle and the feet of the altitudes of that triangle. (see fig.  , points d, e, f, g, h, and i.) (at a slightly earlier date, charles brianchon and jean-victor poncelet had stated and proven the same theorem.) but soon after feuerbach, mathematician olry terquem himself proved the existence of the circle. he was the first to recognize the added significance of the three midpoints between the triangle's vertices and the orthocenter. (see fig.  , points j, k, and l.) thus, terquem was the first to use the name nine-point circle. in      karl feuerbach discovered that any triangle's nine-point circle is externally tangent to that triangle's three excircles and internally tangent to its incircle; this result is known as feuerbach's theorem. he proved that: the triangle center at which the incircle and the nine-point circle touch is called the feuerbach point. figure   figure   "
311,311,Nonprobability Sampling,1,https://en.wikipedia.org/wiki/Nonprobability_sampling,"sampling is the use of a subset of the population to represent the whole population or to inform about (social) processes that are meaningful beyond the particular cases, individuals or sites studied. probability sampling, or random sampling, is a sampling technique in which the probability of getting any particular sample may be calculated. in cases where external validity is not of critical importance to the study's goals or purpose, researchers might prefer to use nonprobability sampling. nonprobability sampling does not meet this criterion. nonprobability sampling techniques are not intended to be used to infer from the sample to the general population in statistical terms. instead, for example, grounded theory can be produced through iterative nonprobability sampling until theoretical saturation is reached (strauss and corbin,     ). thus, one cannot say the same on the basis of a nonprobability sample than on the basis of a probability sample. the grounds for drawing generalizations (e.g., propose new theory, propose policy) from studies based on nonprobability samples are based on the notion of ""theoretical saturation"" and ""analytical generalization"" (yin,     ) instead of on statistical generalization. researchers working with the notion of purposive sampling assert that while probability methods are suitable for large-scale studies concerned with representativeness, nonprobability approaches are more suitable for in-depth qualitative research in which the focus is often to understand complex social phenomena (e.g., marshall     ; small     ). one of the advantages of nonprobability sampling is its lower cost compared to probability sampling. moreover, the in-depth analysis of a small-n purposive sample or a case study enables the ""discovery"" and identification of patterns and causal mechanisms that do not draw time and context-free assumptions. nonprobability sampling is often not appropriate in statistical quantitative research, though, as these assertions raise some questions — how can one understand a complex social phenomenon by drawing only the most convenient expressions of that phenomenon into consideration? what assumption about homogeneity in the world must one make to justify such assertions? alas, the consideration that research can only be based in statistical inference focuses on the problems of bias linked to nonprobability sampling and acknowledges only one situation in which a nonprobability sample can be appropriate — if one is interested only in the specific cases studied (for example, if one is interested in the battle of gettysburg), one does not need to draw a probability sample from similar cases (lucas     a). nonprobability sampling is however widely used in qualitative research. examples of nonprobability sampling include: quota sampling- this is similar to stratified random sampling, in which the researcher identifies subsets of the population of interest and then sets a target number for each category in the sample. next, the researcher samples from the population of interest nonrandomly until the quotas are filled. nonprobability sampling should not intend to obtain the same types of results or be held to the same quality standards as those of probability sampling (steinke,     ). studies intended to use probability sampling sometimes end up using nonprobability samples because of characteristics of the sampling method. for example, using a sample of people in the paid labor force to analyze the effect of education on earnings is to use a nonprobability sample of persons who could be in the paid labor force. because the education people obtain could determine their likelihood of being in the paid labor force, the sample in the paid labor force is a nonprobability sample for the question at issue. in such cases results are biased. the statistical model one uses can also render the data a nonprobability sample. for example, lucas (    b) notes that several published studies that use multilevel modeling have been based on samples that are probability samples in general, but nonprobability samples for one or more of the levels of analysis in the study. evidence indicates that in such cases the bias is poorly behaved, such that inferences from such analyses are unjustified. these problems occur in the academic literature, but they may be more common in non-academic research. for example, in public opinion polling by private companies (or other organizations unable to require response), the sample can be self-selected rather than random. this often introduces an important type of error, self-selection bias, in which a potential participant's willingness to volunteer for the sample may be determined by characteristics such as submissiveness or availability. the samples in such surveys should be treated as nonprobability samples of the population, and the validity of the findings based on them is unknown and cannot be established. "
312,312,Norm,0,https://en.wikipedia.org/wiki/Norm_(mathematics),"in mathematics, a norm is a function from a real or complex vector space to the non-negative real numbers that behaves in certain ways like the distance from the origin: it commutes with scaling, obeys a form of the triangle inequality, and is zero only at the origin. in particular, the euclidean distance of a vector from the origin is a norm, called the euclidean norm, or  -norm, which may also be defined as the square root of the inner product of a vector with itself. a pseudonorm or seminorm satisfies the first two properties of a norm, but may be zero for vectors other than the origin. a vector space with a specified norm is called a normed vector space. in a similar manner, a vector space with a seminorm is called a seminormed vector space. given a vector space x {\displaystyle x} over a subfield f of the complex numbers c , {\displaystyle \mathbb {c} ,} a norm on x {\displaystyle x} is a real-valued function p : x → r {\displaystyle p:x\to \mathbb {r} } with the following properties, where | s | {\displaystyle |s|} denotes the usual absolute value of a scalar s {\displaystyle s} : a seminorm on x {\displaystyle x} is a function p : x → r {\displaystyle p:x\to \mathbb {r} } that has properties ( ) and ( ) so that in particular, every norm is also a seminorm (and thus also a sublinear functional). however, there exist seminorms that are not norms. properties ( ) and ( ) imply that if p {\displaystyle p} is a norm (or more generally, a seminorm) then p (   ) =   {\displaystyle p( )= } and that p {\displaystyle p} also has the following property: some authors include non-negativity as part of the definition of ""norm"", although this is not necessary. suppose that p and q are two norms (or seminorms) on a vector space x . {\displaystyle x.} then p and q are called equivalent, if there exist two real constants c and c with c >   such that for every vector x ∈ x , {\displaystyle x\in x,} if a norm p : x → r {\displaystyle p\colon x\to \mathbb {r} } is given on a vector space x, then the norm of a vector z ∈ x {\displaystyle z\in x} is usually denoted by enclosing it within double vertical lines: ‖ z ‖ = p ( z ) . {\displaystyle \|z\|=p(z).} such notation is also sometimes used if p is only a seminorm. for the length of a vector in euclidean space (which is an example of a norm, as explained below), the notation | x | {\displaystyle |x|} with single vertical lines is also widespread. in latex and related markup languages, the double bar of norm notation is entered with the macro \|, which renders as ‖ . {\displaystyle \|.} the double vertical line used to denote parallel lines, parallel operator and parallel addition is entered with \parallel and is rendered as ∥ . {\displaystyle \parallel .} although looking similar, these two macros must not be confused as \| denotes a bracket and \parallel denotes an operator. therefore, their size and the spaces around them are not computed in the same way. similarly, the single vertical bar is coded as | when used as a bracket, and as \mid when used as an operator. in unicode, the representation of the ""double vertical line"" character is u+     ‖ double vertical line. the ""double vertical line"" symbol should not be confused with the ""parallel to"" symbol, u+     ∥ parallel to, which is intended to denote parallel lines and parallel operators. the double vertical line should also not be confused with u+  c  ǁ latin letter lateral click, aimed to denote lateral clicks in linguistics. the single vertical line | has a unicode representation u+   c | vertical line. "
313,313,normal vector,1,https://en.wikipedia.org/wiki/Normal_(geometry),"in geometry, a normal is an object such as a line, ray, or vector that is perpendicular to a given object. for example, the normal line to a plane curve at a given point is the (infinite) line perpendicular to the tangent line to the curve at the point. a normal vector may have length one (a unit vector) or its length may represent the curvature of the object (a curvature vector); its algebraic sign may indicate sides (interior or exterior). in three dimensions, a surface normal, or simply normal, to a surface at point p {\displaystyle p} is a vector perpendicular to the tangent plane of the surface at p. the word ""normal"" is also used as an adjective: a line normal to a plane, the normal component of a force, the normal vector, etc. the concept of normality generalizes to orthogonality (right angles). the concept has been generalized to differentiable manifolds of arbitrary dimension embedded in a euclidean space. the normal vector space or normal space of a manifold at point p {\displaystyle p} is the set of vectors which are orthogonal to the tangent space at p . {\displaystyle p.} normal vectors are of special interest in the case of smooth curves and smooth surfaces. the normal is often used in  d computer graphics (notice the singular, as only one normal will be defined) to determine a surface's orientation toward a light source for flat shading, or the orientation of each of the surface's corners (vertices) to mimic a curved surface with phong shading. the normal distance of a point q to a curve or to a surface is the euclidean distance between q and its perpendicular projection on the object (at the point p on the object where the normal contains q). the normal distance is a type of perpendicular distance generalizing the distance from a point to a line and the distance from a point to a plane. it can be used for curve fitting and for defining offset surfaces. for a convex polygon (such as a triangle), a surface normal can be calculated as the vector cross product of two (non-parallel) edges of the polygon. for a plane given by the equation a x + b y + c z + d =   , {\displaystyle ax+by+cz+d= ,} the vector n = ( a , b , c ) {\displaystyle \mathbf {n} =(a,b,c)} is a normal. for a plane whose equation is given in parametric form if a (possibly non-flat) surface s {\displaystyle s} in  -space r   {\displaystyle \mathbb {r} ^{ }} is parameterized by a system of curvilinear coordinates r ( s , t ) = ( x ( s , t ) , y ( s , t ) , z ( s , t ) ) , {\displaystyle \mathbf {r} (s,t)=(x(s,t),y(s,t),z(s,t)),} with s {\displaystyle s} and t {\displaystyle t} real variables, then a normal to s is by definition a normal to a tangent plane, given by the cross product of the partial derivatives if a surface s {\displaystyle s} is given implicitly as the set of points ( x , y , z ) {\displaystyle (x,y,z)} satisfying f ( x , y , z ) =   , {\displaystyle f(x,y,z)= ,} then a normal at a point ( x , y , z ) {\displaystyle (x,y,z)} on the surface is given by the gradient "
314,314,Normal Distribution,0,https://en.wikipedia.org/wiki/Normal_distribution," i ( μ , σ ) = (   / σ         / σ   ) {\displaystyle {\mathcal {i}}(\mu ,\sigma )={\begin{pmatrix} /\sigma ^{ }& \\ & /\sigma ^{ }\end{pmatrix}}} a normal distribution is a probability distribution used to model phenomena that have a default behaviour and cumulative possible deviations from that behaviour. for instance, a proficient archer's arrows are expected to land around the bull's eye of the target; however, due to aggregating imperfections in the archer's technique, most arrows will miss the bull's eye by some distance. the average of this distance is known in archery as accuracy, while the amount of variation in the distances as precision. in the context of a normal distribution, accuracy and precision are referred to as the mean and the standard deviation, respectively. thus, a narrow measure of an archer's proficiency can be expressed with two values: a mean and a standard deviation. in a normal distribution, these two values mean: there is a ~  % probability that an arrow will land within one standard deviation of the archer's average accuracy; a ~  % probability that an arrow will land within two standard deviations of the archer's average accuracy; ~  . % within three; and so on, slowly increasing towards    %. more rigorously, in probability theory, a normal distribution (also known as gaussian, gauss, or laplace–gauss distribution) is a type of continuous probability distribution for a real-valued random variable. the general form of its probability density function is the parameter μ {\displaystyle \mu } is the mean or expectation of the distribution (and also its median and mode), while the parameter σ {\displaystyle \sigma } is its standard deviation. the variance of the distribution is σ   {\displaystyle \sigma ^{ }} . a random variable with a gaussian distribution is said to be normally distributed, and is called a normal deviate. normal distributions are important in statistics and are often used in the natural and social sciences to represent real-valued random variables whose distributions are not known. their importance is partly due to the central limit theorem. it states that, under some conditions, the average of many samples (observations) of a random variable with finite mean and variance is itself a random variable—whose distribution converges to a normal distribution as the number of samples increases. therefore, physical quantities that are expected to be the sum of many independent processes, such as measurement errors, often have distributions that are nearly normal. moreover, gaussian distributions have some unique properties that are valuable in analytic studies. for instance, any linear combination of a fixed collection of normal deviates is a normal deviate. many results and methods, such as propagation of uncertainty and least squares parameter fitting, can be derived analytically in explicit form when the relevant variables are normally distributed. a normal distribution is sometimes informally called a bell curve. however, many other distributions are bell-shaped (such as the cauchy, student's t, and logistic distributions). the univariate probability distribution is generalized for vectors in the multivariate normal distribution and for matrices in the matrix normal distribution. the simplest case of a normal distribution is known as the standard normal distribution or unit normal distribution. this is a special case when μ =   {\displaystyle \mu = } and σ =   {\displaystyle \sigma = } , and it is described by this probability density function (or density): "
315,315,Number Theory,2,https://en.wikipedia.org/wiki/Number_theory,"number theory (or arithmetic or higher arithmetic in older usage) is a branch of pure mathematics devoted primarily to the study of the integers and integer-valued functions. german mathematician carl friedrich gauss (    –    ) said, ""mathematics is the queen of the sciences—and number theory is the queen of mathematics."" [note  ] number theorists study prime numbers as well as the properties of mathematical objects made out of integers (for example, rational numbers) or defined as generalizations of the integers (for example, algebraic integers). integers can be considered either in themselves or as solutions to equations (diophantine geometry). questions in number theory are often best understood through the study of analytical objects (for example, the riemann zeta function) that encode properties of the integers, primes or other number-theoretic objects in some fashion (analytic number theory). one may also study real numbers in relation to rational numbers, for example, as approximated by the latter (diophantine approximation). the older term for number theory is arithmetic. by the early twentieth century, it had been superseded by ""number theory"".[note  ] (the word ""arithmetic"" is used by the general public to mean ""elementary calculations""; it has also acquired other meanings in mathematical logic, as in peano arithmetic, and computer science, as in floating-point arithmetic.) the use of the term arithmetic for number theory regained some ground in the second half of the   th century, arguably in part due to french influence.[note  ] in particular, arithmetical is commonly preferred as an adjective to number-theoretic. the earliest historical find of an arithmetical nature is a fragment of a table: the broken clay tablet plimpton     (larsa, mesopotamia, ca.      bc) contains a list of ""pythagorean triples"", that is, integers ( a , b , c ) {\displaystyle (a,b,c)} such that a   + b   = c   {\displaystyle a^{ }+b^{ }=c^{ }} . the triples are too many and too large to have been obtained by brute force. the heading over the first column reads: ""the takiltum of the diagonal which has been subtracted such that the width..."" the table's layout suggests that it was constructed by means of what amounts, in modern language, to the identity which is implicit in routine old babylonian exercises. if some other method was used, the triples were first constructed and then reordered by c / a {\displaystyle c/a} , presumably for actual use as a ""table"", for example, with a view to applications. it is not known what these applications may have been, or whether there could have been any; babylonian astronomy, for example, truly came into its own only later. it has been suggested instead that the table was a source of numerical examples for school problems. [note  ] while babylonian number theory—or what survives of babylonian mathematics that can be called thus—consists of this single, striking fragment, babylonian algebra (in the secondary-school sense of ""algebra"") was exceptionally well developed. late neoplatonic sources state that pythagoras learned mathematics from the babylonians. much earlier sources state that thales and pythagoras traveled and studied in egypt. euclid ix   –   is very probably pythagorean; it is very simple material (""odd times even is even"", ""if an odd number measures [= divides] an even number, then it also measures [= divides] half of it""), but it is all that is needed to prove that   {\displaystyle {\sqrt { }}} is irrational. pythagorean mystics gave great importance to the odd and the even. the discovery that   {\displaystyle {\sqrt { }}} is irrational is credited to the early pythagoreans (pre-theodorus). by revealing (in modern terms) that numbers could be irrational, this discovery seems to have provoked the first foundational crisis in mathematical history; its proof or its divulgation are sometimes credited to hippasus, who was expelled or split from the pythagorean sect. this forced a distinction between numbers (integers and the rationals—the subjects of arithmetic), on the one hand, and lengths and proportions (which we would identify with real numbers, whether rational or not), on the other hand. the pythagorean tradition spoke also of so-called polygonal or figurate numbers. while square numbers, cubic numbers, etc., are seen now as more natural than triangular numbers, pentagonal numbers, etc., the study of the sums of triangular and pentagonal numbers would prove fruitful in the early modern period (  th to early   th century). "
316,316,Ordered Pair,1,https://en.wikipedia.org/wiki/Ordered_pair,"in mathematics, an ordered pair (a, b) is a pair of objects. the order in which the objects appear in the pair is significant: the ordered pair (a, b) is different from the ordered pair (b, a) unless a = b. (in contrast, the unordered pair {a, b} equals the unordered pair {b, a}.) ordered pairs are also called  -tuples, or sequences (sometimes, lists in a computer science context) of length  . ordered pairs of scalars are sometimes called  -dimensional vectors. (technically, this is an abuse of terminology since an ordered pair need not be an element of a vector space.) the entries of an ordered pair can be other ordered pairs, enabling the recursive definition of ordered n-tuples (ordered lists of n objects). for example, the ordered triple (a,b,c) can be defined as (a, (b,c)), i.e., as one pair nested in another. in the ordered pair (a, b), the object a is called the first entry, and the object b the second entry of the pair. alternatively, the objects are called the first and second components, the first and second coordinates, or the left and right projections of the ordered pair. cartesian products and binary relations (and hence functions) are defined in terms of ordered pairs. let ( a   , b   ) {\displaystyle (a_{ },b_{ })} and ( a   , b   ) {\displaystyle (a_{ },b_{ })} be ordered pairs. then the characteristic (or defining) property of the ordered pair is: the set of all ordered pairs whose first entry is in some set a and whose second entry is in some set b is called the cartesian product of a and b, and written a × b. a binary relation between sets a and b is a subset of a × b. the (a, b) notation may be used for other purposes, most notably as denoting open intervals on the real number line. in such situations, the context will usually make it clear which meaning is intended. for additional clarification, the ordered pair may be denoted by the variant notation ⟨ a , b ⟩ {\textstyle \langle a,b\rangle } , but this notation also has other uses. the left and right projection of a pair p is usually denoted by π (p) and π (p), or by πℓ(p) and πr(p), respectively. in contexts where arbitrary n-tuples are considered, πni(t) is a common notation for the i-th component of an n-tuple t. in some introductory mathematics textbooks an informal (or intuitive) definition of ordered pair is given, such as for any two objects a and b, the ordered pair (a, b) is a notation specifying the two objects a and b, in that order. "
317,317,Formation of ordinary differential equations,0,https://en.wikipedia.org/wiki/Ordinary_differential_equation,"in mathematics, an ordinary differential equation (ode) is a differential equation containing one or more functions of one independent variable and the derivatives of those functions. the term ordinary is used in contrast with the term partial differential equation which may be with respect to more than one independent variable. a linear differential equation is a differential equation that is defined by a linear polynomial in the unknown function and its derivatives, that is an equation of the form where a   ( x ) {\displaystyle a_{ }(x)} , ..., a n ( x ) {\displaystyle a_{n}(x)} and b ( x ) {\displaystyle b(x)} are arbitrary differentiable functions that do not need to be linear, and y ′ , … , y ( n ) {\displaystyle y',\ldots ,y^{(n)}} are the successive derivatives of the unknown function y of the variable x. among ordinary differential equations, linear differential equations play a prominent role for several reasons. most elementary and special functions that are encountered in physics and applied mathematics are solutions of linear differential equations (see holonomic function). when physical phenomena are modeled with non-linear equations, they are generally approximated by linear differential equations for an easier solution. the few non-linear odes that can be solved explicitly are generally solved by transforming the equation into an equivalent linear ode (see, for example riccati equation). some odes can be solved explicitly in terms of known functions and integrals. when that is not possible, the equation for computing the taylor series of the solutions may be useful. for applied problems, numerical methods for ordinary differential equations can supply an approximation of the solution. ordinary differential equations (odes) arise in many contexts of mathematics and social and natural sciences. mathematical descriptions of change use differentials and derivatives. various differentials, derivatives, and functions become related via equations, such that a differential equation is a result that describes dynamically changing phenomena, evolution, and variation. often, quantities are defined as the rate of change of other quantities (for example, derivatives of displacement with respect to time), or gradients of quantities, which is how they enter differential equations. specific mathematical fields include geometry and analytical mechanics. scientific fields include much of physics and astronomy (celestial mechanics), meteorology (weather modeling), chemistry (reaction rates), biology (infectious diseases, genetic variation), ecology and population modeling (population competition), economics (stock trends, interest rates and the market equilibrium price changes). many mathematicians have studied differential equations and contributed to the field, including newton, leibniz, the bernoulli family, riccati, clairaut, d'alembert, and euler. a simple example is newton's second law of motion — the relationship between the displacement x and the time t of an object under the force f, is given by the differential equation which constrains the motion of a particle of constant mass m. in general, f is a function of the position x(t) of the particle at time t. the unknown function x(t) appears on both sides of the differential equation, and is indicated in the notation f(x(t)). "
318,318,Orthogonal matrix,1,https://en.wikipedia.org/wiki/Orthogonal_matrix,"in linear algebra, an orthogonal matrix, or orthonormal matrix, is a real square matrix whose columns and rows are orthonormal vectors. one way to express this is this leads to the equivalent characterization: a matrix q is orthogonal if its transpose is equal to its inverse: an orthogonal matrix q is necessarily invertible (with inverse q−  = qt), unitary (q−  = q∗), where q∗ is the hermitian adjoint (conjugate transpose) of q, and therefore normal (q∗q = qq∗) over the real numbers. the determinant of any orthogonal matrix is either +  or − . as a linear transformation, an orthogonal matrix preserves the inner product of vectors, and therefore acts as an isometry of euclidean space, such as a rotation, reflection or rotoreflection. in other words, it is a unitary transformation. the set of n × n orthogonal matrices forms a group, o(n), known as the orthogonal group. the subgroup so(n) consisting of orthogonal matrices with determinant +  is called the special orthogonal group, and each of its elements is a special orthogonal matrix. as a linear transformation, every special orthogonal matrix acts as a rotation. an orthogonal matrix is the real specialization of a unitary matrix, and thus always a normal matrix. although we consider only real matrices here, the definition can be used for matrices with entries from any field. however, orthogonal matrices arise naturally from dot products, and for matrices of complex numbers that leads instead to the unitary requirement. orthogonal matrices preserve the dot product, so, for vectors u and v in an n-dimensional real euclidean space thus finite-dimensional linear isometries—rotations, reflections, and their combinations—produce orthogonal matrices. the converse is also true: orthogonal matrices imply orthogonal transformations. however, linear algebra includes orthogonal transformations between spaces which may be neither finite-dimensional nor of the same dimension, and these have no orthogonal matrix equivalent. orthogonal matrices are important for a number of reasons, both theoretical and practical. the n × n orthogonal matrices form a group under matrix multiplication, the orthogonal group denoted by o(n), which—with its subgroups—is widely used in mathematics and the physical sciences. for example, the point group of a molecule is a subgroup of o( ). because floating point versions of orthogonal matrices have advantageous properties, they are key to many algorithms in numerical linear algebra, such as qr decomposition. as another example, with appropriate normalization the discrete cosine transform (used in mp  compression) is represented by an orthogonal matrix. below are a few examples of small orthogonal matrices and possible interpretations. the simplest orthogonal matrices are the   ×   matrices and [− ], which we can interpret as the identity and a reflection of the real line across the origin. "
319,319,Orthonormal basis,2,https://en.wikipedia.org/wiki/Orthonormal_basis,"in mathematics, particularly linear algebra, an orthonormal basis for an inner product space v with finite dimension is a basis for v {\displaystyle v} whose vectors are orthonormal, that is, they are all unit vectors and orthogonal to each other. for example, the standard basis for a euclidean space r n {\displaystyle \mathbb {r} ^{n}} is an orthonormal basis, where the relevant inner product is the dot product of vectors. the image of the standard basis under a rotation or reflection (or any orthogonal transformation) is also orthonormal, and every orthonormal basis for r n {\displaystyle \mathbb {r} ^{n}} arises in this fashion. for a general inner product space v , {\displaystyle v,} an orthonormal basis can be used to define normalized orthogonal coordinates on v . {\displaystyle v.} under these coordinates, the inner product becomes a dot product of vectors. thus the presence of an orthonormal basis reduces the study of a finite-dimensional inner product space to the study of r n {\displaystyle \mathbb {r} ^{n}} under dot product. every finite-dimensional inner product space has an orthonormal basis, which may be obtained from an arbitrary basis using the gram–schmidt process. in functional analysis, the concept of an orthonormal basis can be generalized to arbitrary (infinite-dimensional) inner product spaces. given a pre-hilbert space h , {\displaystyle h,} an orthonormal basis for h {\displaystyle h} is an orthonormal set of vectors with the property that every vector in h {\displaystyle h} can be written as an infinite linear combination of the vectors in the basis. in this case, the orthonormal basis is sometimes called a hilbert basis for h . {\displaystyle h.} note that an orthonormal basis in this sense is not generally a hamel basis, since infinite linear combinations are required. specifically, the linear span of the basis must be dense in h , {\displaystyle h,} but it may not be the entire space. if we go on to hilbert spaces, a non-orthonormal set of vectors having the same linear span as an orthonormal basis may not be a basis at all. for instance, any square-integrable function on the interval [ −   ,   ] {\displaystyle [- , ]} can be expressed (almost everywhere) as an infinite sum of legendre polynomials (an orthonormal basis), but not necessarily as an infinite sum of the monomials x n . {\displaystyle x^{n}.} if b {\displaystyle b} is an orthogonal basis of h , {\displaystyle h,} then every element x ∈ h {\displaystyle x\in h} may be written as when b {\displaystyle b} is orthonormal, this simplifies to even if b {\displaystyle b} is uncountable, only countably many terms in this sum will be non-zero, and the expression is therefore well-defined. this sum is also called the fourier expansion of x , {\displaystyle x,} and the formula is usually known as parseval's identity. if b {\displaystyle b} is an orthonormal basis of h , {\displaystyle h,} then h {\displaystyle h} is isomorphic to ℓ   ( b ) {\displaystyle \ell ^{ }(b)} in the following sense: there exists a bijective linear map φ : h → ℓ   ( b ) {\displaystyle \phi :h\to \ell ^{ }(b)} such that given a hilbert space h {\displaystyle h} and a set s {\displaystyle s} of mutually orthogonal vectors in h , {\displaystyle h,} we can take the smallest closed linear subspace v {\displaystyle v} of h {\displaystyle h} containing s . {\displaystyle s.} then s {\displaystyle s} will be an orthogonal basis of v ; {\displaystyle v;} which may of course be smaller than h {\displaystyle h} itself, being an incomplete orthogonal set, or be h , {\displaystyle h,} when it is a complete orthogonal set. using zorn's lemma and the gram–schmidt process (or more simply well-ordering and transfinite recursion), one can show that every hilbert space admits an orthonormal basis; furthermore, any two orthonormal bases of the same space have the same cardinality (this can be proven in a manner akin to that of the proof of the usual dimension theorem for vector spaces, with separate cases depending on whether the larger basis candidate is countable or not). a hilbert space is separable if and only if it admits a countable orthonormal basis. (one can prove this last statement without using the axiom of choice.) "
320,320,Orthonormality,2,https://en.wikipedia.org/wiki/Orthonormality,"in linear algebra, two vectors in an inner product space are orthonormal if they are orthogonal (or perpendicular along a line) unit vectors. a set of vectors form an orthonormal set if all vectors in the set are mutually orthogonal and all of unit length. an orthonormal set which forms a basis is called an orthonormal basis. the construction of orthogonality of vectors is motivated by a desire to extend the intuitive notion of perpendicular vectors to higher-dimensional spaces. in the cartesian plane, two vectors are said to be perpendicular if the angle between them is   ° (i.e. if they form a right angle). this definition can be formalized in cartesian space by defining the dot product and specifying that two vectors in the plane are orthogonal if their dot product is zero. similarly, the construction of the norm of a vector is motivated by a desire to extend the intuitive notion of the length of a vector to higher-dimensional spaces. in cartesian space, the norm of a vector is the square root of the vector dotted with itself. that is, many important results in linear algebra deal with collections of two or more orthogonal vectors. but often, it is easier to deal with vectors of unit length. that is, it often simplifies things to only consider vectors whose norm equals  . the notion of restricting orthogonal pairs of vectors to only those of unit length is important enough to be given a special name. two vectors which are orthogonal and of length   are said to be orthonormal. what does a pair of orthonormal vectors in  -d euclidean space look like? let u = (x , y ) and v = (x , y ). consider the restrictions on x , x , y , y  required to make u and v form an orthonormal pair. expanding these terms gives   equations: converting from cartesian to polar coordinates, and considering equation (   ) {\displaystyle ( )} and equation (   ) {\displaystyle ( )} immediately gives the result r  = r  =  . in other words, requiring the vectors be of unit length restricts the vectors to lie on the unit circle. after substitution, equation (   ) {\displaystyle ( )} becomes cos ⁡ θ   cos ⁡ θ   + sin ⁡ θ   sin ⁡ θ   =   {\displaystyle \cos \theta _{ }\cos \theta _{ }+\sin \theta _{ }\sin \theta _{ }= } . rearranging gives tan ⁡ θ   = − cot ⁡ θ   {\displaystyle \tan \theta _{ }=-\cot \theta _{ }} . using a trigonometric identity to convert the cotangent term gives it is clear that in the plane, orthonormal vectors are simply radii of the unit circle whose difference in angles equals   °. "
321,321,Palindrome,2,https://en.wikipedia.org/wiki/Palindrome,"a palindrome is a word, number, phrase, or other sequence of characters which reads the same backward as forward, such as madam or racecar. there are also numeric palindromes, including date/time stamps using short digits   /  /     :   and long digits   /  /    . for example; tuesday,    february      is considered a palindrome day (         using dd-mm-yyyy format) as it can be read from left to right or vice versa. sentence-length palindromes ignore capitalization, punctuation, and word boundaries. composing literature in palindromes is an example of constrained writing. the word palindrome was introduced by henry peacham in     . it is derived from the greek roots πάλιν 'again' and δρóμος 'way, direction'; a different word is used in greek, καρκινικός 'carcinic' (lit. crab-like) to refer to letter-by-letter reversible writing. the ancient greek poet sotades ( rd century bce) invented a form of ionic meter called sotadic or sotadean verse, which is sometimes said to have been palindromic, but no examples survive, and the exact nature of the ""reverse"" readings is unclear. a palindrome was found as a graffito at herculaneum, a city buried by ash in    ce. this palindrome, called the sator square, consists of a sentence written in latin: ""sator arepo tenet opera rotas"" (""the sower arepo holds with effort the wheels""). it is remarkable for the fact that the first letters of each word form the first word, the second letters form the second word, and so forth. hence, it can be arranged into a word square that reads in four different ways: horizontally or vertically from either top left to bottom right or bottom right to top left. as such, they can be referred to as palindromatic.[citation needed] a palindrome with the same square property is the hebrew palindrome, ""we explained the glutton who is in the honey was burned and incinerated"", (פרשנו רעבתן שבדבש נתבער ונשרף; perashnu: ra`avtan shebad'vash nitba`er venisraf), credited to abraham ibn ezra in     , and referring to the halachic question as to whether a fly landing in honey makes the honey treif (non-kosher). the palindromic latin riddle ""in girum imus nocte et consumimur igni"" (""we go in a circle at night and are consumed by fire"") describes the behavior of moths. it is likely that this palindrome is from medieval rather than ancient times. the second word, borrowed from greek, should properly be spelled gyrum. byzantine baptismal fonts were often inscribed with the palindrome, νιψον ανομηματα μη μοναν οψιν (""nipson anomēmata mē monan opsin"") 'wash [your] sins, not only [your] face', attributed to gregory of nazianzus; most notably in the basilica of hagia sophia in constantinople. a variant, also a palindrome, replaces the plural ανομηματα (""sins"") by the singular ανομημα (""sin""). the inscription is found on fonts in many churches in western europe: orléans (st. menin's abbey); dulwich college; nottingham (st. mary's); worlingworth; harlow; knapton; london (st martin, ludgate); and hadleigh (suffolk). a greek poet in      vienna even composed a poem, ποίημα καρκινικόν (carcinic poem), in ancient greek, where every one of the     lines was a palindrome. in english, there are dozens of palindrome words, such as eye, madam, and deified, but english writers generally only cited latin and greek palindromic sentences in the early   th century, even though john taylor had coined one in     : ""lewd did i live, & evil i did dwel"" (with the ampersand being something of a ""fudge"" ). this is generally considered to be the first english-language palindrome sentence, and was long reputed (notably by the grammarian james ""hermes"" harris) to be the only one, despite many efforts to find others. (taylor had also composed two other, ""rather indifferent"", palindromic lines of poetry: ""deer madam, reed"", ""deem if i meed"". ) then in     , a certain ""j.t.r."" coined ""able was i ere i saw elba"", which became famous after it was (implausibly) attributed to napoleon. "
322,322,Conic form of a parabola,2,https://en.wikipedia.org/wiki/Parabola,"in mathematics, a parabola is a plane curve which is mirror-symmetrical and is approximately u-shaped. it fits several superficially different mathematical descriptions, which can all be proved to define exactly the same curves. one description of a parabola involves a point (the focus) and a line (the directrix). the focus does not lie on the directrix. the parabola is the locus of points in that plane that are equidistant from both the directrix and the focus. another description of a parabola is as a conic section, created from the intersection of a right circular conical surface and a plane parallel to another plane that is tangential to the conical surface.[a] the line perpendicular to the directrix and passing through the focus (that is, the line that splits the parabola through the middle) is called the ""axis of symmetry"". the point where the parabola intersects its axis of symmetry is called the ""vertex"" and is the point where the parabola is most sharply curved. the distance between the vertex and the focus, measured along the axis of symmetry, is the ""focal length"". the ""latus rectum"" is the chord of the parabola that is parallel to the directrix and passes through the focus. parabolas can open up, down, left, right, or in some other arbitrary direction. any parabola can be repositioned and rescaled to fit exactly on any other parabola—that is, all parabolas are geometrically similar. parabolas have the property that, if they are made of material that reflects light, then light that travels parallel to the axis of symmetry of a parabola and strikes its concave side is reflected to its focus, regardless of where on the parabola the reflection occurs. conversely, light that originates from a point source at the focus is reflected into a parallel (""collimated"") beam, leaving the parabola parallel to the axis of symmetry. the same effects occur with sound and other waves. this reflective property is the basis of many practical uses of parabolas. the parabola has many important applications, from a parabolic antenna or parabolic microphone to automobile headlight reflectors and the design of ballistic missiles. it is frequently used in physics, engineering, and many other areas. the earliest known work on conic sections was by menaechmus in the  th century bc. he discovered a way to solve the problem of doubling the cube using parabolas. (the solution, however, does not meet the requirements of compass-and-straightedge construction.) the area enclosed by a parabola and a line segment, the so-called ""parabola segment"", was computed by archimedes by the method of exhaustion in the  rd century bc, in his the quadrature of the parabola. the name ""parabola"" is due to apollonius, who discovered many properties of conic sections. it means ""application"", referring to ""application of areas"" concept, that has a connection with this curve, as apollonius had proved. the focus–directrix property of the parabola and other conic sections is due to pappus. galileo showed that the path of a projectile follows a parabola, a consequence of uniform acceleration due to gravity. the idea that a parabolic reflector could produce an image was already well known before the invention of the reflecting telescope. designs were proposed in the early to mid-  th century by many mathematicians, including rené descartes, marin mersenne, and james gregory. when isaac newton built the first reflecting telescope in     , he skipped using a parabolic mirror because of the difficulty of fabrication, opting for a spherical mirror. parabolic mirrors are used in most modern reflecting telescopes and in satellite dishes and radar receivers. a parabola can be defined geometrically as a set of points (locus of points) in the euclidean plane: the midpoint v {\displaystyle v} of the perpendicular from the focus f {\displaystyle f} onto the directrix l {\displaystyle l} is called vertex, and the line f v {\displaystyle fv} is the axis of symmetry of the parabola. "
323,323,parallel lines,1,https://en.wikipedia.org/wiki/Parallel_Lines," parallel lines is the third studio album by american rock band blondie, released on september   ,     , by chrysalis records to international commercial success. the album reached no.   in the united kingdom in february      and proved to be the band's commercial breakthrough in the united states, where it reached no.   in april     . in billboard magazine, parallel lines was listed at no.   in the top pop albums year-end chart of     . the album spawned several successful singles, notably the international hit ""heart of glass"". ""musically, blondie were hopelessly horrible when we first began rehearsing for parallel lines, and in terms of my attitude they didn't know what had hit them. i basically went in there like adolf hitler and said, 'you are going to make a great record, and that means you're going to start playing better.'"" —mike chapman, in an interview for sound on sound, recalling blondie's initial musical inexperience blondie's second studio album plastic letters was their last album produced by richard gottehrer, whose sound had formed the basis of blondie's new wave and punk output. during a tour of the west coast of the us in support of plastic letters, blondie encountered australian producer mike chapman in california. peter leeds, blondie's manager, conspired with chrysalis records to encourage chapman to work with blondie on new music. drummer clem burke recalls feeling enthusiastic about the proposition, believing chapman could create innovative and eclectic records. however, lead vocalist debbie harry was far less enthusiastic about chapman's involvement as she knew him only by reputation; according to chapman, her animosity towards him was because ""they were new york. [he] was l.a."". harry's cautiousness abated after she played chapman early cuts of ""heart of glass"" and ""sunday girl"" and he was impressed. in june      the band entered the record plant in new york to record their third album, and first with chapman. however, chapman found the band difficult to work with, remembering them as the worst band he ever worked with in terms of musical ability, although praising frank infante as ""an amazing guitarist"". sessions with chris stein were hampered by his being stoned during recording, and chapman encouraged him to write songs rather than play guitar. similarly, according to chapman, jimmy destri would prove himself to be far better at songwriting than as a keyboardist and clem burke had poor timing playing drums. as a result, chapman spent time improving the band, especially stein with whom chapman spent hours rerecording his parts to ensure they were right. bassist nigel harrison became so frustrated with chapman's drive for perfection that he threw a synthesizer at him during recording. chapman recalls the atmosphere at the record plant in an interview for sound on sound: the blondies were tough in the studio, real tough. none of them liked each other, except chris and debbie, and there was so much animosity. they were really, really juvenile in their approach to life—a classic new york underground rock band—and they didn't give a fuck about anything. they just wanted to have fun and didn't want to work too hard getting it. chapman took an unorthodox approach when recording with harry whom he describes as ""a great singer and a great vocal stylist, with a beautifully identifiable voice. however ... also very moody"". chapman was far more cautious of demanding much from harry as he saw her as a highly emotional person who would vest these emotions in the songs they made. he remembers harry disappearing into the bathroom in tears for several hours at a time during recording. during a day of recording, harry sang two lead parts and some harmonies, less work than she did previously with gottehrer. this was due to chapman encouraging her to be cautious about the way she sang, particularly to recognise phrasing, timing and attitude. blondie recorded parallel lines in six weeks, despite being given six months by terry ellis, co-founder of chrysalis records, to do so. for the drums, a traditional set-up was used and chapman fitted neumann microphones to the toms, snare and hi-hat, as well as several above the site. when recording, chapman would start with the bass track, which was difficult to record at the time, by way of ""pencil erasing"". chapman explained in an interview for sound on sound, ""that meant using a pencil to hold the tape away from the head and erasing up to the kick drum. if a bass part was ahead of the kick, you could erase it so that it sounded like it was on top of the kick. that's very easy to do these days, but back then it was quite a procedure just to get the bottom end sounding nice and tight."" a combination di/amp method was used to record harrison's bass and destri's synthesizer. shure sm   and akg     microphones were used to capture infante's les paul guitar. king crimson guitarist robert fripp makes a guest appearance on his main instrument on ""fade away and radiate"". after the basic track was complete, chapman would record lead and backing vocals with harry. however, this process was hampered by many songs not being written in time for the vocals to be recorded. ""sunday girl"", ""picture this"" and ""one way or another"" were all unfinished during the rehearsal sessions. when recording vocal parts, chapman remembers asking harry if she was ready to sing, only for her to reply ""yeah, just a minute"" as she was still writing lyrics down. chapman notes that many ""classic"" songs from the album were created this way. "
324,324,Parallelepiped,3,https://en.wikipedia.org/wiki/Parallelepiped,"in geometry, a parallelepiped is a three-dimensional figure formed by six parallelograms (the term rhomboid is also sometimes used with this meaning). by analogy, it relates to a parallelogram just as a cube relates to a square. in euclidean geometry, the four concepts—parallelepiped and cube in three dimensions, parallelogram and square in two dimensions—are defined, but in the context of a more general affine geometry, in which angles are not differentiated, only parallelograms and parallelepipeds exist. three equivalent definitions of parallelepiped are the rectangular cuboid (six rectangular faces), cube (six square faces), and the rhombohedron (six rhombus faces) are all specific cases of parallelepiped. ""parallelepiped"" is now usually pronounced /ˌpærəlɛlɪˈpɪpɛd/, /ˌpærəlɛlɪˈpaɪpɛd/, or /-pɪd/; traditionally it was /ˌpærəlɛlˈɛpɪpɛd/ parr-ə-lel-ep-i-ped in accordance with its etymology in greek παραλληλεπίπεδον parallelepipedon, a body ""having parallel planes"". parallelepipeds are a subclass of the prismatoids. any of the three pairs of parallel faces can be viewed as the base planes of the prism. a parallelepiped has three sets of four parallel edges; the edges within each set are of equal length. parallelepipeds result from linear transformations of a cube (for the non-degenerate cases: the bijective linear transformations). since each face has point symmetry, a parallelepiped is a zonohedron. also the whole parallelepiped has point symmetry ci (see also triclinic). each face is, seen from the outside, the mirror image of the opposite face. the faces are in general chiral, but the parallelepiped is not. a space-filling tessellation is possible with congruent copies of any parallelepiped. a parallelepiped can be considered as an oblique prism with a parallelogram as base. hence the volume v {\displaystyle v} of a parallelepiped is the product of the base area b {\displaystyle b} and the height h {\displaystyle h} (see diagram). with the mixed product of three vectors is called triple product. it can be described by a determinant. hence for a → = ( a   , a   , a   ) t , b → = ( b   , b   , b   ) t , c → = ( c   , c   , c   ) t , {\displaystyle {\vec {a}}=(a_{ },a_{ },a_{ })^{t},~{\vec {b}}=(b_{ },b_{ },b_{ })^{t},~{\vec {c}}=(c_{ },c_{ },c_{ })^{t},} the volume is: "
325,325,parametric equations,2,https://en.wikipedia.org/wiki/Parametric_equation,"in mathematics, a parametric equation defines a group of quantities as functions of one or more independent variables called parameters. parametric equations are commonly used to express the coordinates of the points that make up a geometric object such as a curve or surface, in which case the equations are collectively called a parametric representation or parameterization (alternatively spelled as parametrisation) of the object. for example, the equations form a parametric representation of the unit circle, where t is the parameter: a point (x, y) is on the unit circle if and only if there is a value of t such that these two equations generate that point. sometimes the parametric equations for the individual scalar output variables are combined into a single parametric equation in vectors: parametric representations are generally nonunique (see the ""examples in two dimensions"" section below), so the same quantities may be expressed by a number of different parameterizations. in addition to curves and surfaces, parametric equations can describe manifolds and algebraic varieties of higher dimension, with the number of parameters being equal to the dimension of the manifold or variety, and the number of equations being equal to the dimension of the space in which the manifold or variety is considered (for curves the dimension is one and one parameter is used, for surfaces dimension two and two parameters, etc.). parametric equations are commonly used in kinematics, where the trajectory of an object is represented by equations depending on time as the parameter. because of this application, a single parameter is often labeled t; however, parameters can represent other physical quantities (such as geometric variables) or can be selected arbitrarily for convenience. parameterizations are non-unique; more than one set of parametric equations can specify the same curve. in kinematics, objects' paths through space are commonly described as parametric curves, with each spatial coordinate depending explicitly on an independent parameter (usually time). used in this way, the set of parametric equations for the object's coordinates collectively constitute a vector-valued function for position. such parametric curves can then be integrated and differentiated termwise. thus, if a particle's position is described parametrically as then its velocity can be found as and its acceleration as another important use of parametric equations is in the field of computer-aided design (cad). for example, consider the following three representations, all of which are commonly used to describe planar curves. "
326,326,Partial derivative,1,https://en.wikipedia.org/wiki/Partial_derivative,"in mathematics, a partial derivative of a function of several variables is its derivative with respect to one of those variables, with the others held constant (as opposed to the total derivative, in which all variables are allowed to vary). partial derivatives are used in vector calculus and differential geometry. the partial derivative of a function f ( x , y , … ) {\displaystyle f(x,y,\dots )} with respect to the variable x {\displaystyle x} is variously denoted by sometimes, for z = f ( x , y , … ) {\displaystyle z=f(x,y,\ldots )} , the partial derivative of z {\displaystyle z} with respect to x {\displaystyle x} is denoted as ∂ z ∂ x . {\displaystyle {\tfrac {\partial z}{\partial x}}.} since a partial derivative generally has the same arguments as the original function, its functional dependence is sometimes explicitly signified by the notation, such as in: the symbol used to denote partial derivatives is ∂. one of the first known uses of this symbol in mathematics is by marquis de condorcet from     , who used it for partial differences. the modern partial derivative notation was created by adrien-marie legendre (    ), although he later abandoned it; carl gustav jacob jacobi reintroduced the symbol in     . like ordinary derivatives, the partial derivative is defined as a limit. let u be an open subset of r n {\displaystyle \mathbb {r} ^{n}} and f : u → r {\displaystyle f:u\to \mathbb {r} } a function. the partial derivative of f at the point a = ( a   , … , a n ) ∈ u {\displaystyle \mathbf {a} =(a_{ },\ldots ,a_{n})\in u} with respect to the i-th variable xi is defined as even if all partial derivatives ∂f/∂xi(a) exist at a given point a, the function need not be continuous there. however, if all partial derivatives exist in a neighborhood of a and are continuous there, then f is totally differentiable in that neighborhood and the total derivative is continuous. in this case, it is said that f is a c  function. this can be used to generalize for vector valued functions, f : u → r m {\displaystyle f:u\to \mathbb {r} ^{m}} , by carefully using a componentwise argument. the partial derivative ∂ f ∂ x {\displaystyle {\frac {\partial f}{\partial x}}} can be seen as another function defined on u and can again be partially differentiated. if all mixed second order partial derivatives are continuous at a point (or on a set), f is termed a c  function at that point (or on that set); in this case, the partial derivatives can be exchanged by clairaut's theorem: for the following examples, let f {\displaystyle f} be a function in x , y {\displaystyle x,y} and z {\displaystyle z} . first-order partial derivatives: second-order partial derivatives: "
327,327,Partial fraction decomposition,1,https://en.wikipedia.org/wiki/Partial_fraction_decomposition,"in algebra, the partial fraction decomposition or partial fraction expansion of a rational fraction (that is, a fraction such that the numerator and the denominator are both polynomials) is an operation that consists of expressing the fraction as a sum of a polynomial (possibly zero) and one or several fractions with a simpler denominator. the importance of the partial fraction decomposition lies in the fact that it provides algorithms for various computations with rational functions, including the explicit computation of antiderivatives, taylor series expansions, inverse z-transforms, and inverse laplace transforms. the concept was discovered independently in      by both johann bernoulli and gottfried leibniz. in symbols, the partial fraction decomposition of a rational fraction of the form f ( x ) g ( x ) , {\displaystyle \textstyle {\frac {f(x)}{g(x)}},} where f and g are polynomials, is its expression as where p(x) is a polynomial, and, for each j, the denominator gj (x) is a power of an irreducible polynomial (that is not factorable into polynomials of positive degrees), and the numerator fj (x) is a polynomial of a smaller degree than the degree of this irreducible polynomial. when explicit computation is involved, a coarser decomposition is often preferred, which consists of replacing ""irreducible polynomial"" by ""square-free polynomial"" in the description of the outcome. this allows replacing polynomial factorization by the much easier to compute square-free factorization. this is sufficient for most applications, and avoids introducing irrational coefficients when the coefficients of the input polynomials are integers or rational numbers. let be a rational fraction, where f and g are univariate polynomials in the indeterminate x over a field. the existence of the partial fraction can be proved by applying inductively the following reduction steps. there exist two polynomials e and f  such that and where deg ⁡ p {\displaystyle \deg p} denotes the degree of the polynomial p. "
328,328,Pascals Triangle,1,https://en.wikipedia.org/wiki/Pascal%27s_triangle,"in mathematics, pascal's triangle is a triangular array of the binomial coefficients that arises in probability theory, combinatorics, and algebra. in much of the western world, it is named after the french mathematician blaise pascal, although other mathematicians studied it centuries before him in india, persia, china, germany, and italy. the rows of pascal's triangle are conventionally enumerated starting with row n =   {\displaystyle n= } at the top (the  th row). the entries in each row are numbered from the left beginning with k =   {\displaystyle k= } and are usually staggered relative to the numbers in the adjacent rows. the triangle may be constructed in the following manner: in row   (the topmost row), there is a unique nonzero entry  . each entry of each subsequent row is constructed by adding the number above and to the left with the number above and to the right, treating blank entries as  . for example, the initial number in the first (or any other) row is   (the sum of   and  ), whereas the numbers   and   in the third row are added to produce the number   in the fourth row. the entry in the n {\displaystyle n} th row and k {\displaystyle k} th column of pascal's triangle is denoted ( n k ) {\displaystyle {n \choose k}} . for example, the unique nonzero entry in the topmost row is (     ) =   {\displaystyle {  \choose  }= } . with this notation, the construction of the previous paragraph may be written as follows: for any non-negative integer n {\displaystyle n} and any integer   ≤ k ≤ n {\displaystyle  \leq k\leq n} . this recurrence for the binomial coefficients is known as pascal's rule. the pattern of numbers that forms pascal's triangle was known well before pascal's time. pascal innovated many previously unattested uses of the triangle's numbers, uses he described comprehensively in the earliest known mathematical treatise to be specially devoted to the triangle, his traité du triangle arithmétique (    ; published     ). centuries before, discussion of the numbers had arisen in the context of indian studies of combinatorics and binomial numbers. it appears from later commentaries that the binomial coefficients and the additive formula for generating them, ( n r ) = ( n −   r ) + ( n −   r −   ) {\displaystyle {n \choose r}={n-  \choose r}+{n-  \choose r- }} , were known to pingala in or before the  nd century bc. while pingala's work only survives in fragments, the commentator varāhamihira, around    , gave a clear description of the additive formula, and a more detailed explanation of the same rule was given by halayudha, around    . halayudha also explained obscure references to meru-prastaara, the staircase of mount meru, giving the first surviving description of the arrangement of these numbers into a triangle. in approximately    , the jain mathematician mahāvīra gave a different formula for the binomial coefficients, using multiplication, equivalent to the modern formula ( n r ) = n ! r ! ( n − r ) ! {\displaystyle {n \choose r}={\frac {n!}{r!(n-r)!}}} . in     , four columns of the first sixteen rows were given by the mathematician bhattotpala, who was the first recorded mathematician to equate the additive and multiplicative formulas for these numbers. at around the same time, the persian mathematician al-karaji (   –    ) wrote a now-lost book which contained the first description of pascal's triangle. it was later repeated by the persian poet-astronomer-mathematician omar khayyám (    –    ); thus the triangle is also referred to as the khayyam triangle in iran. several theorems related to the triangle were known, including the binomial theorem. khayyam used a method of finding nth roots based on the binomial expansion, and therefore on the binomial coefficients. pascal's triangle was known in china in the early   th century through the work of the chinese mathematician jia xian (    –    ). in the   th century, yang hui (    –    ) presented the triangle and hence it is still called yang hui's triangle (杨辉三角; 楊輝三角) in china. in europe, pascal's triangle appeared for the first time in the arithmetic of jordanus de nemore (  th century). the binomial coefficients were calculated by gersonides in the early   th century, using the multiplicative formula for them. petrus apianus (    –    ) published the full triangle on the frontispiece of his book on business calculations in     . michael stifel published a portion of the triangle (from the second to the middle column in each row) in     , describing it as a table of figurate numbers. in italy, pascal's triangle is referred to as tartaglia's triangle, named for the italian algebraist niccolò fontana tartaglia (    –    ), who published six rows of the triangle in     . gerolamo cardano, also, published the triangle as well as the additive and multiplicative rules for constructing it in     . pascal's traité du triangle arithmétique (treatise on arithmetical triangle) was published in     . in this, pascal collected several results then known about the triangle, and employed them to solve problems in probability theory. the triangle was later named after pascal by pierre raymond de montmort (    ) who called it ""table de m. pascal pour les combinaisons"" (french: table of mr. pascal for combinations) and abraham de moivre (    ) who called it ""triangulum arithmeticum pascalianum"" (latin: pascal's arithmetic triangle), which became the modern western name. "
329,329,Pearson correlation coefficient,3,https://en.wikipedia.org/wiki/Pearson_correlation_coefficient," in statistics, the pearson correlation coefficient (pcc, pronounced /ˈpɪərsən/) ― also known as pearson's r, the pearson product-moment correlation coefficient (ppmcc), the bivariate correlation, or colloquially simply as the correlation coefficient ― is a measure of linear correlation between two sets of data. it is the ratio between the covariance of two variables and the product of their standard deviations; thus it is essentially a normalized measurement of the covariance, such that the result always has a value between −  and  . as with covariance itself, the measure can only reflect a linear correlation of variables, and ignores many other types of relationship or correlation. as a simple example, one would expect the age and height of a sample of teenagers from a high school to have a pearson correlation coefficient significantly greater than  , but less than   (as   would represent an unrealistically perfect correlation). it was developed by karl pearson from a related idea introduced by francis galton in the     s, and for which the mathematical formula was derived and published by auguste bravais in     .[a] the naming of the coefficient is thus an example of stigler's law. pearson's correlation coefficient is the covariance of the two variables divided by the product of their standard deviations. the form of the definition involves a ""product moment"", that is, the mean (the first moment about the origin) of the product of the mean-adjusted random variables; hence the modifier product-moment in the name. pearson's correlation coefficient, when applied to a population, is commonly represented by the greek letter ρ (rho) and may be referred to as the population correlation coefficient or the population pearson correlation coefficient. given a pair of random variables ( x , y ) {\displaystyle (x,y)} , the formula for ρ is: where: the formula for ρ {\displaystyle \rho } can be expressed in terms of mean and expectation. since the formula for ρ {\displaystyle \rho } can also be written as "
330,330,Periodic Functions,2,https://en.wikipedia.org/wiki/Periodic_function,"a periodic function is a function that repeats its values at regular intervals. for example, the trigonometric functions, which repeat at intervals of   π {\displaystyle  \pi } radians, are periodic functions. periodic functions are used throughout science to describe oscillations, waves, and other phenomena that exhibit periodicity. any function that is not periodic is called aperiodic. a function f is said to be periodic if, for some nonzero constant p, it is the case that for all values of x in the domain. a nonzero constant p for which this is the case is called a period of the function. if there exists a least positive constant p with this property, it is called the fundamental period (also primitive period, basic period, or prime period.) often, ""the"" period of a function is used to mean its fundamental period. a function with period p will repeat on intervals of length p, and these intervals are sometimes also referred to as periods of the function. geometrically, a periodic function can be defined as a function whose graph exhibits translational symmetry, i.e. a function f is periodic with period p if the graph of f is invariant under translation in the x-direction by a distance of p. this definition of periodicity can be extended to other geometric shapes and patterns, as well as be generalized to higher dimensions, such as periodic tessellations of the plane. a sequence can also be viewed as a function defined on the natural numbers, and for a periodic sequence these notions are defined accordingly. the sine function is periodic with period   π {\displaystyle  \pi } , since for all values of x {\displaystyle x} . this function repeats on intervals of length   π {\displaystyle  \pi } (see the graph to the right). everyday examples are seen when the variable is time; for instance the hands of a clock or the phases of the moon show periodic behaviour. periodic motion is motion in which the position(s) of the system are expressible as periodic functions, all with the same period. for a function on the real numbers or on the integers, that means that the entire graph can be formed from copies of one particular portion, repeated at regular intervals. a simple example of a periodic function is the function f {\displaystyle f} that gives the ""fractional part"" of its argument. its period is  . in particular, the graph of the function f {\displaystyle f} is the sawtooth wave. "
331,331,periodic sequence,1,https://en.wikipedia.org/wiki/Periodic_sequence,"in mathematics, a periodic sequence (sometimes called a cycle[citation needed]) is a sequence for which the same terms are repeated over and over: the number p of repeated terms is called the period (period). a (purely) periodic sequence (with period p), or a p-periodic sequence, is a sequence a , a , a , ... satisfying one size fits all for all values of n. if a sequence is regarded as a function whose domain is the set of natural numbers, then a periodic sequence is simply a special type of periodic function.[citation needed] the smallest p for which a periodic sequence is p-periodic is called its least period or exact period. [verification needed] every constant function is  -periodic. the sequence   ,   ,   ,   ,   ,   … {\displaystyle  , , , , , \dots } is periodic with least period  . the sequence of digits in the decimal expansion of  /  is periodic with period  : more generally, the sequence of digits in the decimal expansion of any rational number is eventually periodic (see below). [verification needed] the sequence of powers of −  is periodic with period two: more generally, the sequence of powers of any root of unity is periodic. the same holds true for the powers of any element of finite order in a group.[citation needed] "
332,332,Permutation,2,https://en.wikipedia.org/wiki/Permutation,"in mathematics, a permutation of a set is, loosely speaking, an arrangement of its members into a sequence or linear order, or if the set is already ordered, a rearrangement of its elements. the word ""permutation"" also refers to the act or process of changing the linear order of an ordered set. permutations differ from combinations, which are selections of some members of a set regardless of order. for example, written as tuples, there are six permutations of the set { ,  ,  }, namely ( ,  ,  ), ( ,  ,  ), ( ,  ,  ), ( ,  ,  ), ( ,  ,  ), and ( ,  ,  ). these are all the possible orderings of this three-element set. anagrams of words whose letters are different are also permutations: the letters are already ordered in the original word, and the anagram is a reordering of the letters. the study of permutations of finite sets is an important topic in the fields of combinatorics and group theory. permutations are used in almost every branch of mathematics, and in many other fields of science. in computer science, they are used for analyzing sorting algorithms; in quantum physics, for describing states of particles; and in biology, for describing rna sequences. the number of permutations of n distinct objects is n factorial, usually written as n!, which means the product of all positive integers less than or equal to n. technically, a permutation of a set s is defined as a bijection from s to itself. that is, it is a function from s to s for which every element occurs exactly once as an image value. this is related to the rearrangement of the elements of s in which each element s is replaced by the corresponding f(s). for example, the permutation ( ,  ,  ) mentioned above is described by the function α {\displaystyle \alpha } defined as the collection of all permutations of a set form a group called the symmetric group of the set. the group operation is the composition (performing two given rearrangements in succession), which results in another rearrangement. as properties of permutations do not depend on the nature of the set elements, it is often the permutations of the set {   ,   , … , n } {\displaystyle \{ , ,\ldots ,n\}} that are considered for studying permutations. in elementary combinatorics, the k-permutations, or partial permutations, are the ordered arrangements of k distinct elements selected from a set. when k is equal to the size of the set, these are the permutations of the set. permutations called hexagrams were used in china in the i ching (pinyin: yi jing) as early as      bc. al-khalil (   –   ), an arab mathematician and cryptographer, wrote the book of cryptographic messages. it contains the first use of permutations and combinations, to list all possible arabic words with and without vowels. the rule to determine the number of permutations of n objects was known in indian culture around     . the lilavati by the indian mathematician bhaskara ii contains a passage that translates to: "
333,333,Permutation matrix,2,https://en.wikipedia.org/wiki/Permutation_matrix," in mathematics, particularly in matrix theory, a permutation matrix is a square binary matrix that has exactly one entry of   in each row and each column and  s elsewhere. each such matrix, say p, represents a permutation of m elements and, when used to multiply another matrix, say a, results in permuting the rows (when pre-multiplying, to form pa) or columns (when post-multiplying, to form ap) of the matrix a. given a permutation π of m elements, represented in two-line form by there are two natural ways to associate the permutation with a permutation matrix; namely, starting with the m × m identity matrix, im, either permute the columns or permute the rows, according to π. both methods of defining permutation matrices appear in the literature and the properties expressed in one representation can be easily converted to the other representation. this article will primarily deal with just one of these representations and the other will only be mentioned when there is a difference to be aware of. the m × m permutation matrix pπ = (pij) obtained by permuting the columns of the identity matrix im, that is, for each i, pij =   if j = π(i) and pij =   otherwise, will be referred to as the column representation in this article. since the entries in row i are all   except that a   appears in column π(i), we may write where e j {\displaystyle \mathbf {e} _{j}} , a standard basis vector, denotes a row vector of length m with   in the jth position and   in every other position. for example, the permutation matrix pπ corresponding to the permutation π = (                     ) {\displaystyle \pi ={\begin{pmatrix} & & & & \\ & & & & \end{pmatrix}}} is observe that the jth column of the i  identity matrix now appears as the π(j)th column of pπ. the other representation, obtained by permuting the rows of the identity matrix im, that is, for each j, pij =   if i = π(j) and pij =   otherwise, will be referred to as the row representation. the column representation of a permutation matrix is used throughout this section, except when otherwise indicated. "
334,334,Pigeonhole Principle,1,https://en.wikipedia.org/wiki/Pigeonhole_principle,"in mathematics, the pigeonhole principle states that if n {\displaystyle n} items are put into m {\displaystyle m} containers, with n > m {\displaystyle n>m} , then at least one container must contain more than one item. for example, if one has three gloves (and none is ambidextrous/reversible), then there must be at least two right-handed gloves, or at least two left-handed gloves, because there are three objects, but only two categories of handedness to put them into. this seemingly obvious statement, a type of counting argument, can be used to demonstrate possibly unexpected results. for example, given that the population of london is greater than the maximum number of hairs that can be present on a human's head, then the pigeonhole principle requires that there must be at least two people in london who have the same number of hairs on their heads. although the pigeonhole principle appears as early as      in a book attributed to jean leurechon, it is commonly called dirichlet's box principle or dirichlet's drawer principle after an      treatment of the principle by peter gustav lejeune dirichlet under the name schubfachprinzip (""drawer principle"" or ""shelf principle""). the principle has several generalizations and can be stated in various ways. in a more quantified version: for natural numbers k {\displaystyle k} and m {\displaystyle m} , if n = k m +   {\displaystyle n=km+ } objects are distributed among m {\displaystyle m} sets, then the pigeonhole principle asserts that at least one of the sets will contain at least k +   {\displaystyle k+ } objects. for arbitrary n {\displaystyle n} and m {\displaystyle m} this generalizes to k +   = ⌊ ( n −   ) / m ⌋ +   = ⌈ n / m ⌉ , {\displaystyle k+ =\lfloor (n- )/m\rfloor + =\lceil n/m\rceil ,} where ⌊ ⋯ ⌋ {\displaystyle \lfloor \cdots \rfloor } and ⌈ ⋯ ⌉ {\displaystyle \lceil \cdots \rceil } denote the floor and ceiling functions, respectively. though the most straightforward application is to finite sets (such as pigeons and boxes), it is also used with infinite sets that cannot be put into one-to-one correspondence. to do so requires the formal statement of the pigeonhole principle, which is ""there does not exist an injective function whose codomain is smaller than its domain"". advanced mathematical proofs like siegel's lemma build upon this more general concept. dirichlet published his works in both french and german, using either the german schubfach or the french tiroir. the strict original meaning of these terms corresponds to the english drawer, that is, an open-topped box that can be slid in and out of the cabinet that contains it. (dirichlet wrote about distributing pearls among drawers.) these terms were morphed to the word pigeonhole in the sense of a small open space in a desk, cabinet, or wall for keeping letters or papers, metaphorically rooted in structures that house pigeons. because furniture with pigeonholes is commonly used for storing or sorting things into many categories (such as letters in a post office or room keys in a hotel), the translation pigeonhole may be a better rendering of dirichlet's original drawer metaphor. that understanding of the term pigeonhole, referring to some furniture features, is fading—especially among those who do not speak english natively but as a lingua franca in the scientific world—in favour of the more pictorial interpretation, literally involving pigeons and holes. the suggestive (though not misleading) interpretation of ""pigeonhole"" as ""dovecote"" has lately found its way back to a german back-translation of the ""pigeonhole principle"" as the ""taubenschlagprinzip"". besides the original terms ""schubfachprinzip"" in german and ""principe des tiroirs"" in french, other literal translations are still in use in arabic (""مبدأ برج الحمام""), bulgarian (""принцип на чекмеджетата""), chinese (""抽屉原理""), danish (""skuffeprincippet""), dutch (""ladenprincipe ""), hungarian (""skatulyaelv""), italian (""principio dei cassetti""), japanese (""引き出し論法""), persian (""اصل لانه کبوتری""), polish (""zasada szufladkowa""), swedish (""lådprincipen""), turkish (""çekmece ilkesi"") and vietnamese (""nguyên lý hộp""). assume a drawer contains a mixture of black socks and blue socks, each of which can be worn on either foot, and that you are pulling a number of socks from the drawer without looking. what is the minimum number of pulled socks required to guarantee a pair of the same color? using the pigeonhole principle, to have at least one pair of the same color (m =   holes, one per color) using one pigeonhole per color, you need to pull only three socks from the drawer (n =   items). either you have three of one color, or you have two of one color and one of the other. if there are n people who can shake hands with one another (where n >  ), the pigeonhole principle shows that there is always a pair of people who will shake hands with the same number of people. in this application of the principle, the 'hole' to which a person is assigned is the number of hands shaken by that person. since each person shakes hands with some number of people from   to n −  , there are n possible holes. on the other hand, either the ' ' hole or the 'n −  ' hole or both must be empty, for it is impossible (if n >  ) for some person to shake hands with everybody else while some person shakes hands with nobody. this leaves n people to be placed into at most n −   non-empty holes, so that the principle applies. this hand-shaking example is equivalent to the statement that in any graph with more than one vertex, there is at least one pair of vertices that share the same degree. this can be seen by associating each person with a vertex and each edge with a handshake. "
335,335,Plane geometry,2,https://en.wikipedia.org/wiki/Plane_(geometry)," in mathematics, a plane is a flat, two-dimensional surface that extends indefinitely. a plane is the two-dimensional analogue of a point (zero dimensions), a line (one dimension) and three-dimensional space. planes can arise as subspaces of some higher-dimensional space, as with one of a room's walls, infinitely extended, or they may enjoy an independent existence in their own right, as in the setting of two-dimensional euclidean geometry. when working exclusively in two-dimensional euclidean space, the definite article is used, so the plane refers to the whole space. many fundamental tasks in mathematics, geometry, trigonometry, graph theory, and graphing are performed in a two-dimensional space, often in the plane. euclid set forth the first great landmark of mathematical thought, an axiomatic treatment of geometry. he selected a small core of undefined terms (called common notions) and postulates (or axioms) which he then used to prove various geometrical statements. although the plane in its modern sense is not directly given a definition anywhere in the elements, it may be thought of as part of the common notions. euclid never used numbers to measure length, angle, or area. although the euclidean plane is not quite the same as the cartesian plane, they are formally equivalent. a plane is a ruled surface this section is solely concerned with planes embedded in three dimensions: specifically, in r . in a euclidean space of any number of dimensions, a plane is uniquely determined by any of the following: the following statements hold in three-dimensional euclidean space but not in higher dimensions, though they have higher-dimensional analogues: in a manner analogous to the way lines in a two-dimensional space are described using a point-slope form for their equations, planes in a three dimensional space have a natural description using a point in the plane and a vector orthogonal to it (the normal vector) to indicate its ""inclination"". specifically, let r  be the position vector of some point p  = (x , y , z ), and let n = (a, b, c) be a nonzero vector. the plane determined by the point p  and the vector n consists of those points p, with position vector r, such that the vector drawn from p  to p is perpendicular to n. recalling that two vectors are perpendicular if and only if their dot product is zero, it follows that the desired plane can be described as the set of all points r such that "
336,336,Poisson Distribution,2,https://en.wikipedia.org/wiki/Poisson_distribution," γ ( ⌊ k +   ⌋ , λ ) ⌊ k ⌋ ! {\displaystyle {\frac {\gamma (\lfloor k+ \rfloor ,\lambda )}{\lfloor k\rfloor !}}} , or e − λ ∑ i =   ⌊ k ⌋ λ i i ! {\displaystyle e^{-\lambda }\sum _{i= }^{\lfloor k\rfloor }{\frac {\lambda ^{i}}{i!}}\ } , or q ( ⌊ k +   ⌋ , λ ) {\displaystyle q(\lfloor k+ \rfloor ,\lambda )} λ [   − log ⁡ ( λ ) ] + e − λ ∑ k =   ∞ λ k log ⁡ ( k ! ) k ! {\displaystyle \lambda [ -\log(\lambda )]+e^{-\lambda }\sum _{k= }^{\infty }{\frac {\lambda ^{k}\log(k!)}{k!}}} (for large λ {\displaystyle \lambda } ) in probability theory and statistics, the poisson distribution (/ˈpwɑːsɒn/; french pronunciation: ​[pwasɔ̃]), named after french mathematician siméon denis poisson, is a discrete probability distribution that expresses the probability of a given number of events occurring in a fixed interval of time or space if these events occur with a known constant mean rate and independently of the time since the last event. the poisson distribution can also be used for the number of events in other specified interval types such as distance, area or volume. for instance, a call center receives an average of     calls per hour,    hours a day. the calls are independent; receiving one does not change the probability of when the next one will arrive. the number of calls received during any minute has a poisson probability distribution: the most likely numbers are   and   but   and   are also likely and there is a small probability of it being as low as zero and a very small probability it could be   . another example is the number of decay events that occur from a radioactive source during a defined observation period. the distribution was first introduced by siméon denis poisson (    –    ) and published together with his probability theory in his work recherches sur la probabilité des jugements en matière criminelle et en matière civile (    ). :    -    the work theorized about the number of wrongful convictions in a given country by focusing on certain random variables n that count, among other things, the number of discrete occurrences (sometimes called ""events"" or ""arrivals"") that take place during a time-interval of given length. the result had already been given in      by abraham de moivre in de mensura sortis seu; de probabilitate eventuum in ludis a casu fortuito pendentibus . :     :   -   :     :     this makes it an example of stigler's law and it has prompted some authors to argue that the poisson distribution should bear the name of de moivre. in     , simon newcomb fitted the poisson distribution to the number of stars found in a unit of space. a further practical application of this distribution was made by ladislaus bortkiewicz in      when he was given the task of investigating the number of soldiers in the prussian army killed accidentally by horse kicks; :   -   this experiment introduced the poisson distribution to the field of reliability engineering. a discrete random variable x is said to have a poisson distribution, with parameter λ >   {\displaystyle \lambda > } , if it has a probability mass function given by: :    where the positive real number λ is equal to the expected value of x and also to its variance. "
337,337,Polar coordinate system,3,https://en.wikipedia.org/wiki/Polar_coordinate_system,"in mathematics, the polar coordinate system is a two-dimensional coordinate system in which each point on a plane is determined by a distance from a reference point and an angle from a reference direction. the reference point (analogous to the origin of a cartesian coordinate system) is called the pole, and the ray from the pole in the reference direction is the polar axis. the distance from the pole is called the radial coordinate, radial distance or simply radius, and the angle is called the angular coordinate, polar angle, or azimuth. angles in polar notation are generally expressed in either degrees or radians ( π rad being equal to    °). grégoire de saint-vincent and bonaventura cavalieri independently introduced the concepts in the mid-  th century, though the actual term polar coordinates has been attributed to gregorio fontana in the   th century. the initial motivation for the introduction of the polar system was the study of circular and orbital motion. polar coordinates are most appropriate in any context where the phenomenon being considered is inherently tied to direction and length from a center point in a plane, such as spirals. planar physical systems with bodies moving around a central point, or phenomena originating from a central point, are often simpler and more intuitive to model using polar coordinates. the polar coordinate system is extended to three dimensions in two ways: the cylindrical and spherical coordinate systems. the concepts of angle and radius were already used by ancient peoples of the first millennium bc. the greek astronomer and astrologer hipparchus (   –    bc) created a table of chord functions giving the length of the chord for each angle, and there are references to his using polar coordinates in establishing stellar positions. in on spirals, archimedes describes the archimedean spiral, a function whose radius depends on the angle. the greek work, however, did not extend to a full coordinate system. from the  th century ad onward, astronomers developed methods for approximating and calculating the direction to mecca (qibla)—and its distance—from any location on the earth. from the  th century onward they were using spherical trigonometry and map projection methods to determine these quantities accurately. the calculation is essentially the conversion of the equatorial polar coordinates of mecca (i.e. its longitude and latitude) to its polar coordinates (i.e. its qibla and distance) relative to a system whose reference meridian is the great circle through the given location and the earth's poles and whose polar axis is the line through the location and its antipodal point. there are various accounts of the introduction of polar coordinates as part of a formal coordinate system. the full history of the subject is described in harvard professor julian lowell coolidge's origin of polar coordinates. grégoire de saint-vincent and bonaventura cavalieri independently introduced the concepts in the mid-seventeenth century. saint-vincent wrote about them privately in      and published his work in     , while cavalieri published his in      with a corrected version appearing in     . cavalieri first used polar coordinates to solve a problem relating to the area within an archimedean spiral. blaise pascal subsequently used polar coordinates to calculate the length of parabolic arcs. in method of fluxions (written     , published     ), sir isaac newton examined the transformations between polar coordinates, which he referred to as the ""seventh manner; for spirals"", and nine other coordinate systems. in the journal acta eruditorum (    ), jacob bernoulli used a system with a point on a line, called the pole and polar axis respectively. coordinates were specified by the distance from the pole and the angle from the polar axis. bernoulli's work extended to finding the radius of curvature of curves expressed in these coordinates. the actual term polar coordinates has been attributed to gregorio fontana and was used by   th-century italian writers. the term appeared in english in george peacock's      translation of lacroix's differential and integral calculus. alexis clairaut was the first to think of polar coordinates in three dimensions, and leonhard euler was the first to actually develop them. the radial coordinate is often denoted by r or ρ, and the angular coordinate by φ, θ, or t. the angular coordinate is specified as φ by iso standard   -  . however, in mathematical literature the angle is often denoted by θ instead. "
338,338,polygon,2,https://en.wikipedia.org/wiki/Polygon," in geometry, a polygon (/ˈpɒlɪɡɒn/) is a plane figure that is described by a finite number of straight line segments connected to form a closed polygonal chain (or polygonal circuit). the bounded plane region, the bounding circuit, or the two together, may be called a polygon. the segments of a polygonal circuit are called its edges or sides. the points where two edges meet are the polygon's vertices (singular: vertex) or corners. the interior of a solid polygon is sometimes called its body. an n-gon is a polygon with n sides; for example, a triangle is a  -gon. a simple polygon is one which does not intersect itself. mathematicians are often concerned only with the bounding polygonal chains of simple polygons and they often define a polygon accordingly. a polygonal boundary may be allowed to cross over itself, creating star polygons and other self-intersecting polygons. a polygon is a  -dimensional example of the more general polytope in any number of dimensions. there are many more generalizations of polygons defined for different purposes. the word polygon derives from the greek adjective πολύς (polús) 'much', 'many' and γωνία (gōnía) 'corner' or 'angle'. it has been suggested that γόνυ (gónu) 'knee' may be the origin of gon. polygons are primarily classified by the number of sides. see the table below. polygons may be characterized by their convexity or type of non-convexity: the property of regularity may be defined in other ways: a polygon is regular if and only if it is both isogonal and isotoxal, or equivalently it is both cyclic and equilateral. a non-convex regular polygon is called a regular star polygon. euclidean geometry is assumed throughout. "
339,339,polynomial,1,https://en.wikipedia.org/wiki/Polynomial,"in mathematics, a polynomial is an expression consisting of indeterminates (also called variables) and coefficients, that involves only the operations of addition, subtraction, multiplication, and non-negative integer exponentiation of variables. an example of a polynomial of a single indeterminate x is x  −  x +  . an example in three variables is x  +  xyz  − yz +  . polynomials appear in many areas of mathematics and science. for example, they are used to form polynomial equations, which encode a wide range of problems, from elementary word problems to complicated scientific problems; they are used to define polynomial functions, which appear in settings ranging from basic chemistry and physics to economics and social science; they are used in calculus and numerical analysis to approximate other functions. in advanced mathematics, polynomials are used to construct polynomial rings and algebraic varieties, which are central concepts in algebra and algebraic geometry. the word polynomial joins two diverse roots: the greek poly, meaning ""many"", and the latin nomen, or ""name"". it was derived from the term binomial by replacing the latin root bi- with the greek poly-. that is, it means a sum of many terms (many monomials). the word polynomial was first used in the   th century. the x occurring in a polynomial is commonly called a variable or an indeterminate. when the polynomial is considered as an expression, x is a fixed symbol which does not have any value (its value is ""indeterminate""). however, when one considers the function defined by the polynomial, then x represents the argument of the function, and is therefore called a ""variable"". many authors use these two words interchangeably. a polynomial p in the indeterminate x is commonly denoted either as p or as p(x). formally, the name of the polynomial is p, not p(x), but the use of the functional notation p(x) dates from a time when the distinction between a polynomial and the associated function was unclear. moreover, the functional notation is often useful for specifying, in a single phrase, a polynomial and its indeterminate. for example, ""let p(x) be a polynomial"" is a shorthand for ""let p be a polynomial in the indeterminate x"". on the other hand, when it is not necessary to emphasize the name of the indeterminate, many formulas are much simpler and easier to read if the name(s) of the indeterminate(s) do not appear at each occurrence of the polynomial. the ambiguity of having two notations for a single mathematical object may be formally resolved by considering the general meaning of the functional notation for polynomials. if a denotes a number, a variable, another polynomial, or, more generally, any expression, then p(a) denotes, by convention, the result of substituting a for x in p. thus, the polynomial p defines the function which is the polynomial function associated to p. frequently, when using this notation, one supposes that a is a number. however, one may use it over any domain where addition and multiplication are defined (that is, any ring). in particular, if a is a polynomial then p(a) is also a polynomial. more specifically, when a is the indeterminate x, then the image of x by this function is the polynomial p itself (substituting x for x does not change anything). in other words, which justifies formally the existence of two notations for the same polynomial. a polynomial expression is an expression that can be built from constants and symbols called variables or indeterminates by means of addition, multiplication and exponentiation to a non-negative integer power. the constants are generally numbers, but may be any expression that do not involve the indeterminates, and represent mathematical objects that can be added and multiplied. two polynomial expressions are considered as defining the same polynomial if they may be transformed, one to the other, by applying the usual properties of commutativity, associativity and distributivity of addition and multiplication. for example ( x −   ) ( x −   ) {\displaystyle (x- )(x- )} and x   −   x +   {\displaystyle x^{ }- x+ } are two polynomial expressions that represent the same polynomial; so, one writes ( x −   ) ( x −   ) = x   −   x +  . {\displaystyle (x- )(x- )=x^{ }- x+ .} "
340,340,Polynomial Remainder Theorem,1,https://en.wikipedia.org/wiki/Polynomial_remainder_theorem,"in algebra, the polynomial remainder theorem or little bézout's theorem (named after étienne bézout) is an application of euclidean division of polynomials. it states that the remainder of the division of a polynomial f ( x ) {\displaystyle f(x)} by a linear polynomial x − r {\displaystyle x-r} is equal to f ( r ) . {\displaystyle f(r).} in particular, x − r {\displaystyle x-r} is a divisor of f ( x ) {\displaystyle f(x)} if and only if f ( r ) =   , {\displaystyle f(r)= ,} a property known as the factor theorem. let f ( x ) = x   −    x   −    {\displaystyle f(x)=x^{ }-  x^{ }-  } . polynomial division of f ( x ) {\displaystyle f(x)} by ( x −   ) {\displaystyle (x- )} gives the quotient x   −   x −    {\displaystyle x^{ }- x-  } and the remainder −     {\displaystyle -   } . therefore, f (   ) = −     {\displaystyle f( )=-   } . show that the polynomial remainder theorem holds for an arbitrary second degree polynomial f ( x ) = a x   + b x + c {\displaystyle f(x)=ax^{ }+bx+c} by using algebraic manipulation: multiplying both sides by (x − r) gives since r = a r   + b r + c {\displaystyle r=ar^{ }+br+c} is the remainder, we have indeed shown that f ( r ) = r {\displaystyle f(r)=r} . the polynomial remainder theorem follows from the theorem of euclidean division, which, given two polynomials f(x) (the dividend) and g(x) (the divisor), asserts the existence (and the uniqueness) of a quotient q(x) and a remainder r(x) such that if the divisor is g ( x ) = x − r , {\displaystyle g(x)=x-r,} where r is a constant, then either r(x) =   or its degree is zero; in both cases, r(x) is a constant that is independent of x; that is setting x = r {\displaystyle x=r} in this formula, we obtain: a slightly different proof, which may appear to some people as more elementary, starts with an observation that f ( x ) − f ( r ) {\displaystyle f(x)-f(r)} is a linear combination of terms of the form x k − r k , {\displaystyle x^{k}-r^{k},} each of which is divisible by x − r {\displaystyle x-r} since x k − r k = ( x − r ) ( x k −   + x k −   r + ⋯ + x r k −   + r k −   ) . {\displaystyle x^{k}-r^{k}=(x-r)(x^{k- }+x^{k- }r+\dots +xr^{k- }+r^{k- }).} the polynomial remainder theorem may be used to evaluate f ( r ) {\displaystyle f(r)} by calculating the remainder, r {\displaystyle r} . although polynomial long division is more difficult than evaluating the function itself, synthetic division is computationally easier. thus, the function may be more ""cheaply"" evaluated using synthetic division and the polynomial remainder theorem. "
341,341,Polynomial Ring,0,https://en.wikipedia.org/wiki/Polynomial_ring,"ring homomorphisms algebraic structures related structures algebraic number theory p-adic number theory and decimals algebraic geometry noncommutative algebraic geometry free algebra clifford algebra in mathematics, especially in the field of algebra, a polynomial ring or polynomial algebra is a ring (which is also a commutative algebra) formed from the set of polynomials in one or more indeterminates (traditionally also called variables) with coefficients in another ring, often a field. "
342,342,polytope,0,https://en.wikipedia.org/wiki/Polytope,"in elementary geometry, a polytope is a geometric object with flat sides (faces). it is a generalization in any number of dimensions of the three-dimensional polyhedron. polytopes may exist in any general number of dimensions n as an n-dimensional polytope or n-polytope. in this context, ""flat sides"" means that the sides of a (k+ )-polytope consist of k-polytopes that may have (k− )-polytopes in common. for example, a two-dimensional polygon is a  -polytope and a three-dimensional polyhedron is a  -polytope. some theories further generalize the idea to include such objects as unbounded apeirotopes and tessellations, decompositions or tilings of curved manifolds including spherical polyhedra, and set-theoretic abstract polytopes. polytopes in more than three dimensions were first discovered by ludwig schläfli. the german term polytop was coined by the mathematician reinhold hoppe, and was introduced to english mathematicians as polytope by alicia boole stott. the term polytope is nowadays a broad term that covers a wide class of objects, and various definitions appear in the mathematical literature. many of these definitions are not equivalent to each other, resulting in different overlapping sets of objects being called polytopes. they represent different approaches to generalizing the convex polytopes to include other objects with similar properties. the original approach broadly followed by ludwig schläfli, thorold gosset and others begins with the extension by analogy into four or more dimensions, of the idea of a polygon and polyhedron respectively in two and three dimensions. attempts to generalise the euler characteristic of polyhedra to higher-dimensional polytopes led to the development of topology and the treatment of a decomposition or cw-complex as analogous to a polytope. in this approach, a polytope may be regarded as a tessellation or decomposition of some given manifold. an example of this approach defines a polytope as a set of points that admits a simplicial decomposition. in this definition, a polytope is the union of finitely many simplices, with the additional property that, for any two simplices that have a nonempty intersection, their intersection is a vertex, edge, or higher dimensional face of the two. however this definition does not allow star polytopes with interior structures, and so is restricted to certain areas of mathematics. the discovery of star polyhedra and other unusual constructions led to the idea of a polyhedron as a bounding surface, ignoring its interior. in this light convex polytopes in p-space are equivalent to tilings of the (p− )-sphere, while others may be tilings of other elliptic, flat or toroidal (p− )-surfaces – see elliptic tiling and toroidal polyhedron. a polyhedron is understood as a surface whose faces are polygons, a  -polytope as a hypersurface whose facets (cells) are polyhedra, and so forth. the idea of constructing a higher polytope from those of lower dimension is also sometimes extended downwards in dimension, with an (edge) seen as a  -polytope bounded by a point pair, and a point or vertex as a  -polytope. this approach is used for example in the theory of abstract polytopes. in certain fields of mathematics, the terms ""polytope"" and ""polyhedron"" are used in a different sense: a polyhedron is the generic object in any dimension (referred to as polytope in this article) and polytope means a bounded polyhedron. this terminology is typically confined to polytopes and polyhedra that are convex. with this terminology, a convex polyhedron is the intersection of a finite number of halfspaces and is defined by its sides while a convex polytope is the convex hull of a finite number of points and is defined by its vertices. polytopes in lower numbers of dimensions have standard names: "
343,343,Power Law,0,https://en.wikipedia.org/wiki/Power_law,"in statistics, a power law is a functional relationship between two quantities, where a relative change in one quantity results in a proportional relative change in the other quantity, independent of the initial size of those quantities: one quantity varies as a power of another. for instance, considering the area of a square in terms of the length of its side, if the length is doubled, the area is multiplied by a factor of four. the distributions of a wide variety of physical, biological, and man-made phenomena approximately follow a power law over a wide range of magnitudes: these include the sizes of craters on the moon and of solar flares, the foraging pattern of various species, the sizes of activity patterns of neuronal populations, the frequencies of words in most languages, frequencies of family names, the species richness in clades of organisms, the sizes of power outages, criminal charges per convict, volcanic eruptions, human judgments of stimulus intensity and many other quantities. few empirical distributions fit a power law for all their values, but rather follow a power law in the tail. acoustic attenuation follows frequency power-laws within wide frequency bands for many complex media. allometric scaling laws for relationships between biological variables are among the best known power-law functions in nature. one attribute of power laws is their scale invariance. given a relation f ( x ) = a x − k {\displaystyle f(x)=ax^{-k}} , scaling the argument x {\displaystyle x} by a constant factor c {\displaystyle c} causes only a proportionate scaling of the function itself. that is, where ∝ {\displaystyle \propto } denotes direct proportionality. that is, scaling by a constant c {\displaystyle c} simply multiplies the original power-law relation by the constant c − k {\displaystyle c^{-k}} . thus, it follows that all power laws with a particular scaling exponent are equivalent up to constant factors, since each is simply a scaled version of the others. this behavior is what produces the linear relationship when logarithms are taken of both f ( x ) {\displaystyle f(x)} and x {\displaystyle x} , and the straight-line on the log–log plot is often called the signature of a power law. with real data, such straightness is a necessary, but not sufficient, condition for the data following a power-law relation. in fact, there are many ways to generate finite amounts of data that mimic this signature behavior, but, in their asymptotic limit, are not true power laws (e.g., if the generating process of some data follows a log-normal distribution).[citation needed] thus, accurately fitting and validating power-law models is an active area of research in statistics; see below. a power-law x − k {\displaystyle x^{-k}} has a well-defined mean over x ∈ [   , ∞ ) {\displaystyle x\in [ ,\infty )} only if k >   {\displaystyle k> } , and it has a finite variance only if k >   {\displaystyle k> } ; most identified power laws in nature have exponents such that the mean is well-defined but the variance is not, implying they are capable of black swan behavior. this can be seen in the following thought experiment: imagine a room with your friends and estimate the average monthly income in the room. now imagine the world's richest person entering the room, with a monthly income of about   billion us$. what happens to the average income in the room? income is distributed according to a power-law known as the pareto distribution (for example, the net worth of americans is distributed according to a power law with an exponent of  ). on the one hand, this makes it incorrect to apply traditional statistics that are based on variance and standard deviation (such as regression analysis). on the other hand, this also allows for cost-efficient interventions. for example, given that car exhaust is distributed according to a power-law among cars (very few cars contribute to most contamination) it would be sufficient to eliminate those very few cars from the road to reduce total exhaust substantially. the median does exist, however: for a power law x –k, with exponent k >   {\displaystyle k> } , it takes the value   /(k –  )xmin, where xmin is the minimum value for which the power law holds. the equivalence of power laws with a particular scaling exponent can have a deeper origin in the dynamical processes that generate the power-law relation. in physics, for example, phase transitions in thermodynamic systems are associated with the emergence of power-law distributions of certain quantities, whose exponents are referred to as the critical exponents of the system. diverse systems with the same critical exponents—that is, which display identical scaling behaviour as they approach criticality—can be shown, via renormalization group theory, to share the same fundamental dynamics. for instance, the behavior of water and co  at their boiling points fall in the same universality class because they have identical critical exponents.[citation needed][clarification needed] in fact, almost all material phase transitions are described by a small set of universality classes. similar observations have been made, though not as comprehensively, for various self-organized critical systems, where the critical point of the system is an attractor. formally, this sharing of dynamics is referred to as universality, and systems with precisely the same critical exponents are said to belong to the same universality class. scientific interest in power-law relations stems partly from the ease with which certain general classes of mechanisms generate them. the demonstration of a power-law relation in some data can point to specific kinds of mechanisms that might underlie the natural phenomenon in question, and can indicate a deep connection with other, seemingly unrelated systems; see also universality above. the ubiquity of power-law relations in physics is partly due to dimensional constraints, while in complex systems, power laws are often thought to be signatures of hierarchy or of specific stochastic processes. a few notable examples of power laws are pareto's law of income distribution, structural self-similarity of fractals, and scaling laws in biological systems. research on the origins of power-law relations, and efforts to observe and validate them in the real world, is an active topic of research in many fields of science, including physics, computer science, linguistics, geophysics, neuroscience, systematics, sociology, economics and more. however, much of the recent interest in power laws comes from the study of probability distributions: the distributions of a wide variety of quantities seem to follow the power-law form, at least in their upper tail (large events). the behavior of these large events connects these quantities to the study of theory of large deviations (also called extreme value theory), which considers the frequency of extremely rare events like stock market crashes and large natural disasters. it is primarily in the study of statistical distributions that the name ""power law"" is used. "
344,344,Power rule,1,https://en.wikipedia.org/wiki/Power_rule,"in calculus, the power rule is used to differentiate functions of the form f ( x ) = x r {\displaystyle f(x)=x^{r}} , whenever r {\displaystyle r} is a real number. since differentiation is a linear operation on the space of differentiable functions, polynomials can also be differentiated using this rule. the power rule underlies the taylor series as it relates a power series with a function's derivatives. let f : r ↦ r {\displaystyle f:\mathbb {r} \mapsto \mathbb {r} } be a function satisfying f ( x ) = x r {\displaystyle f(x)=x^{r}} for all x {\displaystyle x} , with r ∈ r {\displaystyle r\in \mathbb {r} } . then, the power rule for integration states that for any real number r ≠ −   {\displaystyle r\neq - } . it can be derived by inverting the power rule for differentiation. to start, we should choose a working definition of the value of f ( x ) = x r {\displaystyle f(x)=x^{r}} , where r {\displaystyle r} is any real number. although it is feasible to define the value as the limit of a sequence of rational powers that approach the irrational power whenever we encounter such a power, or as the least upper bound of a set of rational powers less than the given power, this type of definition is not amenable to differentiation. it is therefore preferable to use a functional definition, which is usually taken to be x r = exp ⁡ ( r ln ⁡ x ) = e r ln ⁡ x {\displaystyle x^{r}=\exp(r\ln x)=e^{r\ln x}} for all values of x >   {\displaystyle x> } , where exp {\displaystyle \exp } is the natural exponential function and e {\displaystyle e} is euler's number. first, we may demonstrate that the derivative of f ( x ) = e x {\displaystyle f(x)=e^{x}} is f ′ ( x ) = e x {\displaystyle f'(x)=e^{x}} . if f ( x ) = e x {\displaystyle f(x)=e^{x}} , then ln ⁡ ( f ( x ) ) = x {\displaystyle \ln(f(x))=x} , where ln {\displaystyle \ln } is the natural logarithm function, the inverse function of the exponential function, as demonstrated by euler. since the latter two functions are equal for all values of x >   {\displaystyle x> } , their derivatives are also equal, whenever either derivative exists, so we have, by the chain rule, when x <   {\displaystyle x< } , we may use the same definition with x r = ( ( −   ) ( − x ) ) r = ( −   ) r ( − x ) r {\displaystyle x^{r}=((- )(-x))^{r}=(- )^{r}(-x)^{r}} , where we now have − x >   {\displaystyle -x> } . this necessarily leads to the same result. note that because ( −   ) r {\displaystyle (- )^{r}} does not have a conventional definition when r {\displaystyle r} is not a rational number, irrational power functions are not well defined for negative bases. in addition, as rational powers of −  with even denominators (in lowest terms) are not real numbers, these expressions are only real valued for rational powers with odd denominators (in lowest terms). finally, whenever the function is differentiable at x =   {\displaystyle x= } , the defining limit for the derivative is: the exclusion of the expression     {\displaystyle  ^{ }} (the case x =  ) from our scheme of exponentiation is due to the fact that the function f ( x , y ) = x y {\displaystyle f(x,y)=x^{y}} has no limit at ( , ), since x   {\displaystyle x^{ }} approaches   as x approaches  , while   y {\displaystyle  ^{y}} approaches   as y approaches  . thus, it would be problematic to ascribe any particular value to it, as the value would contradict one of the two cases, dependent on the application. it is traditionally left undefined. let n be a positive integer. it is required to prove that d d x x n = n x n −   . {\displaystyle {\frac {d}{dx}}x^{n}=nx^{n- }.} "
345,345,Power series,2,https://en.wikipedia.org/wiki/Power_series,"in mathematics, a power series (in one variable) is an infinite series of the form in many situations c (the center of the series) is equal to zero, for instance when considering a maclaurin series. in such cases, the power series takes the simpler form beyond their role in mathematical analysis, power series also occur in combinatorics as generating functions (a kind of formal power series) and in electronic engineering (under the name of the z-transform). the familiar decimal notation for real numbers can also be viewed as an example of a power series, with integer coefficients, but with the argument x fixed at  ⁄  . in number theory, the concept of p-adic numbers is also closely related to that of a power series. any polynomial can be easily expressed as a power series around any center c, although all but finitely many of the coefficients will be zero since a power series has infinitely many terms by definition. for instance, the polynomial f ( x ) = x   +   x +   {\textstyle f(x)=x^{ }+ x+ } can be written as a power series around the center c =   {\textstyle c= } as the geometric series formula valid for all real x. these power series are also examples of taylor series. negative powers are not permitted in a power series; for instance,   + x −   + x −   + ⋯ {\textstyle  +x^{- }+x^{- }+\cdots } is not considered a power series (although it is a laurent series). similarly, fractional powers such as x     {\textstyle x^{\frac { }{ }}} are not permitted (but see puiseux series). the coefficients a n {\textstyle a_{n}} are not allowed to depend on x {\textstyle x} , thus for instance: a power series ∑ n =   ∞ a n ( x − c ) n {\textstyle \sum _{n= }^{\infty }a_{n}(x-c)^{n}} is convergent for some values of the variable x, which will always include x = c (as usual, ( x − c )   {\displaystyle (x-c)^{ }} evaluates as   and the sum of the series is thus a   {\displaystyle a_{ }} for x = c). the series may diverge for other values of x. if c is not the only point of convergence, then there is always a number r with   < r ≤ ∞ such that the series converges whenever |x – c| < r and diverges whenever |x – c| > r. the number r is called the radius of convergence of the power series; in general it is given as the set of the complex numbers such that |x – c| < r is called the disc of convergence of the series. the series converges absolutely inside its disc of convergence, and converges uniformly on every compact subset of the disc of convergence. "
346,346,Power set,1,https://en.wikipedia.org/wiki/Power_set,"in mathematics, the power set (or powerset) of a set s is the set of all subsets of s, including the empty set and s itself. in axiomatic set theory (as developed, for example, in the zfc axioms), the existence of the power set of any set is postulated by the axiom of power set. the powerset of s is variously denoted as p(s), 𝒫(s), p(s), p ( s ) {\displaystyle \mathbb {p} (s)} , ℘(s) (using the ""weierstrass p""), or  s. the notation  s, meaning the set of all functions from s to a given set of two elements (e.g., { ,  }), is used because the powerset of s can be identified with, equivalent to, or bijective to the set of all the functions from s to the given two elements set. any subset of p(s) is called a family of sets over s. if s is the set {x, y, z}, then all the subsets of s are and hence the power set of s is {{}, {x}, {y}, {z}, {x, y}, {x, z}, {y, z}, {x, y, z}}. if s is a finite set with the cardinality |s| = n (i.e., the number of all elements in the set s is n), then the number of all the subsets of s is |p(s)| =  n. this fact as well as the reason of the notation  s denoting the power set p(s) are demonstrated in the below. cantor's diagonal argument shows that the power set of a set (whether infinite or not) always has strictly higher cardinality than the set itself (or informally, the power set must be larger than the original set). in particular, cantor's theorem shows that the power set of a countably infinite set is uncountably infinite. the power set of the set of natural numbers can be put in a one-to-one correspondence with the set of real numbers (see cardinality of the continuum). the power set of a set s, together with the operations of union, intersection and complement, can be viewed as the prototypical example of a boolean algebra. in fact, one can show that any finite boolean algebra is isomorphic to the boolean algebra of the power set of a finite set. for infinite boolean algebras, this is no longer true, but every infinite boolean algebra can be represented as a subalgebra of a power set boolean algebra (see stone's representation theorem). the power set of a set s forms an abelian group when it is considered with the operation of symmetric difference (with the empty set as the identity element and each set being its own inverse), and a commutative monoid when considered with the operation of intersection. it can hence be shown, by proving the distributive laws, that the power set considered together with both of these operations forms a boolean ring. in set theory, xy is the notation representing the set of all functions from y to x. as "" "" can be defined as { , } (see, for example, von neumann ordinals),  s (i.e., { , }s) is the set of all functions from s to { , }. as shown above,  s and the power set of s, p(s), is considered identical set-theoretically. this equivalence can be applied to the example above, in which s = {x, y, z}, to get the isomorphism with the binary representations of numbers from   to  n −  , with n being the number of elements in the set s or |s| = n. first, the enumerated set { (x,  ), (y,  ), (z,  ) } is defined in which the number in each ordered pair represents the position of the paired element of s in a sequence of binary digits such as {x, y} =    ( ); x of s is located at the first from the right of this sequence and y is at the second from the right, and   in the sequence means the element of s corresponding to the position of it in the sequence exists in the subset of s for the sequence while   means it does not. "
347,347,Powe sum symmetric polynomial,0,https://en.wikipedia.org/wiki/Power_sum_symmetric_polynomial,"in mathematics, specifically in commutative algebra, the power sum symmetric polynomials are a type of basic building block for symmetric polynomials, in the sense that every symmetric polynomial with rational coefficients can be expressed as a sum and difference of products of power sum symmetric polynomials with rational coefficients. however, not every symmetric polynomial with integral coefficients is generated by integral combinations of products of power-sum polynomials: they are a generating set over the rationals, but not over the integers. the power sum symmetric polynomial of degree k in n {\displaystyle n} variables x , ..., xn, written pk for k =  ,  ,  , ..., is the sum of all kth powers of the variables. formally, the first few of these polynomials are thus, for each nonnegative integer k {\displaystyle k} , there exists exactly one power sum symmetric polynomial of degree k {\displaystyle k} in n {\displaystyle n} variables. the polynomial ring formed by taking all integral linear combinations of products of the power sum symmetric polynomials is a commutative ring. the following lists the n {\displaystyle n} power sum symmetric polynomials of positive degrees up to n for the first three positive values of n . {\displaystyle n.} in every case, p   = n {\displaystyle p_{ }=n} is one of the polynomials. the list goes up to degree n because the power sum symmetric polynomials of degrees   to n are basic in the sense of the theorem stated below. for n =  : for n =  : for n =  : the set of power sum symmetric polynomials of degrees  ,  , ..., n in n variables generates the ring of symmetric polynomials in n variables. more specifically: "
348,348,Prime Number,1,https://en.wikipedia.org/wiki/Prime_number," a prime number (or a prime) is a natural number greater than   that is not a product of two smaller natural numbers. a natural number greater than   that is not prime is called a composite number. for example,   is prime because the only ways of writing it as a product,   ×   or   ×  , involve   itself. however,   is composite because it is a product (  ×  ) in which both numbers are smaller than  . primes are central in number theory because of the fundamental theorem of arithmetic: every natural number greater than   is either a prime itself or can be factorized as a product of primes that is unique up to their order. the property of being prime is called primality. a simple but slow method of checking the primality of a given number n {\displaystyle n} , called trial division, tests whether n {\displaystyle n} is a multiple of any integer between   and n {\displaystyle {\sqrt {n}}} . faster algorithms include the miller–rabin primality test, which is fast but has a small chance of error, and the aks primality test, which always produces the correct answer in polynomial time but is too slow to be practical. particularly fast methods are available for numbers of special forms, such as mersenne numbers. as of december     [update] the largest known prime number is a mersenne prime with   ,   ,    decimal digits. there are infinitely many primes, as demonstrated by euclid around     bc. no known simple formula separates prime numbers from composite numbers. however, the distribution of primes within the natural numbers in the large can be statistically modelled. the first result in that direction is the prime number theorem, proven at the end of the   th century, which says that the probability of a randomly chosen large number being prime is inversely proportional to its number of digits, that is, to its logarithm. several historical questions regarding prime numbers are still unsolved. these include goldbach's conjecture, that every even integer greater than   can be expressed as the sum of two primes, and the twin prime conjecture, that there are infinitely many pairs of primes having just one even number between them. such questions spurred the development of various branches of number theory, focusing on analytic or algebraic aspects of numbers. primes are used in several routines in information technology, such as public-key cryptography, which relies on the difficulty of factoring large numbers into their prime factors. in abstract algebra, objects that behave in a generalized way like prime numbers include prime elements and prime ideals. a natural number ( ,  ,  ,  ,  ,  , etc.) is called a prime number (or a prime) if it is greater than   and cannot be written as the product of two smaller natural numbers. the numbers greater than   that are not prime are called composite numbers. in other words, n {\displaystyle n} is prime if n {\displaystyle n} items cannot be divided up into smaller equal-size groups of more than one item, or if it is not possible to arrange n {\displaystyle n} dots into a rectangular grid that is more than one dot wide and more than one dot high. for example, among the numbers   through  , the numbers  ,  , and   are the prime numbers, as there are no other numbers that divide them evenly (without a remainder).   is not prime, as it is specifically excluded in the definition.   =   ×   and   =   ×   are both composite. the divisors of a natural number n {\displaystyle n} are the natural numbers that divide n {\displaystyle n} evenly. every natural number has both   and itself as a divisor. if it has any other divisor, it cannot be prime. this idea leads to a different but equivalent definition of the primes: they are the numbers with exactly two positive divisors,   and the number itself. yet another way to express the same thing is that a number n {\displaystyle n} is prime if it is greater than one and if none of the numbers   ,   , … , n −   {\displaystyle  , ,\dots ,n- } divides n {\displaystyle n} evenly. the first    prime numbers (all the prime numbers less than    ) are: no even number n {\displaystyle n} greater than   is prime because any such number can be expressed as the product   × n /   {\displaystyle  \times n/ } . therefore, every prime number other than   is an odd number, and is called an odd prime. similarly, when written in the usual decimal system, all prime numbers larger than   end in  ,  ,  , or  . the numbers that end with other digits are all composite: decimal numbers that end in  ,  ,  ,  , or   are even, and decimal numbers that end in   or   are divisible by  . "
349,349,Prime number theorem,1,https://en.wikipedia.org/wiki/Prime_number_theorem,"in mathematics, the prime number theorem (pnt) describes the asymptotic distribution of the prime numbers among the positive integers. it formalizes the intuitive idea that primes become less common as they become larger by precisely quantifying the rate at which this occurs. the theorem was proved independently by jacques hadamard and charles jean de la vallée poussin in      using ideas introduced by bernhard riemann (in particular, the riemann zeta function). the first such distribution found is π(n) ~ n/log(n), where π(n) is the prime-counting function (the number of primes less than or equal to n) and log(n) is the natural logarithm of n. this means that for large enough n, the probability that a random integer not greater than n is prime is very close to   / log(n). consequently, a random integer with at most  n digits (for large enough n) is about half as likely to be prime as a random integer with at most n digits. for example, among the positive integers of at most      digits, about one in      is prime (log(      ) ≈     . ), whereas among positive integers of at most      digits, about one in      is prime (log(      ) ≈     . ). in other words, the average gap between consecutive prime numbers among the first n integers is roughly log(n). let π(x) be the prime-counting function defined to be the number of primes less than or equal to x, for any real number x. for example, π(  ) =   because there are four prime numbers ( ,  ,   and  ) less than or equal to   . the prime number theorem then states that x / log x is a good approximation to π(x) (where log here means the natural logarithm), in the sense that the limit of the quotient of the two functions π(x) and x / log x as x increases without bound is  : known as the asymptotic law of distribution of prime numbers. using asymptotic notation this result can be restated as this notation (and the theorem) does not say anything about the limit of the difference of the two functions as x increases without bound. instead, the theorem states that x / log x approximates π(x) in the sense that the relative error of this approximation approaches   as x increases without bound. the prime number theorem is equivalent to the statement that the nth prime number pn satisfies the asymptotic notation meaning, again, that the relative error of this approximation approaches   as n increases without bound. for example, the  ×    th prime number is                    , and ( ×    )log( ×    ) rounds to                    , a relative error of about  . %. on the other hand, the following asymptotic relations are logically equivalent as outlined below, the prime number theorem is also equivalent to where ϑ and ψ are the first and the second chebyshev functions respectively, and to "
350,350,Independent Events,1,https://en.wikipedia.org/wiki/Probability," probability is the branch of mathematics concerning numerical descriptions of how likely an event is to occur, or how likely it is that a proposition is true. the probability of an event is a number between   and  , where, roughly speaking,   indicates impossibility of the event and   indicates certainty.[note  ] the higher the probability of an event, the more likely it is that the event will occur. a simple example is the tossing of a fair (unbiased) coin. since the coin is fair, the two outcomes (""heads"" and ""tails"") are both equally probable; the probability of ""heads"" equals the probability of ""tails""; and since no other outcomes are possible, the probability of either ""heads"" or ""tails"" is  /  (which could also be written as  .  or   %). these concepts have been given an axiomatic mathematical formalization in probability theory, which is used widely in areas of study such as statistics, mathematics, science, finance, gambling, artificial intelligence, machine learning, computer science, game theory, and philosophy to, for example, draw inferences about the expected frequency of events. probability theory is also used to describe the underlying mechanics and regularities of complex systems. when dealing with experiments that are random and well-defined in a purely theoretical setting (like tossing a coin), probabilities can be numerically described by the number of desired outcomes, divided by the total number of all outcomes. for example, tossing a coin twice will yield ""head-head"", ""head-tail"", ""tail-head"", and ""tail-tail"" outcomes. the probability of getting an outcome of ""head-head"" is   out of   outcomes, or, in numerical terms,  / ,  .   or   %. however, when it comes to practical application, there are two major competing categories of probability interpretations, whose adherents hold different views about the fundamental nature of probability: the word probability derives from the latin probabilitas, which can also mean ""probity"", a measure of the authority of a witness in a legal case in europe, and often correlated with the witness's nobility. in a sense, this differs much from the modern meaning of probability, which in contrast is a measure of the weight of empirical evidence, and is arrived at from inductive reasoning and statistical inference. the scientific study of probability is a modern development of mathematics. gambling shows that there has been an interest in quantifying the ideas of probability for millennia, but exact mathematical descriptions arose much later. there are reasons for the slow development of the mathematics of probability. whereas games of chance provided the impetus for the mathematical study of probability, fundamental issues [note  ] are still obscured by the superstitions of gamblers. according to richard jeffrey, ""before the middle of the seventeenth century, the term 'probable' (latin probabilis) meant approvable, and was applied in that sense, univocally, to opinion and to action. a probable action or opinion was one such as sensible people would undertake or hold, in the circumstances."" however, in legal contexts especially, 'probable' could also apply to propositions for which there was good evidence. the sixteenth-century italian polymath gerolamo cardano demonstrated the efficacy of defining odds as the ratio of favourable to unfavourable outcomes (which implies that the probability of an event is given by the ratio of favourable outcomes to the total number of possible outcomes ). aside from the elementary work by cardano, the doctrine of probabilities dates to the correspondence of pierre de fermat and blaise pascal (    ). christiaan huygens (    ) gave the earliest known scientific treatment of the subject. jakob bernoulli's ars conjectandi (posthumous,     ) and abraham de moivre's doctrine of chances (    ) treated the subject as a branch of mathematics. see ian hacking's the emergence of probability and james franklin's the science of conjecture for histories of the early development of the very concept of mathematical probability. the theory of errors may be traced back to roger cotes's opera miscellanea (posthumous,     ), but a memoir prepared by thomas simpson in      (printed     ) first applied the theory to the discussion of errors of observation. the reprint (    ) of this memoir lays down the axioms that positive and negative errors are equally probable, and that certain assignable limits define the range of all errors. simpson also discusses continuous errors and describes a probability curve. the first two laws of error that were proposed both originated with pierre-simon laplace. the first law was published in     , and stated that the frequency of an error could be expressed as an exponential function of the numerical magnitude of the error—disregarding sign. the second law of error was proposed in      by laplace, and stated that the frequency of the error is an exponential function of the square of the error. the second law of error is called the normal distribution or the gauss law. ""it is difficult historically to attribute that law to gauss, who in spite of his well-known precocity had probably not made this discovery before he was two years old."" "
351,351,Probability density function,2,https://en.wikipedia.org/wiki/Probability_density_function," in probability theory, a probability density function (pdf), or density of a continuous random variable, is a function whose value at any given sample (or point) in the sample space (the set of possible values taken by the random variable) can be interpreted as providing a relative likelihood that the value of the random variable would be close to that sample. in other words, while the absolute likelihood for a continuous random variable to take on any particular value is   (since there is an infinite set of possible values to begin with), the value of the pdf at two different samples can be used to infer, in any particular draw of the random variable, how much more likely it is that the random variable would be close to one sample compared to the other sample. in a more precise sense, the pdf is used to specify the probability of the random variable falling within a particular range of values, as opposed to taking on any one value. this probability is given by the integral of this variable's pdf over that range—that is, it is given by the area under the density function but above the horizontal axis and between the lowest and greatest values of the range. the probability density function is nonnegative everywhere, and its integral over the entire space is equal to  . the terms ""probability distribution function"" and ""probability function"" have also sometimes been used to denote the probability density function. however, this use is not standard among probabilists and statisticians. in other sources, ""probability distribution function"" may be used when the probability distribution is defined as a function over general sets of values or it may refer to the cumulative distribution function, or it may be a probability mass function (pmf) rather than the density. ""density function"" itself is also used for the probability mass function, leading to further confusion. in general though, the pmf is used in the context of discrete random variables (random variables that take values on a countable set), while the pdf is used in the context of continuous random variables. suppose bacteria of a certain species typically live   to   hours. the probability that a bacterium lives exactly   hours is equal to zero. a lot of bacteria live for approximately   hours, but there is no chance that any given bacterium dies at exactly  .  ... hours. however, the probability that the bacterium dies between   hours and  .   hours is quantifiable. suppose the answer is  .   (i.e.,  %). then, the probability that the bacterium dies between   hours and  .    hours should be about  .   , since this time interval is one-tenth as long as the previous. the probability that the bacterium dies between   hours and  .     hours should be about  .    , and so on. in this example, the ratio (probability of dying during an interval) / (duration of the interval) is approximately constant, and equal to   per hour (or   hour− ). for example, there is  .   probability of dying in the  .  -hour interval between   and  .   hours, and ( .   probability /  .   hours) =   hour− . this quantity   hour−  is called the probability density for dying at around   hours. therefore, the probability that the bacterium dies at   hours can be written as (  hour− ) dt. this is the probability that the bacterium dies within an infinitesimal window of time around   hours, where dt is the duration of this window. for example, the probability that it lives longer than   hours, but shorter than (  hours +   nanosecond), is (  hour− )×(  nanosecond) ≈  ×  −   (using the unit conversion  . ×     nanoseconds =   hour). there is a probability density function f with f(  hours) =   hour− . the integral of f over any window of time (not only infinitesimal windows but also large windows) is the probability that the bacterium dies in that window. a probability density function is most commonly associated with absolutely continuous univariate distributions. a random variable x {\displaystyle x} has density f x {\displaystyle f_{x}} , where f x {\displaystyle f_{x}} is a non-negative lebesgue-integrable function, if: hence, if f x {\displaystyle f_{x}} is the cumulative distribution function of x {\displaystyle x} , then: and (if f x {\displaystyle f_{x}} is continuous at x {\displaystyle x} ) "
352,352,Probability Distribution,1,https://en.wikipedia.org/wiki/Probability_distribution,"in probability theory and statistics, a probability distribution is the mathematical function that gives the probabilities of occurrence of different possible outcomes for an experiment. it is a mathematical description of a random phenomenon in terms of its sample space and the probabilities of events (subsets of the sample space). for instance, if x is used to denote the outcome of a coin toss (""the experiment""), then the probability distribution of x would take the value  .  (  in   or  / ) for x = heads, and  .  for x = tails (assuming that the coin is fair). examples of random phenomena include the weather conditions at some future date, the height of a randomly selected person, the fraction of male students in a school, the results of a survey to be conducted, etc. a probability distribution is a mathematical description of the probabilities of events, subsets of the sample space. the sample space, often denoted by ω {\displaystyle \omega } , is the set of all possible outcomes of a random phenomenon being observed; it may be any set: a set of real numbers, a set of vectors, a set of arbitrary non-numerical values, etc. for example, the sample space of a coin flip would be ω = {heads, tails}. to define probability distributions for the specific case of random variables (so the sample space can be seen as a numeric set), it is common to distinguish between discrete and absolutely continuous random variables. in the discrete case, it is sufficient to specify a probability mass function p {\displaystyle p} assigning a probability to each possible outcome: for example, when throwing a fair die, each of the six values   to   has the probability  / . the probability of an event is then defined to be the sum of the probabilities of the outcomes that satisfy the event; for example, the probability of the event ""the die rolls an even value"" is in contrast, when a random variable takes values from a continuum then typically, any individual outcome has probability zero and only events that include infinitely many outcomes, such as intervals, can have positive probability. for example, consider measuring the weight of a piece of ham in the supermarket, and assume the scale has many digits of precision. the probability that it weighs exactly     g is zero, as it will most likely have some non-zero decimal digits. nevertheless, one might demand, in quality control, that a package of ""    g"" of ham must weigh between     g and     g with at least   % probability, and this demand is less sensitive to the accuracy of measurement instruments. absolutely continuous probability distributions can be described in several ways. the probability density function describes the infinitesimal probability of any given value, and the probability that the outcome lies in a given interval can be computed by integrating the probability density function over that interval. an alternative description of the distribution is by means of the cumulative distribution function, which describes the probability that the random variable is no larger than a given value (i.e., p ( x < x ) {\displaystyle p(x<x)} for some x {\displaystyle x} ). the cumulative distribution function is the area under the probability density function from − ∞ {\displaystyle -\infty } to x {\displaystyle x} , as described by the picture to the right. a probability distribution can be described in various forms, such as by a probability mass function or a cumulative distribution function. one of the most general descriptions, which applies for absolutely continuous and discrete variables, is by means of a probability function p : a → r {\displaystyle p\colon {\mathcal {a}}\to \mathbb {r} } whose input space a {\displaystyle {\mathcal {a}}} is related to the sample space, and gives a real number probability as its output. the probability function p {\displaystyle p} can take as argument subsets of the sample space itself, as in the coin toss example, where the function p {\displaystyle p} was defined so that p(heads) =  .  and p(tails) =  . . however, because of the widespread use of random variables, which transform the sample space into a set of numbers (e.g., r {\displaystyle \mathbb {r} } , n {\displaystyle \mathbb {n} } ), it is more common to study probability distributions whose argument are subsets of these particular kinds of sets (number sets), and all probability distributions discussed in this article are of this type. it is common to denote as p ( x ∈ e ) {\displaystyle p(x\in e)} ) the probability that a certain variable x {\displaystyle x} belongs to a certain event e {\displaystyle e} . the above probability function only characterizes a probability distribution if it satisfies all the kolmogorov axioms, that is: the concept of probability function is made more rigorous by defining it as the element of a probability space ( x , a , p ) {\displaystyle (x,{\mathcal {a}},p)} , where x {\displaystyle x} is the set of possible outcomes, a {\displaystyle {\mathcal {a}}} is the set of all subsets e ⊂ x {\displaystyle e\subset x} whose probability can be measured, and p {\displaystyle p} is the probability function, or probability measure, that assigns a probability to each of these measurable subsets e ∈ a {\displaystyle e\in {\mathcal {a}}} . "
353,353,Probability Mass Function,1,https://en.wikipedia.org/wiki/Probability_mass_function,"in probability and statistics, a probability mass function is a function that gives the probability that a discrete random variable is exactly equal to some value. sometimes it is also known as the discrete density function. the probability mass function is often the primary means of defining a discrete probability distribution, and such functions exist for either scalar or multivariate random variables whose domain is discrete. a probability mass function differs from a probability density function (pdf) in that the latter is associated with continuous rather than discrete random variables. a pdf must be integrated over an interval to yield a probability. the value of the random variable having the largest probability mass is called the mode. probability mass function is the probability distribution of a discrete random variable, and provides the possible values and their associated probabilities. it is the function p : r → [   ,   ] {\displaystyle p:\mathbb {r} \to [ , ]} defined by p x ( x ) = p ( x = x ) {\displaystyle p_{x}(x)=p(x=x)} for − ∞ < x < ∞ {\displaystyle -\infty <x<\infty } , where p {\displaystyle p} is a probability measure. p x ( x ) {\displaystyle p_{x}(x)} can also be simplified as p ( x ) {\displaystyle p(x)} . the probabilities associated with all (hypothetical) values must be non-negative and sum up to  , thinking of probability as mass helps to avoid mistakes since the physical mass is conserved as is the total probability for all hypothetical outcomes x {\displaystyle x} . a probability mass function of a discrete random variable x {\displaystyle x} can be seen as a special case of two more general measure theoretic constructions: the distribution of x {\displaystyle x} and the probability density function of x {\displaystyle x} with respect to the counting measure. we make this more precise below. "
354,354,computation of probability of events,1,https://en.wikipedia.org/wiki/Probability_theory,"probability theory is the branch of mathematics concerned with probability. although there are several different probability interpretations, probability theory treats the concept in a rigorous mathematical manner by expressing it through a set of axioms. typically these axioms formalise probability in terms of a probability space, which assigns a measure taking values between   and  , termed the probability measure, to a set of outcomes called the sample space. any specified subset of the sample space is called an event. central subjects in probability theory include discrete and continuous random variables, probability distributions, and stochastic processes, which provide mathematical abstractions of non-deterministic or uncertain processes or measured quantities that may either be single occurrences or evolve over time in a random fashion. although it is not possible to perfectly predict random events, much can be said about their behavior. two major results in probability theory describing such behaviour are the law of large numbers and the central limit theorem. as a mathematical foundation for statistics, probability theory is essential to many human activities that involve quantitative analysis of data. methods of probability theory also apply to descriptions of complex systems given only partial knowledge of their state, as in statistical mechanics or sequential estimation. a great discovery of twentieth-century physics was the probabilistic nature of physical phenomena at atomic scales, described in quantum mechanics. [unreliable source?] the modern mathematical theory of probability has its roots in attempts to analyze games of chance by gerolamo cardano in the sixteenth century, and by pierre de fermat and blaise pascal in the seventeenth century (for example the ""problem of points""). christiaan huygens published a book on the subject in      and in the   th century, pierre laplace completed what is today considered the classic interpretation. initially, probability theory mainly considered discrete events, and its methods were mainly combinatorial. eventually, analytical considerations compelled the incorporation of continuous variables into the theory. this culminated in modern probability theory, on foundations laid by andrey nikolaevich kolmogorov. kolmogorov combined the notion of sample space, introduced by richard von mises, and measure theory and presented his axiom system for probability theory in     . this became the mostly undisputed axiomatic basis for modern probability theory; but, alternatives exist, such as the adoption of finite rather than countable additivity by bruno de finetti. most introductions to probability theory treat discrete probability distributions and continuous probability distributions separately. the measure theory-based treatment of probability covers the discrete, continuous, a mix of the two, and more. consider an experiment that can produce a number of outcomes. the set of all outcomes is called the sample space of the experiment. the power set of the sample space (or equivalently, the event space) is formed by considering all different collections of possible results. for example, rolling an honest die produces one of six possible results. one collection of possible results corresponds to getting an odd number. thus, the subset { , , } is an element of the power set of the sample space of die rolls. these collections are called events. in this case, { , , } is the event that the die falls on some odd number. if the results that actually occur fall in a given event, that event is said to have occurred. probability is a way of assigning every ""event"" a value between zero and one, with the requirement that the event made up of all possible results (in our example, the event { , , , , , }) be assigned a value of one. to qualify as a probability distribution, the assignment of values must satisfy the requirement that if you look at a collection of mutually exclusive events (events that contain no common results, e.g., the events { , }, { }, and { , } are all mutually exclusive), the probability that any of these events occurs is given by the sum of the probabilities of the events. the probability that any one of the events { , }, { }, or { , } will occur is  / . this is the same as saying that the probability of event { , , , , } is  / . this event encompasses the possibility of any number except five being rolled. the mutually exclusive event { } has a probability of  / , and the event { , , , , , } has a probability of  , that is, absolute certainty. when doing calculations using the outcomes of an experiment, it is necessary that all those elementary events have a number assigned to them. this is done using a random variable. a random variable is a function that assigns to each elementary event in the sample space a real number. this function is usually denoted by a capital letter. in the case of a die, the assignment of a number to a certain elementary events can be done using the identity function. this does not always work. for example, when flipping a coin the two possible outcomes are ""heads"" and ""tails"". in this example, the random variable x could assign to the outcome ""heads"" the number "" "" ( x ( h e a d s ) =   {\displaystyle x(heads)= } ) and to the outcome ""tails"" the number "" "" ( x ( t a i l s ) =   {\displaystyle x(tails)= } ). "
355,355,Product Rule,1,https://en.wikipedia.org/wiki/Product_rule," in calculus, the product rule (or leibniz rule or leibniz product rule) is a formula used to find the derivatives of products of two or more functions. for two functions, it may be stated in lagrange's notation as or in leibniz's notation as the rule may be extended or generalized to products of three or more functions, to a rule for higher-order derivatives of a product, and to other contexts. discovery of this rule is credited to gottfried leibniz, who demonstrated it using differentials. (however, j. m. child, a translator of leibniz's papers, argues that it is due to isaac barrow.) here is leibniz's argument: let u(x) and v(x) be two differentiable functions of x. then the differential of uv is since the term du·dv is ""negligible"" (compared to du and dv), leibniz concluded that and this is indeed the differential form of the product rule. if we divide through by the differential dx, we obtain which can also be written in lagrange's notation as let h(x) = f(x)g(x) and suppose that f and g are each differentiable at x. we want to prove that h is differentiable at x and that its derivative, h′(x), is given by f′(x)g(x) + f(x)g′(x). to do this, f ( x ) g ( x + δ x ) − f ( x ) g ( x + δ x ) {\displaystyle f(x)g(x+\delta x)-f(x)g(x+\delta x)} (which is zero, and thus does not change the value) is added to the numerator to permit its factoring, and then properties of limits are used. the fact that "
356,356,progressions,1,https://en.wikipedia.org/wiki/Progression,progression may refer to: in mathematics: in music: in other fields: 
357,357,Projection (mathematics),2,https://en.wikipedia.org/wiki/Projection_(mathematics),"in mathematics, a projection is a mapping of a set (or other mathematical structure) into a subset (or sub-structure), which is equal to its square for mapping composition, i.e., which is idempotent. the restriction to a subspace of a projection is also called a projection, even if the idempotence property is lost. an everyday example of a projection is the casting of shadows onto a plane (sheet of paper): the projection of a point is its shadow on the sheet of paper, and the projection (shadow) of a point on the sheet of paper is that point itself (idempotency). the shadow of a three-dimensional sphere is a closed disk. originally, the notion of projection was introduced in euclidean geometry to denote the projection of the three-dimensional euclidean space onto a plane in it, like the shadow example. the two main projections of this kind are: the concept of projection in mathematics is a very old one, and most likely has its roots in the phenomenon of the shadows cast by real-world objects on the ground. this rudimentary idea was refined and abstracted, first in a geometric context and later in other branches of mathematics. over time different versions of the concept developed, but today, in a sufficiently abstract setting, we can unify these variations.[citation needed] in cartography, a map projection is a map of a part of the surface of the earth onto a plane, which, in some cases, but not always, is the restriction of a projection in the above meaning. the  d projections are also at the basis of the theory of perspective.[citation needed] the need for unifying the two kinds of projections and of defining the image by a central projection of any point different of the center of projection are at the origin of projective geometry. however, a projective transformation is a bijection of a projective space, a property not shared with the projections of this article.[citation needed] in an abstract setting we can generally say that a projection is a mapping of a set (or of a mathematical structure) which is idempotent, which means that a projection is equal to its composition with itself. a projection may also refer to a mapping which has a right inverse. both notions are strongly related, as follows. let p be an idempotent mapping from a set a into itself (thus p ∘ p = p) and b = p(a) be the image of p. if we denote by π the map p viewed as a map from a onto b and by i the injection of b into a (so that p = i ∘ π), then we have π ∘ i = idb (so that π has a right inverse). conversely, if π has a right inverse, then π ∘ i = idb implies that i ∘ π is idempotent.[citation needed] the original notion of projection has been extended or generalized to various mathematical situations, frequently, but not always, related to geometry, for example: "
358,358,Projective plane,3,https://en.wikipedia.org/wiki/Projective_plane," in mathematics, a projective plane is a geometric structure that extends the concept of a plane. in the ordinary euclidean plane, two lines typically intersect in a single point, but there are some pairs of lines (namely, parallel lines) that do not intersect. a projective plane can be thought of as an ordinary plane equipped with additional ""points at infinity"" where parallel lines intersect. thus any two distinct lines in a projective plane intersect in one and only one point. renaissance artists, in developing the techniques of drawing in perspective, laid the groundwork for this mathematical topic. the archetypical example is the real projective plane, also known as the extended euclidean plane. this example, in slightly different guises, is important in algebraic geometry, topology and projective geometry where it may be denoted variously by pg( , r), rp , or p (r), among other notations. there are many other projective planes, both infinite, such as the complex projective plane, and finite, such as the fano plane. a projective plane is a  -dimensional projective space, but not all projective planes can be embedded in  -dimensional projective spaces. such embeddability is a consequence of a property known as desargues' theorem, not shared by all projective planes. a projective plane consists of a set of lines, a set of points, and a relation between points and lines called incidence, having the following properties: the second condition means that there are no parallel lines. the last condition excludes the so-called degenerate cases (see below). the term ""incidence"" is used to emphasize the symmetric nature of the relationship between points and lines. thus the expression ""point p is incident with line ℓ "" is used instead of either ""p is on ℓ "" or ""ℓ passes through p "". to turn the ordinary euclidean plane into a projective plane proceed as follows: the extended structure is a projective plane and is called the extended euclidean plane or the real projective plane. the process outlined above, used to obtain it, is called ""projective completion"" or projectivization. this plane can also be constructed by starting from r  viewed as a vector space, see § vector space construction below. the points of the moulton plane are the points of the euclidean plane, with coordinates in the usual way. to create the moulton plane from the euclidean plane some of the lines are redefined. that is, some of their point sets will be changed, but other lines will remain unchanged. redefine all the lines with negative slopes so that they look like ""bent"" lines, meaning that these lines keep their points with negative x-coordinates, but the rest of their points are replaced with the points of the line with the same y-intercept but twice the slope wherever their x-coordinate is positive. the moulton plane has parallel classes of lines and is an affine plane. it can be projectivized, as in the previous example, to obtain the projective moulton plane. desargues' theorem is not a valid theorem in either the moulton plane or the projective moulton plane. "
359,359,Proof by Contradiction,0,https://en.wikipedia.org/wiki/Proof_by_contradiction,"in logic and mathematics, proof by contradiction is a form of proof that establishes the truth or the validity of a proposition, by showing that assuming the proposition to be false leads to a contradiction. proof by contradiction is also known as indirect proof, proof by assuming the opposite, and reductio ad impossibile. proof by contradiction is based on the law of noncontradiction as first formalized as a metaphysical principle by aristotle. noncontradiction is also a theorem in propositional logic. this states that an assertion or mathematical statement cannot be both true and false. that is, a proposition q and its negation ¬ {\displaystyle \lnot } q (""not-q"") cannot both be true. in a proof by contradiction, it is shown that the denial of the statement being proved results in such a contradiction. it has the form of a reductio ad absurdum argument, and usually proceeds as follows: the  rd step is based on the following possible truth value cases of a valid argument p → q. it tells that if a false statement is reached via a valid logic from an assumed statement, then the assumed statement is a false statement. this fact is used in proof by contradiction. proof by contradiction is formulated as p ≡ p ∨ ⊥ ≡ ¬ ( ¬ p ) ∨ ⊥ ≡ ¬ p → ⊥ {\displaystyle {\text{p}}\equiv {\text{p}}\vee \bot \equiv \lnot \left(\lnot {\text{p}}\right)\vee \bot \equiv \lnot {\text{p}}\to \bot } , where ⊥ {\displaystyle \bot } is a logical contradiction or a false statement (a statement which truth value is false). if ⊥ {\displaystyle \bot } is reached from ¬ {\displaystyle \lnot } p via a valid logic, then ¬ p → ⊥ {\displaystyle \lnot {\text{p}}\to \bot } is proved as true so p is proved as true. an alternate form of proof by contradiction derives a contradiction with the statement to be proved by showing that ¬ {\displaystyle \lnot } p implies p. this is a contradiction so the assumption ¬ {\displaystyle \lnot } p must be false, equivalently p as true. this is formulated as p ≡ p ∨ p ≡ ¬ ( ¬ p ) ∨ p ≡ ¬ p → p {\displaystyle {\text{p}}\equiv {\text{p}}\vee {\text{p}}\equiv \lnot \left(\lnot {\text{p}}\right)\vee {\text{p}}\equiv \lnot {\text{p}}\to {\text{p}}} . an existence proof by contradiction assumes that some object doesn't exist, and then proves that this would lead to a contradiction; thus, such an object must exist. although it is quite freely used in mathematical proofs, not every school of mathematical thought accepts this kind of nonconstructive proof as universally valid. proof by contradiction also depends on the law of the excluded middle, also first formulated by aristotle. this states that either an assertion or its negation must be true that is, there is no other truth value besides ""true"" and ""false"" that a proposition can take. combined with the principle of noncontradiction, this means that exactly one of p {\displaystyle p} and ¬ p {\displaystyle \lnot p} is true. in proof by contradiction, this permits the conclusion that since the possibility of ¬ p {\displaystyle \lnot p} has been excluded, p {\displaystyle p} must be true. intuitionist mathematicians do not accept the law of the excluded middle, and thus reject arbitrary proof by contradiction as a viable proof technique. however, they do accept the following variation, called ""proof of negation"". "
360,360,Prepositional Logic,1,https://en.wikipedia.org/wiki/Propositional_calculus," propositional calculus is a branch of logic. it is also called propositional logic, statement logic, sentential calculus, sentential logic, or sometimes zeroth-order logic. it deals with propositions (which can be true or false) and relations between propositions, including the construction of arguments based on them. compound propositions are formed by connecting propositions by logical connectives. propositions that contain no logical connectives are called atomic propositions. unlike first-order logic, propositional logic does not deal with non-logical objects, predicates about them, or quantifiers. however, all the machinery of propositional logic is included in first-order logic and higher-order logics. in this sense, propositional logic is the foundation of first-order logic and higher-order logic. logical connectives are found in natural languages. in english for example, some examples are ""and"" (conjunction), ""or"" (disjunction), ""not"" (negation) and ""if"" (but only when used to denote material conditional). the following is an example of a very simple inference within the scope of propositional logic: both premises and the conclusion are propositions. the premises are taken for granted, and with the application of modus ponens (an inference rule), the conclusion follows. as propositional logic is not concerned with the structure of propositions beyond the point where they can't be decomposed any more by logical connectives, this inference can be restated replacing those atomic statements with statement letters, which are interpreted as variables representing statements: the same can be stated succinctly in the following way: when p is interpreted as ""it's raining"" and q as ""it's cloudy"" the above symbolic expressions can be seen to correspond exactly with the original expression in natural language. not only that, but they will also correspond with any other inference of this form, which will be valid on the same basis this inference is. propositional logic may be studied through a formal system in which formulas of a formal language may be interpreted to represent propositions. a system of axioms and inference rules allows certain formulas to be derived. these derived formulas are called theorems and may be interpreted to be true propositions. a constructed sequence of such formulas is known as a derivation or proof and the last formula of the sequence is the theorem. the derivation may be interpreted as proof of the proposition represented by the theorem. "
361,361,Ptolemy theorm,1,https://en.wikipedia.org/wiki/Ptolemy%27s_theorem,"in euclidean geometry, ptolemy's theorem is a relation between the four sides and two diagonals of a cyclic quadrilateral (a quadrilateral whose vertices lie on a common circle). the theorem is named after the greek astronomer and mathematician ptolemy (claudius ptolemaeus). ptolemy used the theorem as an aid to creating his table of chords, a trigonometric table that he applied to astronomy. if the vertices of the cyclic quadrilateral are a, b, c, and d in order, then the theorem states that: where the vertical lines denote the lengths of the line segments between the named vertices. this relation may be verbally expressed as follows: moreover, the converse of ptolemy's theorem is also true: ptolemy's theorem yields as a corollary a pretty theorem regarding an equilateral triangle inscribed in a circle. given an equilateral triangle inscribed on a circle and a point on the circle. the distance from the point to the most distant vertex of the triangle is the sum of the distances from the point to the two nearer vertices. proof: follows immediately from ptolemy's theorem: any square can be inscribed in a circle whose center is the center of the square. if the common length of its four sides is equal to a {\displaystyle a} then the length of the diagonal is equal to a   {\displaystyle a{\sqrt { }}} according to the pythagorean theorem and the relation obviously holds. more generally, if the quadrilateral is a rectangle with sides a and b and diagonal d then ptolemy's theorem reduces to the pythagorean theorem. in this case the center of the circle coincides with the point of intersection of the diagonals. the product of the diagonals is then d , the right hand side of ptolemy's relation is the sum a  + b . "
362,362,Pythagoras theorem,1,https://en.wikipedia.org/wiki/Pythagorean_theorem," in mathematics, the pythagorean theorem, or pythagoras' theorem, is a fundamental relation in euclidean geometry among the three sides of a right triangle. it states that the area of the square whose side is the hypotenuse (the side opposite the right angle) is equal to the sum of the areas of the squares on the other two sides. this theorem can be written as an equation relating the lengths of the legs a, b and the hypotenuse c, often called the pythagorean equation: the theorem, whose history is the subject of much debate, is named for the greek philosopher pythagoras, born around     bc. the theorem has been proven numerous times by many different methods – possibly the most for any mathematical theorem. the proofs are diverse, including both geometric proofs and algebraic proofs, with some dating back thousands of years. the theorem can be generalized in various ways: to higher-dimensional spaces, to spaces that are not euclidean, to objects that are not right triangles, and to objects that are not triangles at all but n-dimensional solids. the pythagorean theorem has attracted interest outside mathematics as a symbol of mathematical abstruseness, mystique, or intellectual power; popular references in literature, plays, musicals, songs, stamps, and cartoons abound. if c denotes the length of the hypotenuse and a and b denote the two lengths of the legs of a right triangle, then the pythagorean theorem can be expressed as the pythagorean equation: if only the lengths of the legs of the right triangle are known but not the hypotenuse, then the length of the hypotenuse can be calculated with the equation if the length of the hypotenuse and of one leg is known, then the length of the other leg can be calculated as or a generalization of this theorem is the law of cosines, which allows the computation of the length of any side of any triangle, given the lengths of the other two sides and the angle between them. if the angle between the other sides is a right angle, the law of cosines reduces to the pythagorean equation. "
363,363,Quadratic Equation,1,https://en.wikipedia.org/wiki/Quadratic_equation,"in algebra, a quadratic equation (from latin quadratus 'square') is any equation that can be rearranged in standard form as where x represents an unknown, and a, b, and c represent known numbers, where a ≠  . if a =  , then the equation is linear, not quadratic, as there is no a x   {\displaystyle ax^{ }} term. the numbers a, b, and c are the coefficients of the equation and may be distinguished by calling them, respectively, the quadratic coefficient, the linear coefficient and the constant or free term. the values of x that satisfy the equation are called solutions of the equation, and roots or zeros of the expression on its left-hand side. a quadratic equation has at most two solutions. if there is only one solution, one says that it is a double root. if all the coefficients are real numbers, there are either two real solutions, or a single real double root, or two complex solutions. a quadratic equation always has two roots, if complex roots are included; and a double root is counted for two. a quadratic equation can be factored into an equivalent equation where r and s are the solutions for x. completing the square on a quadratic equation in standard form results in the quadratic formula, which expresses the solutions in terms of a, b, and c. solutions to problems that can be expressed in terms of quadratic equations were known as early as      bc. because the quadratic equation involves only one unknown, it is called ""univariate"". the quadratic equation contains only powers of x that are non-negative integers, and therefore it is a polynomial equation. in particular, it is a second-degree polynomial equation, since the greatest power is two. a quadratic equation with real or complex coefficients has two solutions, called roots. these two solutions may or may not be distinct, and they may or may not be real. it may be possible to express a quadratic equation ax  + bx + c =   as a product (px + q)(rx + s) =  . in some cases, it is possible, by simple inspection, to determine values of p, q, r, and s that make the two forms equivalent to one another. if the quadratic equation is written in the second form, then the ""zero factor property"" states that the quadratic equation is satisfied if px + q =   or rx + s =  . solving these two linear equations provides the roots of the quadratic. for most students, factoring by inspection is the first method of solving quadratic equations to which they are exposed. :    –    if one is given a quadratic equation in the form x  + bx + c =  , the sought factorization has the form (x + q)(x + s), and one has to find two numbers q and s that add up to b and whose product is c (this is sometimes called ""vieta's rule"" and is related to vieta's formulas). as an example, x  +  x +   factors as (x +  )(x +  ). the more general case where a does not equal   can require a considerable effort in trial and error guess-and-check, assuming that it can be factored at all by inspection. except for special cases such as where b =   or c =  , factoring by inspection only works for quadratic equations that have rational roots. this means that the great majority of quadratic equations that arise in practical applications cannot be solved by factoring by inspection. :     the process of completing the square makes use of the algebraic identity "
364,364,Quadratic Formula,1,https://en.wikipedia.org/wiki/Quadratic_formula,"in elementary algebra, the quadratic formula is a formula that provides the solution(s) to a quadratic equation. there are other ways of solving a quadratic equation instead of using the quadratic formula, such as factoring (direct factoring, grouping, ac method), completing the square, graphing and others. given a general quadratic equation of the form whose discriminant b   −   a c {\displaystyle b^{ }- ac} is positive (with x representing an unknown, a, b and c representing constants with a ≠  ), the quadratic formula is: where the plus–minus symbol ""±"" indicates that the quadratic equation has two solutions. written separately, they become: each of these two solutions is also called a root (or zero) of the quadratic equation. geometrically, these roots represent the x-values at which any parabola, explicitly given as y = ax  + bx + c, crosses the x-axis. as well as being a formula that yields the zeros of any parabola, the quadratic formula can also be used to identify the axis of symmetry of the parabola, and the number of real zeros the quadratic equation contains. the quadratic formula, in the case when the discriminant b   −   a c {\displaystyle b^{ }- ac} is positive, may also be written as which may be simplified to this version of the formula makes it easy to find the roots when using a calculator. in the case when the discriminant b   −   a c {\displaystyle b^{ }- ac} is negative, complex roots are involved. the new quadratic formula is then the following expression (in which the expression outside the square root is the real part and the square root expression is the imaginary part): a lesser known quadratic formula, which is used in muller's method and which can be found from vieta's formulas, provides (assuming a ≠  , c ≠  ) the same roots via the equation: "
365,365,Quadratic Function,1,https://en.wikipedia.org/wiki/Quadratic_function,"in algebra, a quadratic function, a quadratic polynomial, a polynomial of degree  , or simply a quadratic, is a polynomial function with one or more variables in which the highest-degree term is of the second degree. for example, a univariate (single-variable) quadratic function has the form in the single variable x. the graph of a univariate quadratic function is a parabola whose axis of symmetry is parallel to the y-axis, as shown at right. if the quadratic function is set equal to zero, then the result is a quadratic equation. the solutions to the univariate equation are called the roots of the univariate function. the bivariate case in terms of variables x and y has the form with at least one of a, b, c not equal to zero, and an equation setting this function equal to zero gives rise to a conic section (a circle or other ellipse, a parabola, or a hyperbola). a quadratic function in three variables x, y, and z contains exclusively terms x , y , z , xy, xz, yz, x, y, z, and a constant: with at least one of the coefficients a, b, c, d, e, or f of the second-degree terms being non-zero. in general there can be an arbitrarily large number of variables, in which case the resulting surface of setting a quadratic function to zero is called a quadric, but the highest degree term must be of degree  , such as x , xy, yz, etc. the adjective quadratic comes from the latin word quadrātum (""square""). a term like x  is called a square in algebra because it is the area of a square with side x. "
366,366,Quantile Function,0,https://en.wikipedia.org/wiki/Quantile_function,"in probability and statistics, the quantile function, associated with a probability distribution of a random variable, specifies the value of the random variable such that the probability of the variable being less than or equal to that value equals the given probability. intuitively, the quantile function associates with a range at and below a probability input the likelihood that a random variable is realized in that range for some probability distribution. it is also called the percentile function, percent-point function or inverse cumulative distribution function. with reference to a continuous and strictly monotonic distribution function, for example the cumulative distribution function f x : r → [   ,   ] {\displaystyle f_{x}\colon r\to [ , ]} of a random variable x, the quantile function q returns a threshold value x below which random draws from the given c.d.f. would fall p percent of the time. in terms of the distribution function f, the quantile function q returns the value x such that another way to express the quantile function, which extends to more general distribution functions (than only the continuous and strictly monotonic ones) is for a probability   < p <  . here we capture the fact that the quantile function returns the minimum value of x from amongst all those values whose c.d.f value exceeds p, which is equivalent to the previous probability statement in the special case that the distribution is continuous. note that the infimum function can be replaced by the minimum function, since the distribution function is right-continuous and weakly monotonically increasing. the quantile is the unique function satisfying the galois inequalities if the function f is continuous and strictly monotonically increasing, then the inequalities can be replaced by equalities, and we have: in general, even though the distribution function f may fail to possess a left or right inverse, the quantile function q behaves as an ""almost sure left inverse"" for the distribution function, in the sense that for example, the cumulative distribution function of exponential(λ) (i.e. intensity λ and expected value (mean)  /λ) is the quantile function for exponential(λ) is derived by finding the value of q for which   − e − λ q = p {\displaystyle  -e^{-\lambda q}=p} : "
367,367,Quantum algorithm for linear systems of equations,0,https://en.wikipedia.org/wiki/Quantum_algorithm_for_linear_systems_of_equations," the quantum algorithm for linear systems of equations, also called hhl algorithm, designed by aram harrow, avinatan hassidim, and seth lloyd, is a quantum algorithm formulated in      for solving linear systems. the algorithm estimates the result of a scalar measurement on the solution vector to a given linear system of equations. the algorithm is one of the main fundamental algorithms expected to provide a speedup over their classical counterparts, along with shor's factoring algorithm, grover's search algorithm, the quantum fourier transform and quantum simulation. provided the linear system is sparse and has a low condition number κ {\displaystyle \kappa } , and that the user is interested in the result of a scalar measurement on the solution vector, instead of the values of the solution vector itself, then the algorithm has a runtime of o ( log ⁡ ( n ) κ   ) {\displaystyle o(\log(n)\kappa ^{ })} , where n {\displaystyle n} is the number of variables in the linear system. this offers an exponential speedup over the fastest classical algorithm, which runs in o ( n κ ) {\displaystyle o(n\kappa )} (or o ( n κ ) {\displaystyle o(n{\sqrt {\kappa }})} for positive semidefinite matrices). an implementation of the quantum algorithm for linear systems of equations was first demonstrated in      by cai et al., barz et al. and pan et al. in parallel. the demonstrations consisted of simple linear equations on specially designed quantum devices. the first demonstration of a general-purpose version of the algorithm appeared in      in the work of zhao et al. due to the prevalence of linear systems in virtually all areas of science and engineering, the quantum algorithm for linear systems of equations has the potential for widespread applicability. the problem we are trying to solve is: given a n × n {\displaystyle n\times n} hermitian matrix a {\displaystyle a} and a unit vector b → {\displaystyle {\overrightarrow {b}}} , find the solution vector x → {\displaystyle {\overrightarrow {x}}} satisfying a x → = b → {\displaystyle a{\overrightarrow {x}}={\overrightarrow {b}}} . this algorithm assumes that the user is not interested in the values of x → {\displaystyle {\overrightarrow {x}}} itself, but rather the result of applying some operator m {\displaystyle m} onto x, ⟨ x | m | x ⟩ {\displaystyle \langle x|m|x\rangle } . first, the algorithm represents the vector b → {\displaystyle {\overrightarrow {b}}} as a quantum state of the form: next, hamiltonian simulation techniques are used to apply the unitary operator e i a t {\displaystyle e^{iat}} to | b ⟩ {\displaystyle |b\rangle } for a superposition of different times t {\displaystyle t} . the ability to decompose | b ⟩ {\displaystyle |b\rangle } into the eigenbasis of a {\displaystyle a} and to find the corresponding eigenvalues λ j {\displaystyle \lambda _{j}} is facilitated by the use of quantum phase estimation. the state of the system after this decomposition is approximately: where u j {\displaystyle u_{j}} is the eigenvector basis of a {\displaystyle a} , and | b ⟩ = ∑ j = ⁡   n β j | u j ⟩ {\displaystyle |b\rangle =\sum _{j\mathop {=}  }^{n}\beta _{j}|u_{j}\rangle } . we would then like to perform the linear map taking | λ j ⟩ {\displaystyle |\lambda _{j}\rangle } to c λ j −   | λ j ⟩ {\displaystyle c\lambda _{j}^{- }|\lambda _{j}\rangle } , where c {\displaystyle c} is a normalizing constant. the linear mapping operation is not unitary and thus will require a number of repetitions as it has some probability of failing. after it succeeds, we uncompute the | λ j ⟩ {\displaystyle |\lambda _{j}\rangle } register and are left with a state proportional to: "
368,368,Quartiles,2,https://en.wikipedia.org/wiki/Quartile," in statistics, a quartile is a type of quantile which divides the number of data points into four parts, or quarters, of more-or-less equal size. the data must be ordered from smallest to largest to compute quartiles; as such, quartiles are a form of order statistic. the three main quartiles are as follows: along with the minimum and maximum of the data (which are also quartiles), the three quartiles described above provide a five-number summary of the data. this summary is important in statistics because it provides information about both the center and the spread of the data. knowing the lower and upper quartile provides information on how big the spread is and if the dataset is skewed toward one side. since quartiles divide the number of data points evenly, the range is not the same between quartiles (i.e., q -q  ≠ q -q ) and is instead known as the interquartile range (iqr). while the maximum and minimum also show the spread of the data, the upper and lower quartiles can provide more detailed information on the location of specific data points, the presence of outliers in the data, and the difference in spread between the middle   % of the data and the outer data points. for discrete distributions, there is no universal agreement on selecting the quartile values. this rule is employed by the ti-   calculator boxplot and "" -var stats"" functions. the values found by this method are also known as ""tukey's hinges""; see also midhinge. if we have an ordered dataset x   , x   , . . . , x n {\displaystyle x_{ },x_{ },...,x_{n}} , we can interpolate between data points to find the p {\displaystyle p} th empirical quantile if x i {\displaystyle x_{i}} is in the i / ( n +   ) {\displaystyle i/(n+ )} quantile. if we denote the integer part of a number a {\displaystyle a} by [ a ] {\displaystyle [a]} , then the empirical quantile function is given by, q ( p ) = x ( k ) + α ( x ( k +   ) − x ( k ) ) {\displaystyle q(p)=x_{(k)}+\alpha (x_{(k+ )}-x_{(k)})} , where k = [ p ( n +   ) ] {\displaystyle k=[p(n+ )]} and α = p ( n +   ) − [ p ( n +   ) ] {\displaystyle \alpha =p(n+ )-[p(n+ )]} . to find the first, second, and third quartiles of the dataset we would evaluate q (  .   ) {\displaystyle q( .  )} , q (  .  ) {\displaystyle q( . )} , and q (  .   ) {\displaystyle q( .  )} respectively. "
369,369,Quotient rule,1,https://en.wikipedia.org/wiki/Quotient_rule,"in calculus, the quotient rule is a method of finding the derivative of a function that is the ratio of two differentiable functions. let f ( x ) = g ( x ) / h ( x ) , {\displaystyle f(x)=g(x)/h(x),} where both g and h are differentiable and h ( x ) ≠  . {\displaystyle h(x)\neq  .} the quotient rule states that the derivative of f(x) is let f ( x ) = g ( x ) h ( x ) . {\displaystyle f(x)={\frac {g(x)}{h(x)}}.} applying the definition of the derivative and properties of limits gives the following proof. let f ( x ) = g ( x ) h ( x ) , {\displaystyle f(x)={\frac {g(x)}{h(x)}},} so g ( x ) = f ( x ) h ( x ) . {\displaystyle g(x)=f(x)h(x).} the product rule then gives g ′ ( x ) = f ′ ( x ) h ( x ) + f ( x ) h ′ ( x ) . {\displaystyle g'(x)=f'(x)h(x)+f(x)h'(x).} solving for f ′ ( x ) {\displaystyle f'(x)} and substituting back for f ( x ) {\displaystyle f(x)} gives: let f ( x ) = g ( x ) h ( x ) = g ( x ) h ( x ) −   . {\displaystyle f(x)={\frac {g(x)}{h(x)}}=g(x)h(x)^{- }.} then the product rule gives to evaluate the derivative in the second term, apply the power rule along with the chain rule: finally, rewrite as fractions and combine terms to get implicit differentiation can be used to compute the nth derivative of a quotient (partially in terms of its first n −   derivatives). for example, differentiating f h = g {\displaystyle fh=g} twice (resulting in f ″ h +   f ′ h ′ + f h ″ = g ″ {\displaystyle f''h+ f'h'+fh''=g''} ) and then solving for f ″ {\displaystyle f''} yields "
370,370,Ramanujan's sum,1,https://en.wikipedia.org/wiki/Ramanujan%27s_sum,"in number theory, ramanujan's sum, usually denoted cq(n), is a function of two positive integer variables q and n defined by the formula where (a, q) =   means that a only takes on values coprime to q. srinivasa ramanujan mentioned the sums in a      paper. in addition to the expansions discussed in this article, ramanujan's sums are used in the proof of vinogradov's theorem that every sufficiently large odd number is the sum of three primes. for integers a and b, a ∣ b {\displaystyle a\mid b} is read ""a divides b"" and means that there is an integer c such that b a = c . {\displaystyle {\frac {b}{a}}=c.} similarly, a ∤ b {\displaystyle a\nmid b} is read ""a does not divide b"". the summation symbol means that d goes through all the positive divisors of m, e.g. ( a , b ) {\displaystyle (a,\,b)} is the greatest common divisor, ϕ ( n ) {\displaystyle \phi (n)} is euler's totient function, μ ( n ) {\displaystyle \mu (n)} is the möbius function, and ζ ( s ) {\displaystyle \zeta (s)} is the riemann zeta function. "
371,371,Random Variable,2,https://en.wikipedia.org/wiki/Random_variable,"a random variable (also called random quantity, aleatory variable, or stochastic variable) is a mathematical formalization of a quantity or object which depends on random events. informally, randomness typically represents some fundamental element of chance, such as in the roll of a dice; it may also represent uncertainty, such as measurement error. however, the interpretation of probability is philosophically complicated, and even in specific cases is not always straightforward. the purely mathematical analysis of random variables is independent of such interpretational difficulties, and can be based upon a rigorous axiomatic setup. in the formal mathematical language of measure theory, a random variable is defined as a measurable function from a probability measure space (called the sample space) to a measurable space. this allows consideration of the pushforward measure, which is called the distribution of the random variable; the distribution is thus a probability measure on the set of all possible values of the random variable. it is possible for two random variables to have identical distributions but to differ in significant ways; for instance, they may be independent. it is common to consider the special cases of discrete random variables and absolutely continuous random variables, corresponding to whether a random variable is valued in a discrete set (such as a finite set) or in an interval of real numbers. there are other important possibilities, especially in the theory of stochastic processes, wherein it is natural to consider random sequences or random functions. sometimes a random variable is taken to be automatically valued in the real numbers, with more general random quantities instead being called random elements. according to george mackey, pafnuty chebyshev was the first person ""to think systematically in terms of random variables"". a random variable x {\displaystyle x} is a measurable function x : ω → e {\displaystyle x\colon \omega \to e} from a set of possible outcomes ω {\displaystyle \omega } to a measurable space e {\displaystyle e} . the technical axiomatic definition requires ω {\displaystyle \omega } to be a sample space of a probability triple ( ω , f , p ) {\displaystyle (\omega ,{\mathcal {f}},\operatorname {p} )} (see the measure-theoretic definition). a random variable is often denoted by capital roman letters such as x {\displaystyle x} , y {\displaystyle y} , z {\displaystyle z} , t {\displaystyle t} . the probability that x {\displaystyle x} takes on a value in a measurable set s ⊆ e {\displaystyle s\subseteq e} is written as in many cases, x {\displaystyle x} is real-valued, i.e. e = r {\displaystyle e=\mathbb {r} } . in some contexts, the term random element (see extensions) is used to denote a random variable not of this form. when the image (or range) of x {\displaystyle x} is countable, the random variable is called a discrete random variable :     and its distribution is a discrete probability distribution, i.e. can be described by a probability mass function that assigns a probability to each value in the image of x {\displaystyle x} . if the image is uncountably infinite (usually an interval) then x {\displaystyle x} is called a continuous random variable. in the special case that it is absolutely continuous, its distribution can be described by a probability density function, which assigns probabilities to intervals; in particular, each individual point must necessarily have probability zero for an absolutely continuous random variable. not all continuous random variables are absolutely continuous, a mixture distribution is one such counterexample; such random variables cannot be described by a probability density or a probability mass function. any random variable can be described by its cumulative distribution function, which describes the probability that the random variable will be less than or equal to a certain value. "
372,372,Random Walk,0,https://en.wikipedia.org/wiki/Random_walk," in mathematics, a random walk is a random process that describes a path that consists of a succession of random steps on some mathematical space. an elementary example of a random walk is the random walk on the integer number line z {\displaystyle \mathbb {z} } which starts at  , and at each step moves +  or −  with equal probability. other examples include the path traced by a molecule as it travels in a liquid or a gas (see brownian motion), the search path of a foraging animal, or the price of a fluctuating stock and the financial status of a gambler. random walks have applications to engineering and many scientific fields including ecology, psychology, computer science, physics, chemistry, biology, economics, and sociology. the term random walk was first introduced by karl pearson in     . a popular random walk model is that of a random walk on a regular lattice, where at each step the location jumps to another site according to some probability distribution. in a simple random walk, the location can only jump to neighboring sites of the lattice, forming a lattice path. in a simple symmetric random walk on a locally finite lattice, the probabilities of the location jumping to each one of its immediate neighbors are the same. the best-studied example is of random walk on the d-dimensional integer lattice (sometimes called the hypercubic lattice) z d {\displaystyle \mathbb {z} ^{d}} . if the state space is limited to finite dimensions, the random walk model is called a simple bordered symmetric random walk, and the transition probabilities depend on the location of the state because on margin and corner states the movement is limited. an elementary example of a random walk is the random walk on the integer number line, z {\displaystyle \mathbb {z} } , which starts at   and at each step moves +  or −  with equal probability. this walk can be illustrated as follows. a marker is placed at zero on the number line, and a fair coin is flipped. if it lands on heads, the marker is moved one unit to the right. if it lands on tails, the marker is moved one unit to the left. after five flips, the marker could now be on - , - , - ,  ,  ,  . with five flips, three heads and two tails, in any order, it will land on  . there are    ways of landing on   (by flipping three heads and two tails),    ways of landing on −  (by flipping three tails and two heads),   ways of landing on   (by flipping four heads and one tail),   ways of landing on −  (by flipping four tails and one head),   way of landing on   (by flipping five heads), and   way of landing on −  (by flipping five tails). see the figure below for an illustration of the possible outcomes of   flips. to define this walk formally, take independent random variables z   , z   , … {\displaystyle z_{ },z_{ },\dots } , where each variable is either   or − , with a   % probability for either value, and set s   =   {\displaystyle s_{ }= } and s n = ∑ j =   n z j . {\textstyle s_{n}=\sum _{j= }^{n}z_{j}.} the series { s n } {\displaystyle \{s_{n}\}} is called the simple random walk on z {\displaystyle \mathbb {z} } . this series (the sum of the sequence of − s and  s) gives the net distance walked, if each part of the walk is of length one. the expectation e ( s n ) {\displaystyle e(s_{n})} of s n {\displaystyle s_{n}} is zero. that is, the mean of all coin flips approaches zero as the number of flips increases. this follows by the finite additivity property of expectation: a similar calculation, using the independence of the random variables and the fact that e ( z n   ) =   {\displaystyle e(z_{n}^{ })= } , shows that: this hints that e ( | s n | ) {\displaystyle e(|s_{n}|)\,\!} , the expected translation distance after n steps, should be of the order of n {\displaystyle {\sqrt {n}}} . in fact, "
373,373,Range of a Function,1,https://en.wikipedia.org/wiki/Range_of_a_function,"in mathematics, the range of a function may refer to either of two closely related concepts: given two sets x and y, a binary relation f between x and y is a (total) function (from x to y) if for every x in x there is exactly one y in y such that f relates x to y. the sets x and y are called domain and codomain of f, respectively. the image of f is then the subset of y consisting of only those elements y of y such that there is at least one x in x with f(x) = y. as the term ""range"" can have different meanings, it is considered a good practice to define it the first time it is used in a textbook or article. older books, when they use the word ""range"", tend to use it to mean what is now called the codomain. more modern books, if they use the word ""range"" at all, generally use it to mean what is now called the image. to avoid any confusion, a number of modern books don't use the word ""range"" at all. given a function with domain x {\displaystyle x} , the range of f {\displaystyle f} , sometimes denoted ran ⁡ ( f ) {\displaystyle \operatorname {ran} (f)} or range ⁡ ( f ) {\displaystyle \operatorname {range} (f)} , may refer to the codomain or target set y {\displaystyle y} (i.e., the set into which all of the output of f {\displaystyle f} is constrained to fall), or to f ( x ) {\displaystyle f(x)} , the image of the domain of f {\displaystyle f} under f {\displaystyle f} (i.e., the subset of y {\displaystyle y} consisting of all actual outputs of f {\displaystyle f} ). the image of a function is always a subset of the codomain of the function. as an example of the two different usages, consider the function f ( x ) = x   {\displaystyle f(x)=x^{ }} as it is used in real analysis (that is, as a function that inputs a real number and outputs its square). in this case, its codomain is the set of real numbers r {\displaystyle \mathbb {r} } , but its image is the set of non-negative real numbers r + {\displaystyle \mathbb {r} ^{+}} , since x   {\displaystyle x^{ }} is never negative if x {\displaystyle x} is real. for this function, if we use ""range"" to mean codomain, it refers to r {\displaystyle \mathbb {\displaystyle \mathbb {r} ^{}} } ; if we use ""range"" to mean image, it refers to r + {\displaystyle \mathbb {r} ^{+}} . in many cases, the image and the codomain can coincide. for example, consider the function f ( x ) =   x {\displaystyle f(x)= x} , which inputs a real number and outputs its double. for this function, the codomain and the image are the same (both being the set of real numbers), so the word range is unambiguous. "
374,374,Rank of a Matrix,0,https://en.wikipedia.org/wiki/Rank_(linear_algebra),"in linear algebra, the rank of a matrix a is the dimension of the vector space generated (or spanned) by its columns. this corresponds to the maximal number of linearly independent columns of a. this, in turn, is identical to the dimension of the vector space spanned by its rows. rank is thus a measure of the ""nondegenerateness"" of the system of linear equations and linear transformation encoded by a. there are multiple equivalent definitions of rank. a matrix's rank is one of its most fundamental characteristics. the rank is commonly denoted by rank(a) or rk(a); sometimes the parentheses are not written, as in rank a.[i] in this section, we give some definitions of the rank of a matrix. many definitions are possible; see alternative definitions for several of these. the column rank of a is the dimension of the column space of a, while the row rank of a is the dimension of the row space of a. a fundamental result in linear algebra is that the column rank and the row rank are always equal. (two proofs of this result are given in § proofs that column rank = row rank, below.) this number (i.e., the number of linearly independent rows or columns) is simply called the rank of a. a matrix is said to have full rank if its rank equals the largest possible for a matrix of the same dimensions, which is the lesser of the number of rows and columns. a matrix is said to be rank-deficient if it does not have full rank. the rank deficiency of a matrix is the difference between the lesser of the number of rows and columns, and the rank. the rank of a linear map or operator φ {\displaystyle \phi } is defined as the dimension of its image: the matrix the matrix a common approach to finding the rank of a matrix is to reduce it to a simpler form, generally row echelon form, by elementary row operations. row operations do not change the row space (hence do not change the row rank), and, being invertible, map the column space to an isomorphic space (hence do not change the column rank). once in row echelon form, the rank is clearly the same for both row rank and column rank, and equals the number of pivots (or basic columns) and also the number of non-zero rows. "
375,375,Rational Number,1,https://en.wikipedia.org/wiki/Rational_number,"in mathematics, a rational number is a number that can be expressed as the quotient or fraction p/q of two integers, a numerator p and a non-zero denominator q. for example, − /  is a rational number, as is every integer (e.g.   =  / ). the set of all rational numbers, also referred to as ""the rationals"", the field of rationals or the field of rational numbers is usually denoted by a boldface q (or blackboard bold q {\displaystyle \mathbb {q} } , unicode u+ d    𝐐 mathematical bold capital q or u+   a ℚ double-struck capital q); it was thus denoted in      by giuseppe peano after quoziente, italian for ""quotient"",[citation needed] and first appeared in bourbaki's algèbre. the decimal expansion of a rational number either terminates after a finite number of digits (example:  /  =  .  ), or eventually begins to repeat the same finite sequence of digits over and over (example:  /   =  .        ...). conversely, any repeating or terminating decimal represents a rational number. these statements are true in base   , and in every other integer base (for example, binary or hexadecimal).[citation needed] a real number that is not rational is called irrational. irrational numbers include √ , π, e, and φ. the decimal expansion of an irrational number continues without repeating. since the set of rational numbers is countable, and the set of real numbers is uncountable, almost all real numbers are irrational. rational numbers can be formally defined as equivalence classes of pairs of integers (p, q) with q ≠  , using the equivalence relation defined as follows: the fraction p/q then denotes the equivalence class of (p, q). rational numbers together with addition and multiplication form a field which contains the integers, and is contained in any field containing the integers. in other words, the field of rational numbers is a prime field, and a field has characteristic zero if and only if it contains the rational numbers as a subfield. finite extensions of q are called algebraic number fields, and the algebraic closure of q is the field of algebraic numbers. in mathematical analysis, the rational numbers form a dense subset of the real numbers. the real numbers can be constructed from the rational numbers by completion, using cauchy sequences, dedekind cuts, or infinite decimals (for more, see construction of the real numbers).[citation needed] the term rational in reference to the set q refers to the fact that a rational number represents a ratio of two integers. in mathematics, ""rational"" is often used as a noun abbreviating ""rational number"". the adjective rational sometimes means that the coefficients are rational numbers. for example, a rational point is a point with rational coordinates (i.e., a point whose coordinates are rational numbers); a rational matrix is a matrix of rational numbers; a rational polynomial may be a polynomial with rational coefficients, although the term ""polynomial over the rationals"" is generally preferred, to avoid confusion between ""rational expression"" and ""rational function"" (a polynomial is a rational expression and defines a rational function, even if its coefficients are not rational numbers). however, a rational curve is not a curve defined over the rationals, but a curve which can be parameterized by rational functions.[citation needed] although nowadays rational numbers are defined in terms of ratios, the term rational is not a derivation of ratio. on the opposite, it is ratio that is derived from rational: the first use of ratio with its modern meaning was attested in english about     , while the use of rational for qualifying numbers appeared almost a century earlier, in     . this meaning of rational came from the mathematical meaning of irrational, which was first used in     , and it was used in ""translations of euclid (following his peculiar use of ἄλογος)"". this unusual history originated in the fact that ancient greeks ""avoided heresy by forbidding themselves from thinking of those [irrational] lengths as numbers"". so such lengths were irrational, in the sense of illogical, that is ""not to be spoken about"" (ἄλογος in greek). "
376,376,Real valued functions,1,https://en.wikipedia.org/wiki/Real-valued_function,"in mathematics, a real-valued function is a function whose values are real numbers. in other words, it is a function that assigns a real number to each member of its domain. real-valued functions of a real variable (commonly called real functions) and real-valued functions of several real variables are the main object of study of calculus and, more generally, real analysis. in particular, many function spaces consist of real-valued functions. let f ( x , r ) {\displaystyle {\mathcal {f}}(x,{\mathbb {r} })} be the set of all functions from a set x to real numbers r {\displaystyle \mathbb {r} } . because r {\displaystyle \mathbb {r} } is a field, f ( x , r ) {\displaystyle {\mathcal {f}}(x,{\mathbb {r} })} may be turned into a vector space and a commutative algebra over the reals with the following operations: these operations extend to partial functions from x to r , {\displaystyle \mathbb {r} ,} with the restriction that the partial functions f + g and f g are defined only if the domains of f and g have a nonempty intersection; in this case, their domain is the intersection of the domains of f and g. also, since r {\displaystyle \mathbb {r} } is an ordered set, there is a partial order on f ( x , r ) , {\displaystyle {\mathcal {f}}(x,{\mathbb {r} }),} which makes f ( x , r ) {\displaystyle {\mathcal {f}}(x,{\mathbb {r} })} a partially ordered ring. the σ-algebra of borel sets is an important structure on real numbers. if x has its σ-algebra and a function f is such that the preimage f − (b) of any borel set b belongs to that σ-algebra, then f is said to be measurable. measurable functions also form a vector space and an algebra as explained above in § algebraic structure. moreover, a set (family) of real-valued functions on x can actually define a σ-algebra on x generated by all preimages of all borel sets (or of intervals only, it is not important). this is the way how σ-algebras arise in (kolmogorov's) probability theory, where real-valued functions on the sample space ω are real-valued random variables. real numbers form a topological space and a complete metric space. continuous real-valued functions (which implies that x is a topological space) are important in theories of topological spaces and of metric spaces. the extreme value theorem states that for any real continuous function on a compact space its global maximum and minimum exist. the concept of metric space itself is defined with a real-valued function of two variables, the metric, which is continuous. the space of continuous functions on a compact hausdorff space has a particular importance. convergent sequences also can be considered as real-valued continuous functions on a special topological space. "
377,377,Real Numbers,1,https://en.wikipedia.org/wiki/Real_number,"in mathematics, a real number is a value of a continuous quantity that can represent a distance along a line (or alternatively, a quantity that can be represented as an infinite decimal expansion). the adjective real in this context was introduced in the   th century by rené descartes, who distinguished between real and imaginary roots of polynomials. the real numbers include all the rational numbers, such as the integer −  and the fraction  / , and all the irrational numbers, such as   {\displaystyle {\sqrt { }}} ( .        ..., the square root of  , an irrational algebraic number). included within the irrationals are the real transcendental numbers, such as π ( .        ...). in addition to measuring distance, real numbers can be used to measure quantities such as time, mass, energy, velocity, and many more. the set of real numbers is denoted using the symbol r or r {\displaystyle \mathbb {r} } and is sometimes called ""the reals"". real numbers can be thought of as points on an infinitely long line called the number line or real line, where the points corresponding to integers are equally spaced. any real number can be determined by a possibly infinite decimal representation, such as that of  .   , where each consecutive digit is measured in units one-tenth the size of the previous one. the real line can be thought of as a part of the complex plane, and the real numbers can be thought of as a part of the complex numbers. these descriptions of the real numbers are not sufficiently rigorous by the modern standards of pure mathematics. the discovery of a suitably rigorous definition of the real numbers—indeed, the realization that a better definition was needed—was one of the most important developments of   th-century mathematics. the current standard axiomatic definition is that real numbers form the unique dedekind-complete ordered field ( r {\displaystyle \mathbb {r} } ; + ; · ; <), up to an isomorphism,[a] whereas popular constructive definitions of real numbers include declaring them as equivalence classes of cauchy sequences (of rational numbers), dedekind cuts, or infinite decimal representations, together with precise interpretations for the arithmetic operations and the order relation. all these definitions satisfy the axiomatic definition and are thus equivalent. the set of all real numbers is uncountable, in the sense that while both the set of all natural numbers and the set of all real numbers are infinite sets, there can be no one-to-one function from the real numbers to the natural numbers. in fact, the cardinality of the set of all real numbers, denoted by c {\displaystyle {\mathfrak {c}}} and called the cardinality of the continuum, is strictly greater than the cardinality of the set of all natural numbers (denoted ℵ   {\displaystyle \aleph _{ }} , 'aleph-naught'). the statement that there is no subset of the reals with cardinality strictly greater than ℵ   {\displaystyle \aleph _{ }} and strictly smaller than c {\displaystyle {\mathfrak {c}}} is known as the continuum hypothesis (ch). it is neither provable nor refutable using the axioms of zermelo–fraenkel set theory including the axiom of choice (zfc)—the standard foundation of modern mathematics. in fact, some models of zfc satisfy ch, while others violate it. simple fractions were used by the egyptians around      bc; the vedic ""shulba sutras"" (""the rules of chords"") in c.     bc include what may be the first ""use"" of irrational numbers. the concept of irrationality was implicitly accepted by early indian mathematicians such as manava (c.    –    bc), who were aware that the square roots of certain numbers, such as   and   , could not be exactly determined. around     bc, the greek mathematicians led by pythagoras realized the need for irrational numbers, in particular the irrationality of the square root of  . the middle ages brought about the acceptance of zero, negative numbers, integers, and fractional numbers, first by indian and chinese mathematicians, and then by arabic mathematicians, who were also the first to treat irrational numbers as algebraic objects (the latter being made possible by the development of algebra). arabic mathematicians merged the concepts of ""number"" and ""magnitude"" into a more general idea of real numbers. the egyptian mathematician abū kāmil shujā ibn aslam (c.    –   ) was the first to accept irrational numbers as solutions to quadratic equations, or as coefficients in an equation (often in the form of square roots, cube roots and fourth roots). in the   th century, simon stevin created the basis for modern decimal notation, and insisted that there is no difference between rational and irrational numbers in this regard. in the   th century, descartes introduced the term ""real"" to describe roots of a polynomial, distinguishing them from ""imaginary"" ones. in the   th and   th centuries, there was much work on irrational and transcendental numbers. johann heinrich lambert (    ) gave the first flawed proof that π cannot be rational; adrien-marie legendre (    ) completed the proof, and showed that π is not the square root of a rational number. paolo ruffini (    ) and niels henrik abel (    ) both constructed proofs of the abel–ruffini theorem: that the general quintic or higher equations cannot be solved by a general formula involving only arithmetical operations and roots. "
378,378,Reciprocal rule,1,https://en.wikipedia.org/wiki/Reciprocal_rule,"in calculus, the reciprocal rule gives the derivative of the reciprocal of a function f in terms of the derivative of f. the reciprocal rule can be used to show that the power rule holds for negative exponents if it has already been established for positive exponents. also, one can readily deduce the quotient rule from the reciprocal rule and the product rule. the reciprocal rule states that if f is differentiable at a point x and f(x) ≠   then g(x) =  /f(x) is also differentiable at x and this proof relies on the premise that f {\displaystyle f} is differentiable at x , {\displaystyle x,} and on the theorem that f {\displaystyle f} is then also necessarily continuous there. applying the definition of the derivative of g {\displaystyle g} at x {\displaystyle x} with f ( x ) ≠   {\displaystyle f(x)\neq  } gives it may be argued that since an application of the product rule says that and this may be algebraically rearranged to say "
379,379,relu function,0,https://en.wikipedia.org/wiki/Rectifier_(neural_networks),"in the context of artificial neural networks, the rectifier or relu (rectified linear unit) activation function is an activation function defined as the positive part of its argument: where x is the input to a neuron. this is also known as a ramp function and is analogous to half-wave rectification in electrical engineering. this activation function started showing up in the context of visual feature extraction in hierarchical neural networks starting in the late     s. it was later argued that it has strong biological motivations and mathematical justifications. in      it was found to enable better training of deeper networks, compared to the widely used activation functions prior to     , e.g., the logistic sigmoid (which is inspired by probability theory; see logistic regression) and its more practical counterpart, the hyperbolic tangent. the rectifier is, as of     [update], the most popular activation function for deep neural networks. rectified linear units find applications in computer vision and speech recognition using deep neural nets and computational neuroscience. rectifying activation functions were used to separate specific excitation and unspecific inhibition in the neural abstraction pyramid, which was trained in a supervised way to learn several computer vision tasks. in     , the use of the rectifier as a non-linearity has been shown to enable training deep supervised neural networks without requiring unsupervised pre-training. rectified linear units, compared to sigmoid function or similar activation functions, allow faster and effective training of deep neural architectures on large and complex datasets. leaky relus allow a small, positive gradient when the unit is not active. parametric relus (prelus) take this idea further by making the coefficient of leakage into a parameter that is learned along with the other neural-network parameters. note that for a ≤  , this is equivalent to and thus has a relation to ""maxout"" networks. gelu is a smooth approximation to the rectifier. it has a non-monotonic “bump” when x <  , and it serves as the default activation for models such as bert. "
380,380,Regerssion analysis,3,https://en.wikipedia.org/wiki/Regression_analysis,"in statistical modeling, regression analysis is a set of statistical processes for estimating the relationships between a dependent variable (often called the 'outcome' or 'response' variable) and one or more independent variables (often called 'predictors', 'covariates', 'explanatory variables' or 'features'). the most common form of regression analysis is linear regression, in which one finds the line (or a more complex linear combination) that most closely fits the data according to a specific mathematical criterion. for example, the method of ordinary least squares computes the unique line (or hyperplane) that minimizes the sum of squared differences between the true data and that line (or hyperplane). for specific mathematical reasons (see linear regression), this allows the researcher to estimate the conditional expectation (or population average value) of the dependent variable when the independent variables take on a given set of values. less common forms of regression use slightly different procedures to estimate alternative location parameters (e.g., quantile regression or necessary condition analysis ) or estimate the conditional expectation across a broader collection of non-linear models (e.g., nonparametric regression). regression analysis is primarily used for two conceptually distinct purposes. first, regression analysis is widely used for prediction and forecasting, where its use has substantial overlap with the field of machine learning. second, in some situations regression analysis can be used to infer causal relationships between the independent and dependent variables. importantly, regressions by themselves only reveal relationships between a dependent variable and a collection of independent variables in a fixed dataset. to use regressions for prediction or to infer causal relationships, respectively, a researcher must carefully justify why existing relationships have predictive power for a new context or why a relationship between two variables has a causal interpretation. the latter is especially important when researchers hope to estimate causal relationships using observational data. the earliest form of regression was the method of least squares, which was published by legendre in     , and by gauss in     . legendre and gauss both applied the method to the problem of determining, from astronomical observations, the orbits of bodies about the sun (mostly comets, but also later the then newly discovered minor planets). gauss published a further development of the theory of least squares in     , including a version of the gauss–markov theorem. the term ""regression"" was coined by francis galton in the   th century to describe a biological phenomenon. the phenomenon was that the heights of descendants of tall ancestors tend to regress down towards a normal average (a phenomenon also known as regression toward the mean). for galton, regression had only this biological meaning, but his work was later extended by udny yule and karl pearson to a more general statistical context. in the work of yule and pearson, the joint distribution of the response and explanatory variables is assumed to be gaussian. this assumption was weakened by r.a. fisher in his works of      and     . fisher assumed that the conditional distribution of the response variable is gaussian, but the joint distribution need not be. in this respect, fisher's assumption is closer to gauss's formulation of     . in the     s and     s, economists used electromechanical desk ""calculators"" to calculate regressions. before     , it sometimes took up to    hours to receive the result from one regression. regression methods continue to be an area of active research. in recent decades, new methods have been developed for robust regression, regression involving correlated responses such as time series and growth curves, regression in which the predictor (independent variable) or response variables are curves, images, graphs, or other complex data objects, regression methods accommodating various types of missing data, nonparametric regression, bayesian methods for regression, regression in which the predictor variables are measured with error, regression with more predictor variables than observations, and causal inference with regression. in practice, researchers first select a model they would like to estimate and then use their chosen method (e.g., ordinary least squares) to estimate the parameters of that model. regression models involve the following components: in various fields of application, different terminologies are used in place of dependent and independent variables. "
381,381,Relation (mathematics),1,https://en.wikipedia.org/wiki/Relation_(mathematics),"in mathematics, a binary relation is a general concept that defines some relation between the elements of two sets. it is a generalization of the more commonly understood idea of a mathematical function, but with fewer restrictions. a binary relation over sets x and y is a set of ordered pairs (x, y) consisting of elements x in x and y in y. it encodes the common concept of relation: an element x is related to an element y, if and only if the pair (x, y) belongs to the set of ordered pairs that defines the binary relation. a binary relation is the most studied special case n =   of an n-ary relation over sets x , ..., xn, which is a subset of the cartesian product x  × ... × xn. a trivial example of a binary relation over set x of all real numbers ( r {\displaystyle \mathbb {r} } ) and set y of all real numbers ( r {\displaystyle \mathbb {r} } ) is the set of all pairs for which elements x = y {\displaystyle x=y} . this is equivalent to the function y = f ( x ) = x {\displaystyle y=f(x)=x} . another example of a binary relation is the ""divides"" relation over the set of prime numbers p {\displaystyle \mathbb {p} } and the set of integers z {\displaystyle \mathbb {z} } , in which each prime p is related to each integer z that is a multiple of p, but not to an integer that is not a multiple of p. in this relation, for instance, the prime number   is related to numbers such as − ,  ,  ,   , but not to   or  , just as the prime number   is related to  ,  , and  , but not to   or   . binary relations are used in many branches of mathematics to model a wide variety of concepts. these include, among others: a function may be defined as a special kind of binary relation. binary relations are also heavily used in computer science, such as in a relational database management system (rdbms). a binary relation over sets x and y is an element of the power set of x × y. since the latter set is ordered by inclusion (⊆), each relation has a place in the lattice of subsets of x × y. a binary relation is either a homogeneous relation or a heterogeneous relation depending on whether x = y or not. since relations are sets, they can be manipulated using set operations, including union, intersection, and complementation, and satisfying the laws of an algebra of sets. beyond that, operations like the converse of a relation and the composition of relations are available, satisfying the laws of a calculus of relations, for which there are textbooks by ernst schröder, clarence lewis, and gunther schmidt. a deeper analysis of relations involves decomposing them into subsets called concepts, and placing them in a complete lattice. in some systems of axiomatic set theory, relations are extended to classes, which are generalizations of sets. this extension is needed for, among other things, modeling the concepts of ""is an element of"" or ""is a subset of"" in set theory, without running into logical inconsistencies such as russell's paradox. the terms correspondence, dyadic relation and two-place relation are synonyms for binary relation, though some authors use the term ""binary relation"" for any subset of a cartesian product x × y without reference to x and y, and reserve the term ""correspondence"" for a binary relation with reference to x and y. given sets x and y, the cartesian product x × y is defined as {(x, y) | x ∈ x and y ∈ y}, and its elements are called ordered pairs. "
382,382,Riemann hypothesis,3,https://en.wikipedia.org/wiki/Riemann_hypothesis,"in mathematics, the riemann hypothesis is a conjecture that the riemann zeta function has its zeros only at the negative even integers and complex numbers with real part  / . many consider it to be the most important unsolved problem in pure mathematics. it is of great interest in number theory because it implies results about the distribution of prime numbers. it was proposed by bernhard riemann (    ), after whom it is named. the riemann hypothesis and some of its generalizations, along with goldbach's conjecture and the twin prime conjecture, make up hilbert's eighth problem in david hilbert's list of    unsolved problems; it is also one of the clay mathematics institute's millennium prize problems, which offers a million dollars to anyone who solves any of them. the name is also used for some closely related analogues, such as the riemann hypothesis for curves over finite fields. the riemann zeta function ζ(s) is a function whose argument s may be any complex number other than  , and whose values are also complex. it has zeros at the negative even integers; that is, ζ(s) =   when s is one of − , − , − , .... these are called its trivial zeros. the zeta function is also zero for other values of s, which are called nontrivial zeros. the riemann hypothesis is concerned with the locations of these nontrivial zeros, and states that: the real part of every nontrivial zero of the riemann zeta function is  / .thus, if the hypothesis is correct, all the nontrivial zeros lie on the critical line consisting of the complex numbers  /  + i t, where t is a real number and i is the imaginary unit. the riemann zeta function is defined for complex s with real part greater than   by the absolutely convergent infinite series leonhard euler already considered this series in the     s for real values of s, in conjunction with his solution to the basel problem. he also proved that it equals the euler product where the infinite product extends over all prime numbers p. the riemann hypothesis discusses zeros outside the region of convergence of this series and euler product. to make sense of the hypothesis, it is necessary to analytically continue the function to obtain a form that is valid for all complex s. because the zeta function is meromorphic, all choices of how to perform this analytic continuation will lead to the same result, by the identity theorem. a first step in this continuation observes that the series for the zeta function and the dirichlet eta function satisfy the relation within the region of convergence for both series. however, the eta function series on the right converges not just when the real part of s is greater than one, but more generally whenever s has positive real part. thus, the zeta function can be redefined as η ( s ) / (   −   /   s ) {\displaystyle \eta (s)/( - / ^{s})} , extending it from re(s) >   to a larger domain: re(s) >  , except for the points where   −   /   s {\displaystyle  - / ^{s}} is zero. these are the points s =   +   π i n / log ⁡   {\displaystyle s= + \pi in/\log  } where n {\displaystyle n} can be any nonzero integer; the zeta function can be extended to these values too by taking limits (see dirichlet eta function § landau's problem with ζ(s) = η(s)/  and solutions), giving a finite value for all values of s with positive real part except for the simple pole at s =  . "
383,383,Riemann Integral,3,https://en.wikipedia.org/wiki/Riemann_integral,"in the branch of mathematics known as real analysis, the riemann integral, created by bernhard riemann, was the first rigorous definition of the integral of a function on an interval. it was presented to the faculty at the university of göttingen in     , but not published in a journal until     . for many functions and practical applications, the riemann integral can be evaluated by the fundamental theorem of calculus or approximated by numerical integration. let f be a non-negative real-valued function on the interval [a, b], and let s be the region of the plane under the graph of the function f and above the interval [a, b]. see the figure on the top right. this region can be expressed in set-builder notation as we are interested in measuring the area of s. once we have measured it, we will denote the area in the usual way by the basic idea of the riemann integral is to use very simple approximations for the area of s. by taking better and better approximations, we can say that ""in the limit"" we get exactly the area of s under the curve. when f(x) can take negative values, the integral equals the signed area between the graph of f and the x-axis: that is, the area above the x-axis minus the area below the x-axis. a partition of an interval [a, b] is a finite sequence of numbers of the form each [xi, xi +  ] is called a sub-interval of the partition. the mesh or norm of a partition is defined to be the length of the longest sub-interval, that is, a tagged partition p(x, t) of an interval [a, b] is a partition together with a finite sequence of numbers t , ..., tn −   subject to the conditions that for each i, ti ∈ [xi, xi +  ]. in other words, it is a partition together with a distinguished point of every sub-interval. the mesh of a tagged partition is the same as that of an ordinary partition. suppose that two partitions p(x, t) and q(y, s) are both partitions of the interval [a, b]. we say that q(y, s) is a refinement of p(x, t) if for each integer i, with i ∈ [ , n], there exists an integer r(i) such that xi = yr(i) and such that ti = sj for some j with j ∈ [r(i), r(i +  )). said more simply, a refinement of a tagged partition breaks up some of the sub-intervals and adds tags to the partition where necessary, thus it ""refines"" the accuracy of the partition. we can turn the set of all tagged partitions into a directed set by saying that one tagged partition is greater than or equal to another if the former is a refinement of the latter. "
384,384,Riemann Sum,2,https://en.wikipedia.org/wiki/Riemann_sum,"in mathematics, a riemann sum is a certain kind of approximation of an integral by a finite sum. it is named after nineteenth century german mathematician bernhard riemann. one very common application is approximating the area of functions or lines on a graph, but also the length of curves and other approximations. the sum is calculated by partitioning the region into shapes (rectangles, trapezoids, parabolas, or cubics) that together form a region that is similar to the region being measured, then calculating the area for each of these shapes, and finally adding all of these small areas together. this approach can be used to find a numerical approximation for a definite integral even if the fundamental theorem of calculus does not make it easy to find a closed-form solution. because the region filled by the small shapes is usually not exactly the same shape as the region being measured, the riemann sum will differ from the area being measured. this error can be reduced by dividing up the region more finely, using smaller and smaller shapes. as the shapes get smaller and smaller, the sum approaches the riemann integral. let f : [ a , b ] → r {\displaystyle f:[a,b]\rightarrow \mathbb {r} } be a function defined on a closed interval [ a , b ] {\displaystyle [a,b]} of the real numbers, r {\displaystyle \mathbb {r} } , and be a partition of i, where a riemann sum s {\displaystyle s} of f over i with partition p is defined as where δ x i = x i − x i −   {\displaystyle \delta x_{i}=x_{i}-x_{i- }} and x i ∗ ∈ [ x i −   , x i ] {\displaystyle x_{i}^{*}\in [x_{i- },x_{i}]} . one might produce different riemann sums depending on which x i ∗ {\displaystyle x_{i}^{*}} 's are chosen. in the end this will not matter, if the function is riemann integrable, when the difference or width of the summands δ x i {\displaystyle \delta x_{i}} approaches zero. specific choices of x i ∗ {\displaystyle x_{i}^{*}} give us different types of riemann sums: all these methods are among the most basic ways to accomplish numerical integration. loosely speaking, a function is riemann integrable if all riemann sums converge as the partition ""gets finer and finer"". while not derived as a riemann sum, the average of the left and right riemann sums is the trapezoidal sum and is one of the simplest of a very general way of approximating integrals using weighted averages. this is followed in complexity by simpson's rule and newton–cotes formulas. "
385,385,right angled triangle,1,https://en.wikipedia.org/wiki/Right_triangle,"a right triangle (american english) or right-angled triangle (british), or more formally an orthogonal triangle , formerly called a rectangled triangle (ancient greek: ὀρθόςγωνία, lit. 'upright angle'), is a triangle in which one angle is a right angle (that is, a   -degree angle) or two sides are perpendicular. the relation between the sides and other angles of the right triangle is the basis for trigonometry. the side opposite to the right angle is called the hypotenuse (side c in the figure). the sides adjacent to the right angle are called legs (or catheti, singular: cathetus). side a may be identified as the side adjacent to angle b and opposed to (or opposite) angle a, while side b is the side adjacent to angle a and opposed to angle b. if the lengths of all three sides of a right triangle are integers, the triangle is said to be a pythagorean triangle and its side lengths are collectively known as a pythagorean triple. as with any triangle, the area is equal to one half the base multiplied by the corresponding height. in a right triangle, if one leg is taken as the base then the other is height, so the area of a right triangle is one half the product of the two legs. as a formula the area t is where a and b are the legs of the triangle. if the incircle is tangent to the hypotenuse ab at point p, then denoting the semi-perimeter (a + b + c) /   as s, we have pa = s − a and pb = s − b, and the area is given by this formula only applies to right triangles. if an altitude is drawn from the vertex with the right angle to the hypotenuse then the triangle is divided into two smaller triangles which are both similar to the original and therefore similar to each other. from this: in equations, where a, b, c, d, e, f are as shown in the diagram. thus "
386,386,Rolle's Theorem,3,https://en.wikipedia.org/wiki/Rolle%27s_theorem,"in calculus, rolle's theorem or rolle's lemma essentially states that any real-valued differentiable function that attains equal values at two distinct points must have at least one stationary point somewhere between them—that is, a point where the first derivative (the slope of the tangent line to the graph of the function) is zero. the theorem is named after michel rolle. if a real-valued function f is continuous on a proper closed interval [a, b], differentiable on the open interval (a, b), and f (a) = f (b), then there exists at least one c in the open interval (a, b) such that this version of rolle's theorem is used to prove the mean value theorem, of which rolle's theorem is indeed a special case. it is also the basis for the proof of taylor's theorem. although the theorem is named after michel rolle, rolle's      proof covered only the case of polynomial functions. his proof did not use the methods of differential calculus, which at that point in his life he considered to be fallacious. the theorem was first proved by cauchy in      as a corollary of a proof of the mean value theorem. the name ""rolle's theorem"" was first used by moritz wilhelm drobisch of germany in      and by giusto bellavitis of italy in     . for a radius r >  , consider the function its graph is the upper semicircle centered at the origin. this function is continuous on the closed interval [−r, r] and differentiable in the open interval (−r, r), but not differentiable at the endpoints −r and r. since f (−r) = f (r), rolle's theorem applies, and indeed, there is a point where the derivative of f is zero. note that the theorem applies even when the function cannot be differentiated at the endpoints because it only requires the function to be differentiable in the open interval. if differentiability fails at an interior point of the interval, the conclusion of rolle's theorem may not hold. consider the absolute value function then f (− ) = f ( ), but there is no c between −  and   for which the f ′(c) is zero. this is because that function, although continuous, is not differentiable at x =  . note that the derivative of f changes its sign at x =  , but without attaining the value  . the theorem cannot be applied to this function because it does not satisfy the condition that the function must be differentiable for every x in the open interval. however, when the differentiability requirement is dropped from rolle's theorem, f will still have a critical number in the open interval (a, b), but it may not yield a horizontal tangent (as in the case of the absolute value represented in the graph). the second example illustrates the following generalization of rolle's theorem: consider a real-valued, continuous function f on a closed interval [a, b] with f (a) = f (b). if for every x in the open interval (a, b) the right-hand limit "
387,387,Roots of unity,2,https://en.wikipedia.org/wiki/Root_of_unity," in mathematics, a root of unity, occasionally called a de moivre number, is any complex number that yields   when raised to some positive integer power n. roots of unity are used in many branches of mathematics, and are especially important in number theory, the theory of group characters, and the discrete fourier transform. roots of unity can be defined in any field. if the characteristic of the field is zero, the roots are complex numbers that are also algebraic integers. for fields with a positive characteristic, the roots belong to a finite field, and, conversely, every nonzero element of a finite field is a root of unity. any algebraically closed field contains exactly n nth roots of unity, except when n is a multiple of the (positive) characteristic of the field. an nth root of unity, where n is a positive integer, is a number z satisfying the equation unless otherwise specified, the roots of unity may be taken to be complex numbers (including the number  , and the number –  if n is even, which are complex with a zero imaginary part), and in this case, the nth roots of unity are however, the defining equation of roots of unity is meaningful over any field (and even over any ring) f, and this allows considering roots of unity in f. whichever is the field f, the roots of unity in f are either complex numbers, if the characteristic of f is  , or, otherwise, belong to a finite field. conversely, every nonzero element in a finite field is a root of unity in that field. see root of unity modulo n and finite field for further details. an nth root of unity is said to be primitive if it is not an mth root of unity for some smaller m, that is if if n is a prime number, then all nth roots of unity, except  , are primitive. in the above formula in terms of exponential and trigonometric functions, the primitive nth roots of unity are those for which k and n are coprime integers. subsequent sections of this article will comply with complex roots of unity. for the case of roots of unity in fields of nonzero characteristic, see finite field § roots of unity. for the case of roots of unity in rings of modular integers, see root of unity modulo n. "
388,388,Row echelon form,2,https://en.wikipedia.org/wiki/Row_echelon_form,"in linear algebra, a matrix is in echelon form if it has the shape resulting from a gaussian elimination. a matrix being in row echelon form means that gaussian elimination has operated on the rows, and column echelon form means that gaussian elimination has operated on the columns. in other words, a matrix is in column echelon form if its transpose is in row echelon form. therefore, only row echelon forms are considered in the remainder of this article. the similar properties of column echelon form are easily deduced by transposing all the matrices. specifically, a matrix is in row echelon form if some texts add the condition that the leading coefficient must be  . these two conditions imply that all entries in a column below a leading coefficient are zeros. the following is an example of a  ×  matrix in row echelon form, which is not in reduced row echelon form (see below): many properties of matrices may be easily deduced from their row echelon form, such as the rank and the kernel. a matrix is in reduced row echelon form (also called row canonical form) if it satisfies the following conditions: the reduced row echelon form of a matrix may be computed by gauss–jordan elimination. unlike the row echelon form, the reduced row echelon form of a matrix is unique and does not depend on the algorithm used to compute it. for a given matrix, despite the row echelon form not being unique, all row echelon forms and the reduced row echelon form have the same number of zero rows and the pivots are located in the same indices. this is an example of a matrix in reduced row echelon form, which shows that the left part of the matrix is not always an identity matrix: for matrices with integer coefficients, the hermite normal form is a row echelon form that may be calculated using euclidean division and without introducing any rational number or denominator. on the other hand, the reduced echelon form of a matrix with integer coefficients generally contains non-integer coefficients. "
389,389,Sample space,1,https://en.wikipedia.org/wiki/Sample_space,"in probability theory, the sample space (also called sample description space or possibility space ) of an experiment or random trial is the set of all possible outcomes or results of that experiment. a sample space is usually denoted using set notation, and the possible ordered outcomes, or sample points, are listed as elements in the set. it is common to refer to a sample space by the labels s, ω, or u (for ""universal set""). the elements of a sample space may be numbers, words, letters, or symbols. they can also be finite, countably infinite, or uncountably infinite. a subset of the sample space is an event, denoted by e {\displaystyle e} . if the outcome of an experiment is included in e {\displaystyle e} , then event e {\displaystyle e} has occurred. for example, if the experiment is tossing a single coin, the sample space is the set { h , t } {\displaystyle \{h,t\}} , where the outcome h {\displaystyle h} means that the coin was heads and the outcome t {\displaystyle t} means that the coin was tails. the possible events are e = { h } {\displaystyle e=\{h\}} and e = { t } {\displaystyle e=\{t\}} . for tossing two coins, the sample space is { h h , h t , t h , t t } {\displaystyle \{hh,ht,th,tt\}} , where the outcome is h h {\displaystyle hh} if both coins are heads, h t {\displaystyle ht} if the first coin is heads and the second is tails, t h {\displaystyle th} if the first coin is tails and the second is heads, and t t {\displaystyle tt} if both coins are tails. for tossing a single six-sided die, where the result of interest is the number of pips facing up, the sample space is {   ,   ,   ,   ,   ,   } {\displaystyle \{ , , , , , \}} . a well-defined, non-empty sample space s {\displaystyle s} is one of three components in a probabilistic model (a probability space). the other two basic elements are: a well-defined set of possible events (an event space), which is typically the power set of s {\displaystyle s} if s {\displaystyle s} is discrete or a σ-algebra on s {\displaystyle s} if it is continuous, and a probability assigned to each event (a probability measure function). a sample space can be represented visually by a rectangle, with the outcomes of the sample space denoted by points within the rectangle. the events may be represented by ovals, where the points enclosed within the oval make up the event. a set ω {\displaystyle \omega } with outcomes s   , s   , … , s n {\displaystyle s_{ },s_{ },\ldots ,s_{n}} (i.e. ω = { s   , s   , … , s n } {\displaystyle \omega =\{s_{ },s_{ },\ldots ,s_{n}\}} ) must meet some conditions in order to be a sample space: for instance, in the trial of tossing a coin, one possible sample space is ω   = { h , t } {\displaystyle \omega _{ }=\{h,t\}} , where h {\displaystyle h} is the outcome where the coin lands heads and t {\displaystyle t} is for tails. another possible sample space could be ω   = { ( h , r ) , ( h , n r ) , ( t , r ) , ( t , n r ) } {\displaystyle \omega _{ }=\{(h,r),(h,nr),(t,r),(t,nr)\}} . here, r {\displaystyle r} denotes a rainy day and n r {\displaystyle nr} is a day where it is not raining. for most experiments, ω   {\displaystyle \omega _{ }} would be a better choice than ω   {\displaystyle \omega _{ }} , as an experimenter likely do not care about how the weather affects the coin toss. for many experiments, there may be more than one plausible sample space available, depending on what result is of interest to the experimenter. for example, when drawing a card from a standard deck of fifty-two playing cards, one possibility for the sample space could be the various ranks (ace through king), while another could be the suits (clubs, diamonds, hearts, or spades). a more complete description of outcomes, however, could specify both the denomination and the suit, and a sample space describing each individual card can be constructed as the cartesian product of the two sample spaces noted above (this space would contain fifty-two equally likely outcomes). still other sample spaces are possible, such as right-side up or upside down, if some cards have been flipped when shuffling. some treatments of probability assume that the various outcomes of an experiment are always defined so as to be equally likely. for any sample space with n {\displaystyle n} equally likely outcomes, each outcome is assigned the probability   n {\displaystyle {\frac { }{n}}} . however, there are experiments that are not easily described by a sample space of equally likely outcomes—for example, if one were to toss a thumb tack many times and observe whether it landed with its point upward or downward, there is no physical symmetry to suggest that the two outcomes should be equally likely. "
390,390,Random Sampling,0,https://en.wikipedia.org/wiki/Sampling_(statistics),"in statistics, quality assurance, and survey methodology, sampling is the selection of a subset (a statistical sample) of individuals from within a statistical population to estimate characteristics of the whole population. statisticians attempt to collect samples that are representative of the population in question. sampling has lower costs and faster data collection than measuring the entire population and can provide insights in cases where it is infeasible to sample an entire population. each observation measures one or more properties (such as weight, location, colour) of independent objects or individuals. in survey sampling, weights can be applied to the data to adjust for the sample design, particularly in stratified sampling. results from probability theory and statistical theory are employed to guide the practice. in business and medical research, sampling is widely used for gathering information about a population. acceptance sampling is used to determine if a production lot of material meets the governing specifications. successful statistical practice is based on focused problem definition. in sampling, this includes defining the ""population"" from which our sample is drawn. a population can be defined as including all people or items with the characteristic one wishes to understand. because there is very rarely enough time or money to gather information from everyone or everything in a population, the goal becomes finding a representative sample (or subset) of that population. sometimes what defines a population is obvious. for example, a manufacturer needs to decide whether a batch of material from production is of high enough quality to be released to the customer, or should be sentenced for scrap or rework due to poor quality. in this case, the batch is the population. although the population of interest often consists of physical objects, sometimes it is necessary to sample over time, space, or some combination of these dimensions. for instance, an investigation of supermarket staffing could examine checkout line length at various times, or a study on endangered penguins might aim to understand their usage of various hunting grounds over time. for the time dimension, the focus may be on periods or discrete occasions. in other cases, the examined 'population' may be even less tangible. for example, joseph jagger studied the behaviour of roulette wheels at a casino in monte carlo, and used this to identify a biased wheel. in this case, the 'population' jagger wanted to investigate was the overall behaviour of the wheel (i.e. the probability distribution of its results over infinitely many trials), while his 'sample' was formed from observed results from that wheel. similar considerations arise when taking repeated measurements of some physical characteristic such as the electrical conductivity of copper. this situation often arises when seeking knowledge about the cause system of which the observed population is an outcome. in such cases, sampling theory may treat the observed population as a sample from a larger 'superpopulation'. for example, a researcher might study the success rate of a new 'quit smoking' program on a test group of     patients, in order to predict the effects of the program if it were made available nationwide. here the superpopulation is ""everybody in the country, given access to this treatment"" – a group which does not yet exist, since the program isn't yet available to all. the population from which the sample is drawn may not be the same as the population about which information is desired. often there is large but not complete overlap between these two groups due to frame issues etc. (see below). sometimes they may be entirely separate – for instance, one might study rats in order to get a better understanding of human health, or one might study records from people born in      in order to make predictions about people born in     . time spent in making the sampled population and population of concern precise is often well spent, because it raises many issues, ambiguities and questions that would otherwise have been overlooked at this stage. in the most straightforward case, such as the sampling of a batch of material from production (acceptance sampling by lots), it would be most desirable to identify and measure every single item in the population and to include any one of them in our sample. however, in the more general case this is not usually possible or practical. there is no way to identify all rats in the set of all rats. where voting is not compulsory, there is no way to identify which people will vote at a forthcoming election (in advance of the election). these imprecise populations are not amenable to sampling in any of the ways below and to which we could apply statistical theory. "
391,391,scalar multiplication,1,https://en.wikipedia.org/wiki/Scalar_(mathematics),"a scalar is an element of a field which is used to define a vector space. a quantity described by multiple scalars, such as having both direction and magnitude, is called a vector. in linear algebra, real numbers or generally elements of a field are called scalars and relate to vectors in an associated vector space through the operation of scalar multiplication (defined in the vector space), in which a vector can be multiplied by a scalar in the defined way to produce another vector. generally speaking, a vector space may be defined by using any field instead of real numbers (such as complex numbers). then scalars of that vector space will be elements of the associated field (such as complex numbers). a scalar product operation – not to be confused with scalar multiplication – may be defined on a vector space, allowing two vectors to be multiplied in the defined way to produce a scalar. a vector space equipped with a scalar product is called an inner product space. the real component of a quaternion is also called its scalar part. the term scalar is also sometimes used informally to mean a vector, matrix, tensor, or other, usually, ""compound"" value that is actually reduced to a single component. thus, for example, the product of a   × n matrix and an n ×   matrix, which is formally a   ×   matrix, is often said to be a scalar. the term scalar matrix is used to denote a matrix of the form ki where k is a scalar and i is the identity matrix. the word scalar derives from the latin word scalaris, an adjectival form of scala (latin for ""ladder""), from which the english word scale also comes. the first recorded usage of the word ""scalar"" in mathematics occurs in françois viète's analytic art (in artem analyticem isagoge) (    ): [page needed] according to a citation in the oxford english dictionary the first recorded usage of the term ""scalar"" in english came with w. r. hamilton in     , referring to the real part of a quaternion: a vector space is defined as a set of vectors (additive abelian group), a set of scalars (field), and a scalar multiplication operation that takes a scalar k and a vector v to another vector kv. for example, in a coordinate space, the scalar multiplication k ( v   , v   , … , v n ) {\displaystyle k(v_{ },v_{ },\dots ,v_{n})} yields ( k v   , k v   , … , k v n ) {\displaystyle (kv_{ },kv_{ },\dots ,kv_{n})} . in a (linear) function space, kf is the function x ↦ k(f(x)). the scalars can be taken from any field, including the rational, algebraic, real, and complex numbers, as well as finite fields. "
392,392,multiplication by a scalar and product of matrice,1,https://en.wikipedia.org/wiki/Scalar_multiplication,"in mathematics, scalar multiplication is one of the basic operations defining a vector space in linear algebra (or more generally, a module in abstract algebra ). in common geometrical contexts, scalar multiplication of a real euclidean vector by a positive real number multiplies the magnitude of the vector—without changing its direction. the term ""scalar"" itself derives from this usage: a scalar is that which scales vectors. scalar multiplication is the multiplication of a vector by a scalar (where the product is a vector), and is to be distinguished from inner product of two vectors (where the product is a scalar). in general, if k is a field and v is a vector space over k, then scalar multiplication is a function from k × v to v. the result of applying this function to k in k and v in v is denoted kv. scalar multiplication obeys the following rules (vector in boldface): here, + is addition either in the field or in the vector space, as appropriate; and   is the additive identity in either. juxtaposition indicates either scalar multiplication or the multiplication operation in the field. scalar multiplication may be viewed as an external binary operation or as an action of the field on the vector space. a geometric interpretation of scalar multiplication is that it stretches, or contracts, vectors by a constant factor. as a result, it produces a vector in the same or opposite direction of the original vector but of a different length. as a special case, v may be taken to be k itself and scalar multiplication may then be taken to be simply the multiplication in the field. when v is kn, scalar multiplication is equivalent to multiplication of each component with the scalar, and may be defined as such. the same idea applies if k is a commutative ring and v is a module over k. k can even be a rig, but then there is no additive inverse. if k is not commutative, the distinct operations left scalar multiplication cv and right scalar multiplication vc may be defined. the left scalar multiplication of a matrix a with a scalar λ gives another matrix of the same size as a. it is denoted by λa, whose entries of λa are defined by explicitly: "
393,393,Schwarz lemma,2,https://en.wikipedia.org/wiki/Schwarz_lemma,"in mathematics, the schwarz lemma, named after hermann amandus schwarz, is a result in complex analysis about holomorphic functions from the open unit disk to itself. the lemma is less celebrated than deeper theorems, such as the riemann mapping theorem, which it helps to prove. it is, however, one of the simplest results capturing the rigidity of holomorphic functions. let d = { z : | z | <   } {\displaystyle \mathbf {d} =\{z:|z|< \}} be the open unit disk in the complex plane c {\displaystyle \mathbb {c} } centered at the origin, and let f : d → c {\displaystyle f:\mathbf {d} \rightarrow \mathbb {c} } be a holomorphic map such that f (   ) =   {\displaystyle f( )= } and | f ( z ) | ≤   {\displaystyle |f(z)|\leq  } on d {\displaystyle \mathbf {d} } . then | f ( z ) | ≤ | z | {\displaystyle |f(z)|\leq |z|} for all z ∈ d {\displaystyle z\in \mathbf {d} } , and | f ′ (   ) | ≤   {\displaystyle |f'( )|\leq  } . moreover, if | f ( z ) | = | z | {\displaystyle |f(z)|=|z|} for some non-zero z {\displaystyle z} or | f ′ (   ) | =   {\displaystyle |f'( )|= } , then f ( z ) = a z {\displaystyle f(z)=az} for some a ∈ c {\displaystyle a\in \mathbb {c} } with | a | =   {\displaystyle |a|= } . the proof is a straightforward application of the maximum modulus principle on the function which is holomorphic on the whole of d {\displaystyle d} , including at the origin (because f {\displaystyle f} is differentiable at the origin and fixes zero). now if d r = { z : | z | ≤ r } {\displaystyle d_{r}=\{z:|z|\leq r\}} denotes the closed disk of radius r {\displaystyle r} centered at the origin, then the maximum modulus principle implies that, for r <   {\displaystyle r< } , given any z ∈ d r {\displaystyle z\in d_{r}} , there exists z r {\displaystyle z_{r}} on the boundary of d r {\displaystyle d_{r}} such that as r →   {\displaystyle r\rightarrow  } we get | g ( z ) | ≤   {\displaystyle |g(z)|\leq  } . moreover, suppose that | f ( z ) | = | z | {\displaystyle |f(z)|=|z|} for some non-zero z ∈ d {\displaystyle z\in d} , or | f ′ (   ) | =   {\displaystyle |f'( )|= } . then, | g ( z ) | =   {\displaystyle |g(z)|= } at some point of d {\displaystyle d} . so by the maximum modulus principle, g ( z ) {\displaystyle g(z)} is equal to a constant a {\displaystyle a} such that | a | =   {\displaystyle |a|= } . therefore, f ( z ) = a z {\displaystyle f(z)=az} , as desired. a variant of the schwarz lemma, known as the schwarz–pick theorem (after georg pick), characterizes the analytic automorphisms of the unit disc, i.e. bijective holomorphic mappings of the unit disc to itself: let f : d → d be holomorphic. then, for all z , z  ∈ d, "
394,394,Secant,1,https://en.wikipedia.org/wiki/Secant_line,"in geometry, a secant is a line that intersects a curve at a minimum of two distinct points. the word secant comes from the latin word secare, meaning to cut. in the case of a circle, a secant intersects the circle at exactly two points. a chord is the line segment determined by the two points, that is, the interval on the secant whose ends are the two points. a straight line can intersect a circle at zero, one, or two points. a line with intersections at two points is called a secant line, at one point a tangent line and at no points an exterior line. a chord is the line segment that joins two distinct points of a circle. a chord is therefore contained in a unique secant line and each secant line determines a unique chord. in rigorous modern treatments of plane geometry, results that seem obvious and were assumed (without statement) by euclid in his treatment, are usually proved. for example, theorem (elementary circular continuity): if c {\displaystyle {\mathcal {c}}} is a circle and ℓ {\displaystyle \ell } a line that contains a point a that is inside c {\displaystyle {\mathcal {c}}} and a point b that is outside of c {\displaystyle {\mathcal {c}}} then ℓ {\displaystyle \ell } is a secant line for c {\displaystyle {\mathcal {c}}} . in some situations phrasing results in terms of secant lines instead of chords can help to unify statements. as an example of this consider the result: if the point p lies inside the circle this is euclid iii.  , but if the point is outside the circle the result is not contained in the elements. however, robert simson following christopher clavius demonstrated this result, sometimes called the secant-secant theorem, in their commentaries on euclid. for curves more complicated than simple circles, the possibility that a line that intersects a curve in more than two distinct points arises. some authors define a secant line to a curve as a line that intersects the curve in two distinct points. this definition leaves open the possibility that the line may have other points of intersection with the curve. when phrased this way the definitions of a secant line for circles and curves are identical and the possibility of additional points of intersection just does not occur for a circle. secants may be used to approximate the tangent line to a curve, at some point p, if it exists. define a secant to a curve by two points, p and q, with p fixed and q variable. as q approaches p along the curve, if the slope of the secant approaches a limit value, then that limit defines the slope of the tangent line at p. the secant lines pq are the approximations to the tangent line. in calculus, this idea is the geometric definition of the derivative. a tangent line to a curve at a point p may be a secant line to that curve if it intersects the curve in at least one point other than p. another way to look at this is to realize that being a tangent line at a point p is a local property, depending only on the curve in the immediate neighborhood of p, while being a secant line is a global property since the entire domain of the function producing the curve needs to be examined. the concept of a secant line can be applied in a more general setting than euclidean space. let k be a finite set of k points in some geometric setting. a line will be called an n-secant of k if it contains exactly n points of k. for example, if k is a set of    points arranged on a circle in the euclidean plane, a line joining two of them would be a  -secant (or bisecant) and a line passing through only one of them would be a  -secant (or unisecant). a unisecant in this example need not be a tangent line to the circle. "
395,395,Section Formula,1,https://en.wikipedia.org/wiki/Section_formula,"in coordinate geometry, section formula is used to find the ratio in which a line segment is divided by a point internally or externally. it is used to find out the centroid, incenter and excenters of a triangle. in physics, it is used to find the center of mass of systems, equilibrium points, etc. if a point p (lying on ab) divides ab in the ratio m:n then p = ( m x   + n x   m + n , m y   + n y   m + n ) {\displaystyle p=\left({\dfrac {mx_{ }+nx_{ }}{m+n}},{\dfrac {my_{ }+ny_{ }}{m+n}}\right)} the ratio m:n can also be written as m / n :   {\displaystyle m/n: } , or k :   {\displaystyle k: } , where k = m / n {\displaystyle k=m/n} . so, the coordinates of point p {\displaystyle p} dividing the line segment joining the points a ( x   , y   ) {\displaystyle \mathrm {a} (x_{ },y_{ })} and b ( x   , y   ) {\displaystyle \mathrm {b} (x_{ },y_{ })} are: ( m x   + n x   m + n , m y   + n y   m + n ) {\displaystyle \left({\dfrac {mx_{ }+nx_{ }}{m+n}},{\dfrac {my_{ }+ny_{ }}{m+n}}\right)} = ( m n x   + x   m n +   , m n y   + y   m n +   ) {\displaystyle =\left({\frac {{\frac {m}{n}}x_{ }+x_{ }}{{\frac {m}{n}}+ }},{\frac {{\frac {m}{n}}y_{ }+y_{ }}{{\frac {m}{n}}+ }}\right)} = ( k x   + x   k +   , k y   + y   k +   ) {\displaystyle =\left({\frac {kx_{ }+x_{ }}{k+ }},{\frac {ky_{ }+y_{ }}{k+ }}\right)} similarly, the ratio can also be written as k : k −   {\displaystyle k:k- } , and the coordinates of p are ( (   − k ) x   + k x   , (   − k ) y   + k y   ) {\displaystyle (( -k)x_{ }+kx_{ },( -k)y_{ }+ky_{ })} . if a point p {\displaystyle {\displaystyle p}} (lying on a b {\displaystyle {\displaystyle ab}} ) divides a b {\displaystyle {\displaystyle ab}} in the ratio m : n {\displaystyle {\displaystyle m:n}} then p = ( m x   + n x   m + n , m y   + n y   m + n ) {\displaystyle {\displaystyle p=\left({\dfrac {mx_{ }+nx_{ }}{m+n}},{\dfrac {my_{ }+ny_{ }}{m+n}}\right)}} "
396,396,Semi-major and semi-minor axes,2,https://en.wikipedia.org/wiki/Semi-major_and_semi-minor_axes,"in geometry, the major axis of an ellipse is its longest diameter: a line segment that runs through the center and both foci, with ends at the two most widely separated points of the perimeter. the semi-major axis (major semiaxis) is the longest semidiameter or one half of the major axis, and thus runs from the centre, through a focus, and to the perimeter. the semi-minor axis (minor semiaxis) of an ellipse or hyperbola is a line segment that is at right angles with the semi-major axis and has one end at the center of the conic section. for the special case of a circle, the lengths of the semi-axes are both equal to the radius of the circle. the length of the semi-major axis a of an ellipse is related to the semi-minor axis's length b through the eccentricity e and the semi-latus rectum ℓ {\displaystyle \ell } , as follows: the semi-major axis of a hyperbola is, depending on the convention, plus or minus one half of the distance between the two branches. thus it is the distance from the center to either vertex of the hyperbola. a parabola can be obtained as the limit of a sequence of ellipses where one focus is kept fixed as the other is allowed to move arbitrarily far away in one direction, keeping ℓ {\displaystyle \ell } fixed. thus a and b tend to infinity, a faster than b. the major and minor axes are the axes of symmetry for the curve: in an ellipse, the minor axis is the shorter one; in a hyperbola, it is the one that does not intersect the hyperbola. the equation of an ellipse is where (h, k) is the center of the ellipse in cartesian coordinates, in which an arbitrary point is given by (x, y). the semi-major axis is the mean value of the maximum and minimum distances r max {\displaystyle r_{\text{max}}} and r min {\displaystyle r_{\text{min}}} of the ellipse from a focus — that is, of the distances from a focus to the endpoints of the major axis:[citation needed] in astronomy these extreme points are called apsides. the semi-minor axis of an ellipse is the geometric mean of these distances: "
397,397,semicircle,1,https://en.wikipedia.org/wiki/Semicircle,"in mathematics (and more specifically geometry), a semicircle is a one-dimensional locus of points that forms half of a circle. the full arc of a semicircle always measures    ° (equivalently, π radians, or a half-turn). it has only one line of symmetry (reflection symmetry). in non-technical usage, the term ""semicircle"" is sometimes used to refer to a half-disk, which is a two-dimensional geometric shape that also includes the diameter segment from one end of the arc to the other as well as all the interior points. by thales' theorem, any triangle inscribed in a semicircle with a vertex at each of the endpoints of the semicircle and the third vertex elsewhere on the semicircle is a right triangle, with right angle at the third vertex. all lines intersecting the semicircle perpendicularly are concurrent at the center of the circle containing the given semicircle. a semicircle can be used to construct the arithmetic and geometric means of two lengths using straight-edge and compass. for a semicircle with a diameter of a + b, the length of its radius is the arithmetic mean of a and b (since the radius is half of the diameter). the geometric mean can be found by dividing the diameter into two segments of lengths a and b, and then connecting their common endpoint to the semicircle with a segment perpendicular to the diameter. the length of the resulting segment is the geometric mean. this can be proven by applying the pythagorean theorem to three similar right triangles, each having as vertices the point where the perpendicular touches the semicircle and two of the three endpoints of the segments of lengths a and b. the construction of the geometric mean can be used to transform any rectangle into a square of the same area, a problem called the quadrature of a rectangle. the side length of the square is the geometric mean of the side lengths of the rectangle. more generally it is used as a lemma in a general method for transforming any polygonal shape into a similar copy of itself with the area of any other given polygonal shape. the equation of a semicircle with midpoint ( x   , y   ) {\displaystyle (x_{ },y_{ })} on the diameter between its endpoints and which is entirely concave from below is if it is entirely concave from above, the equation is an arbelos is a region in the plane bounded by three semicircles connected at the corners, all on the same side of a straight line (the baseline) that contains their diameters. "
398,398,Separation of variables,2,https://en.wikipedia.org/wiki/Separation_of_variables,"in mathematics, separation of variables (also known as the fourier method) is any of several methods for solving ordinary and partial differential equations, in which algebra allows one to rewrite an equation so that each of two variables occurs on a different side of the equation. suppose a differential equation can be written in the form which we can write more simply by letting y = f ( x ) {\displaystyle y=f(x)} : as long as h(y) ≠  , we can rearrange terms to obtain: so that the two variables x and y have been separated. dx (and dy) can be viewed, at a simple level, as just a convenient notation, which provides a handy mnemonic aid for assisting with manipulations. a formal definition of dx as a differential (infinitesimal) is somewhat advanced. those who dislike leibniz's notation may prefer to write this as but that fails to make it quite as obvious why this is called ""separation of variables"". integrating both sides of the equation with respect to x {\displaystyle x} , we have "
399,399,Sequences,2,https://en.wikipedia.org/wiki/Sequence,"in mathematics, a sequence is an enumerated collection of objects in which repetitions are allowed and order matters. like a set, it contains members (also called elements, or terms). the number of elements (possibly infinite) is called the length of the sequence. unlike a set, the same elements can appear multiple times at different positions in a sequence, and unlike a set, the order does matter. formally, a sequence can be defined as a function from natural numbers (the positions of elements in the sequence) to the elements at each position. the notion of a sequence can be generalized to an indexed family, defined as a function from an index set that may not be numbers to another set of elements. for example, (m, a, r, y) is a sequence of letters with the letter 'm' first and 'y' last. this sequence differs from (a, r, m, y). also, the sequence ( ,  ,  ,  ,  ,  ), which contains the number   at two different positions, is a valid sequence. sequences can be finite, as in these examples, or infinite, such as the sequence of all even positive integers ( ,  ,  , ...). the position of an element in a sequence is its rank or index; it is the natural number for which the element is the image. the first element has index   or  , depending on the context or a specific convention. in mathematical analysis, a sequence is often denoted by letters in the form of a n {\displaystyle a_{n}} , b n {\displaystyle b_{n}} and c n {\displaystyle c_{n}} , where the subscript n refers to the nth element of the sequence; for example, the nth element of the fibonacci sequence f {\displaystyle f} is generally denoted as f n {\displaystyle f_{n}} . in computing and computer science, finite sequences are sometimes called strings, words or lists, the different names commonly corresponding to different ways to represent them in computer memory; infinite sequences are called streams. the empty sequence ( ) is included in most notions of sequence, but may be excluded depending on the context.a sequence can be thought of as a list of elements with a particular order. sequences are useful in a number of mathematical disciplines for studying functions, spaces, and other mathematical structures using the convergence properties of sequences. in particular, sequences are the basis for series, which are important in differential equations and analysis. sequences are also of interest in their own right, and can be studied as patterns or puzzles, such as in the study of prime numbers. there are a number of ways to denote a sequence, some of which are more useful for specific types of sequences. one way to specify a sequence is to list all its elements. for example, the first four odd numbers form the sequence ( ,  ,  ,  ). this notation is used for infinite sequences as well. for instance, the infinite sequence of positive odd integers is written as ( ,  ,  ,  , ...). because notating sequences with ellipsis leads to ambiguity, listing is most useful for customary infinite sequences which can be easily recognized from their first few elements. other ways of denoting a sequence are discussed after the examples. the prime numbers are the natural numbers greater than   that have no divisors but   and themselves. taking these in their natural order gives the sequence ( ,  ,  ,  ,   ,   ,   , ...). the prime numbers are widely used in mathematics, particularly in number theory where many results related to them exist. the fibonacci numbers comprise the integer sequence whose elements are the sum of the previous two elements. the first two elements are either   and   or   and   so that the sequence is ( ,  ,  ,  ,  ,  ,  ,   ,   ,   , ...). other examples of sequences include those made up of rational numbers, real numbers and complex numbers. the sequence (. , .  , .   , .    , ...), for instance, approaches the number  . in fact, every real number can be written as the limit of a sequence of rational numbers (e.g. via its decimal expansion). as another example, π is the limit of the sequence ( ,  . ,  .  ,  .   ,  .    , ...), which is increasing. a related sequence is the sequence of decimal digits of π, that is, ( ,  ,  ,  ,  ,  , ...). unlike the preceding sequence, this sequence does not have any pattern that is easily discernible by inspection. the on-line encyclopedia of integer sequences comprises a large list of examples of integer sequences. "
400,400,Series Mathematics,1,https://en.wikipedia.org/wiki/Series_(mathematics),"in mathematics, a series is, roughly speaking, a description of the operation of adding infinitely many quantities, one after the other, to a given starting quantity. the study of series is a major part of calculus and its generalization, mathematical analysis. series are used in most areas of mathematics, even for studying finite structures (such as in combinatorics) through generating functions. in addition to their ubiquity in mathematics, infinite series are also widely used in other quantitative disciplines such as physics, computer science, statistics and finance. for a long time, the idea that such a potentially infinite summation could produce a finite result was considered paradoxical. this paradox was resolved using the concept of a limit during the   th century. zeno's paradox of achilles and the tortoise illustrates this counterintuitive property of infinite sums: achilles runs after a tortoise, but when he reaches the position of the tortoise at the beginning of the race, the tortoise has reached a second position; when he reaches this second position, the tortoise is at a third position, and so on. zeno concluded that achilles could never reach the tortoise, and thus that movement does not exist. zeno divided the race into infinitely many sub-races, each requiring a finite amount of time, so that the total time for achilles to catch the tortoise is given by a series. the resolution of the paradox is that, although the series has an infinite number of terms, it has a finite sum, which gives the time necessary for achilles to catch up with the tortoise. in modern terminology, any (ordered) infinite sequence ( a   , a   , a   , … ) {\displaystyle (a_{ },a_{ },a_{ },\ldots )} of terms (that is, numbers, functions, or anything that can be added) defines a series, which is the operation of adding the ai one after the other. to emphasize that there are an infinite number of terms, a series may be called an infinite series. such a series is represented (or denoted) by an expression like the infinite sequence of additions implied by a series cannot be effectively carried on (at least in a finite amount of time). however, if the set to which the terms and their finite sums belong has a notion of limit, it is sometimes possible to assign a value to a series, called the sum of the series. this value is the limit as n tends to infinity (if the limit exists) of the finite sums of the n first terms of the series, which are called the nth partial sums of the series. that is, the notation ∑ i =   ∞ a i {\textstyle \sum _{i= }^{\infty }a_{i}} denotes both the series—that is the implicit process of adding the terms one after the other indefinitely—and, if the series is convergent, the sum of the series—the result of the process. this is a generalization of the similar convention of denoting by a + b {\displaystyle a+b} both the addition—the process of adding—and its result—the sum of a and b. generally, the terms of a series come from a ring, often the field r {\displaystyle \mathbb {r} } of the real numbers or the field c {\displaystyle \mathbb {c} } of the complex numbers. in this case, the set of all series is itself a ring (and even an associative algebra), in which the addition consists of adding the series term by term, and the multiplication is the cauchy product. an infinite series or simply a series is an infinite sum, represented by an infinite expression of the form if an abelian group a of terms has a concept of limit (e.g., if it is a metric space), then some series, the convergent series, can be interpreted as having a value in a, called the sum of the series. this includes the common cases from calculus, in which the group is the field of real numbers or the field of complex numbers. given a series s = ∑ n =   ∞ a n {\textstyle s=\sum _{n= }^{\infty }a_{n}} , its kth partial sum is a series σan is said to converge or to be convergent when the sequence (sk) of partial sums has a finite limit. if the limit of sk is infinite or does not exist, the series is said to diverge. when the limit of partial sums exists, it is called the value (or sum) of the series an easy way that an infinite series can converge is if all the an are zero for n sufficiently large. such a series can be identified with a finite sum, so it is only infinite in a trivial sense. "
401,401,Set Builder Notation,1,https://en.wikipedia.org/wiki/Set-builder_notation," { n ∈ z ∣ ( ∃ k ∈ z ) [ n =   k ] } {\displaystyle \{n\in \mathbb {z} \mid (\exists k\in \mathbb {z} )[n= k]\}} the set of all even integers, expressed in set-builder notation. in set theory and its applications to logic, mathematics, and computer science, set-builder notation is a mathematical notation for describing a set by enumerating its elements, or stating the properties that its members must satisfy. defining sets by properties is also known as set comprehension, set abstraction or as defining a set's intension. a set can be described directly by enumerating all of its elements between curly brackets, as in the following two examples: this is sometimes called the ""roster method"" for specifying a set. when it is desired to denote a set that contains elements from a regular sequence, an ellipses notation may be employed, as shown in the next examples: there is no order among the elements of a set (this explains and validates the equality of the last example), but with the ellipses notation, we use an ordered sequence before (or after) the ellipsis as a convenient notational vehicle for explaining which elements are in a set. the first few elements of the sequence are shown, then the ellipses indicate that the simplest interpretation should be applied for continuing the sequence. should no terminating value appear to the right of the ellipses, then the sequence is considered to be unbounded. in general, {   , … , n } {\displaystyle \{ ,\dots ,n\}} denotes the set of all natural numbers i {\displaystyle i} such that   ≤ i ≤ n {\displaystyle  \leq i\leq n} . another notation for {   , … , n } {\displaystyle \{ ,\dots ,n\}} is the bracket notation [ n ] {\displaystyle [n]} . a subtle special case is n =   {\displaystyle n= } , in which [   ] = {   , … ,   } {\displaystyle =\{ ,\dots , \}} is equal to the empty set ∅ {\displaystyle \emptyset } . similarly, { a   , … , a n } {\displaystyle \{a_{ },\dots ,a_{n}\}} denotes the set of all a i {\displaystyle a_{i}} for   ≤ i ≤ n {\displaystyle  \leq i\leq n} . "
402,402,Sets,1,https://en.wikipedia.org/wiki/Set_(mathematics),"a set is the mathematical model for a collection of different things; a set contains elements or members, which can be mathematical objects of any kind: numbers, symbols, points in space, lines, other geometrical shapes, variables, or even other sets. the set with no element is the empty set; a set with a single element is a singleton. a set may have a finite number of elements or be an infinite set. two sets are equal if they have precisely the same elements. sets are ubiquitous in modern mathematics. indeed, set theory, more specifically zermelo–fraenkel set theory, has been the standard way to provide rigorous foundations for all branches of mathematics since the first half of the   th century. the concept of a set emerged in mathematics at the end of the   th century. the german word for set, menge, was coined by bernard bolzano in his work paradoxes of the infinite. georg cantor, one of the founders of set theory, gave the following definition at the beginning of his beiträge zur begründung der transfiniten mengenlehre: a set is a gathering together into a whole of definite, distinct objects of our perception or our thought—which are called elements of the set.bertrand russell called a set a class: when mathematicians deal with what they call a manifold, aggregate, menge, ensemble, or some equivalent name, it is common, especially where the number of terms involved is finite, to regard the object in question (which is in fact a class) as defined by the enumeration of its terms, and as consisting possibly of a single term, which in that case is the class.the foremost property of a set is that it can have elements, also called members. two sets are equal when they have the same elements. more precisely, sets a and b are equal if every element of a is an element of b, and every element of b is an element of a; this property is called the extensionality of sets. the simple concept of a set has proved enormously useful in mathematics, but paradoxes arise if no restrictions are placed on how sets can be constructed: naïve set theory defines a set as any well-defined collection of distinct elements, but problems arise from the vagueness of the term well-defined. "
403,403,set theory,1,https://en.wikipedia.org/wiki/Set_theory,"set theory is the branch of mathematical logic that studies sets, which can be informally described as collections of objects. although objects of any kind can be collected into a set, set theory, as a branch of mathematics, is mostly concerned with those that are relevant to mathematics as a whole. the modern study of set theory was initiated by the german mathematicians richard dedekind and georg cantor in the     s. in particular, georg cantor is commonly considered the founder of set theory. the non-formalized systems investigated during this early stage go under the name of naive set theory. after the discovery of paradoxes within naive set theory (such as russell's paradox, cantor's paradox and burali-forti paradox) various axiomatic systems were proposed in the early twentieth century, of which zermelo–fraenkel set theory (with or without the axiom of choice) is still the best-known and most studied. set theory is commonly employed as a foundational system for the whole of mathematics, particularly in the form of zermelo–fraenkel set theory with the axiom of choice. besides its foundational role, set theory also provides the framework to develop a mathematical theory of infinity, and has various applications in computer science (such as in the theory of relational algebra), philosophy and formal semantics. its foundational appeal, together with its paradoxes, its implications for the concept of infinity and its multiple applications, have made set theory an area of major interest for logicians and philosophers of mathematics. contemporary research into set theory covers a vast array of topics, ranging from the structure of the real number line to the study of the consistency of large cardinals. mathematical topics typically emerge and evolve through interactions among many researchers. set theory, however, was founded by a single paper in      by georg cantor: ""on a property of the collection of all real algebraic numbers"". since the  th century bc, beginning with greek mathematician zeno of elea in the west and early indian mathematicians in the east, mathematicians had struggled with the concept of infinity. especially notable is the work of bernard bolzano in the first half of the   th century. modern understanding of infinity began in     –    , and was motivated by cantor's work in real analysis. an      meeting between cantor and richard dedekind influenced cantor's thinking, and culminated in cantor's      paper. cantor's work initially polarized the mathematicians of his day. while karl weierstrass and dedekind supported cantor, leopold kronecker, now seen as a founder of mathematical constructivism, did not. cantorian set theory eventually became widespread, due to the utility of cantorian concepts, such as one-to-one correspondence among sets, his proof that there are more real numbers than integers, and the ""infinity of infinities"" (""cantor's paradise"") resulting from the power set operation. this utility of set theory led to the article ""mengenlehre"", contributed in      by arthur schoenflies to klein's encyclopedia. the next wave of excitement in set theory came around     , when it was discovered that some interpretations of cantorian set theory gave rise to several contradictions, called antinomies or paradoxes. bertrand russell and ernst zermelo independently found the simplest and best known paradox, now called russell's paradox: consider ""the set of all sets that are not members of themselves"", which leads to a contradiction since it must be a member of itself and not a member of itself. in     , cantor had himself posed the question ""what is the cardinal number of the set of all sets?"", and obtained a related paradox. russell used his paradox as a theme in his      review of continental mathematics in his the principles of mathematics. rather than the term set, russell used the term class, which has subsequently been used more technically. in     , the term set appeared in the book theory of sets of points by husband and wife william henry young and grace chisholm young, published by cambridge university press. the momentum of set theory was such that debate on the paradoxes did not lead to its abandonment. the work of zermelo in      and the work of abraham fraenkel and thoralf skolem in      resulted in the set of axioms zfc, which became the most commonly used set of axioms for set theory. the work of analysts, such as that of henri lebesgue, demonstrated the great mathematical utility of set theory, which has since become woven into the fabric of modern mathematics. set theory is commonly used as a foundational system, although in some areas—such as algebraic geometry and algebraic topology—category theory is thought to be a preferred foundation. set theory begins with a fundamental binary relation between an object o and a set a. if o is a member (or element) of a, the notation o ∈ a is used. a set is described by listing elements separated by commas, or by a characterizing property of its elements, within braces { }. since sets are objects, the membership relation can relate sets as well. "
404,404,Sheppard's correction for moments,3,https://en.wikipedia.org/wiki/Sheppard%27s_correction,"in statistics, sheppard's corrections are approximate corrections to estimates of moments computed from binned data. the concept is named after william fleetwood sheppard. let m k {\displaystyle m_{k}} be the measured kth moment, μ ^ k {\displaystyle {\hat {\mu }}_{k}} the corresponding corrected moment, and c {\displaystyle c} the class interval (bin width). no correction is necessary for the mean (first moment about zero). the first few measured and corrected moments about the mean are then related as follows: when the data come from a normally distributed population, then binning and using the midpoint of the bin as the observed value results in an overestimate of the variance. that is why the correction to the variance is negative. the reason why the uncorrected estimate of the variance is an overestimate is that the error is negatively correlated with the observation. for the uniform distribution, the error is uncorrelated with the observation, so a correction should be +c /  , which is the variance of the error itself rather than −c /  . thus sheppard's correction is biased in favor of population distributions in which the error is negatively correlated with the observation. the cumulants of the sum of the grouped variable and the uniform variable are the sums of the cumulants. as odd cumulants of a uniform distribution are zero; only even moments are affected. the second and fourth cumulants of the uniform distribution on (− . c,  . c) are respectively, c /   and −c /   . the correction to moments can be derived from the relation between cumulants and moments. this statistics-related article is a stub. you can help wikipedia by expanding it."
405,405,Sign function,2,https://en.wikipedia.org/wiki/Sign_function,"in mathematics, the sign function or signum function (from signum, latin for ""sign"") is an odd mathematical function that extracts the sign of a real number. in mathematical expressions the sign function is often represented as sgn. to avoid confusion with the sine function, this function is usually called the signum function. the signum function of a real number x is a piecewise function which is defined as follows: any real number can be expressed as the product of its absolute value and its sign function: it follows that whenever x is not equal to   we have similarly, for any real number x, the signum function is differentiable with derivative   everywhere except at  . it is not differentiable at   in the ordinary sense, but under the generalised notion of differentiation in distribution theory, the derivative of the signum function is two times the dirac delta function, which can be demonstrated using the identity the fourier transform of the signum function is the signum can also be written using the iverson bracket notation: the signum can also be written using the floor and the absolute value functions: the signum function coincides with the limit "
406,406,Singular value decomposition,2,https://en.wikipedia.org/wiki/Singular_value_decomposition," in linear algebra, the singular value decomposition (svd) is a factorization of a real or complex matrix. it generalizes the eigendecomposition of a square normal matrix with an orthonormal eigenbasis to any m × n {\displaystyle m\times n} matrix. it is related to the polar decomposition. specifically, the singular value decomposition of an m × n {\displaystyle m\times n} complex matrix m is a factorization of the form m = u σ v ∗ {\displaystyle \mathbf {m} =\mathbf {u\sigma v^{*}} } , where u is an m × m {\displaystyle m\times m} complex unitary matrix, σ {\displaystyle \mathbf {\sigma } } is an m × n {\displaystyle m\times n} rectangular diagonal matrix with non-negative real numbers on the diagonal, and v is an n × n {\displaystyle n\times n} complex unitary matrix. if m is real, u and v can also be guaranteed to be real orthogonal matrices. in such contexts, the svd is often denoted u σ v t {\displaystyle \mathbf {u\sigma v^{t}} } . the diagonal entries σ i = σ i i {\displaystyle \sigma _{i}=\sigma _{ii}} of σ {\displaystyle \mathbf {\sigma } } are uniquely determined by m and are known as the singular values of m. the number of non-zero singular values is equal to the rank of m. the columns of u and the columns of v are called left-singular vectors and right-singular vectors of m, respectively. they form two sets of orthonormal bases u , ..., um and v , ..., vn , and the singular value decomposition can be written as m = ∑ i =   r σ i u i v i ∗ {\displaystyle \mathbf {m} =\sum _{i= }^{r}\sigma _{i}\mathbf {u} _{i}\mathbf {v} _{i}^{*}} , where r ≤ min { m , n } {\displaystyle r\leq \min\{m,n\}} is the rank of m. the svd is not unique. it is always possible to choose the decomposition so that the singular values σ i i {\displaystyle \sigma _{ii}} are in descending order. in this case, σ {\displaystyle \mathbf {\sigma } } (but not always u and v) is uniquely determined by m. the term sometimes refers to the compact svd, a similar decomposition m = u σ v ∗ {\displaystyle \mathbf {m} =\mathbf {u\sigma v^{*}} } in which σ {\displaystyle \mathbf {\sigma } } is square diagonal of size r × r {\displaystyle r\times r} , where r ≤ min { m , n } {\displaystyle r\leq \min\{m,n\}} is the rank of m, and has only the non-zero singular values. in this variant, u is an m × r {\displaystyle m\times r} semi-unitary matrix and v {\displaystyle \mathbf {v} } is an n × r {\displaystyle n\times r} semi-unitary matrix, such that u ∗ u = v ∗ v = i r {\displaystyle \mathbf {u^{*}u} =\mathbf {v^{*}v} =\mathbf {i} _{r}} . mathematical applications of the svd include computing the pseudoinverse, matrix approximation, and determining the rank, range, and null space of a matrix. the svd is also extremely useful in all areas of science, engineering, and statistics, such as signal processing, least squares fitting of data, and process control. in the special case when m is an m × m real square matrix, the matrices u and v⁎ can be chosen to be real m × m matrices too. in that case, ""unitary"" is the same as ""orthogonal"". then, interpreting both unitary matrices as well as the diagonal matrix, summarized here as a, as a linear transformation x ↦ ax of the space rm, the matrices u and v⁎ represent rotations or reflection of the space, while σ {\displaystyle \mathbf {\sigma } } represents the scaling of each coordinate xi by the factor σi. thus the svd decomposition breaks down any linear transformation of rm into a composition of three geometrical transformations: a rotation or reflection (v⁎), followed by a coordinate-by-coordinate scaling ( σ {\displaystyle \mathbf {\sigma } } ), followed by another rotation or reflection (u). in particular, if m has a positive determinant, then u and v⁎ can be chosen to be both rotations with reflections, or both rotations without reflections. if the determinant is negative, exactly one of them will have a reflection. if the determinant is zero, each can be independently chosen to be of either type. if the matrix m is real but not square, namely m×n with m ≠ n, it can be interpreted as a linear transformation from rn to rm. then u and v⁎ can be chosen to be rotations/reflections of rm and rn, respectively; and σ {\displaystyle \mathbf {\sigma } } , besides scaling the first min { m , n } {\displaystyle \min\{m,n\}} coordinates, also extends the vector with zeros, i.e. removes trailing coordinates, so as to turn rn into rm. "
407,407,Skew-Symmetric matrix,1,https://en.wikipedia.org/wiki/Skew-symmetric_matrix,"in mathematics, particularly in linear algebra, a skew-symmetric (or antisymmetric or antimetric ) matrix is a square matrix whose transpose equals its negative. that is, it satisfies the condition : p.    a skew-symmetric ⟺ a t = − a . {\displaystyle a{\text{ skew-symmetric}}\quad \iff \quad a^{\textsf {t}}=-a.} in terms of the entries of the matrix, if a i j {\textstyle a_{ij}} denotes the entry in the i {\textstyle i} -th row and j {\textstyle j} -th column, then the skew-symmetric condition is equivalent to a skew-symmetric ⟺ a j i = − a i j . {\displaystyle a{\text{ skew-symmetric}}\quad \iff \quad a_{ji}=-a_{ij}.} the matrix is skew-symmetric because throughout, we assume that all matrix entries belong to a field f {\textstyle \mathbb {f} } whose characteristic is not equal to  . that is, we assume that   +   ≠  , where   denotes the multiplicative identity and   the additive identity of the given field. if the characteristic of the field is  , then a skew-symmetric matrix is the same thing as a symmetric matrix. as a result of the first two properties above, the set of all skew-symmetric matrices of a fixed size forms a vector space. the space of n × n {\textstyle n\times n} skew-symmetric matrices has dimension     n ( n −   ) . {\textstyle {\frac { }{ }}n(n- ).} let mat n {\displaystyle {\mbox{mat}}_{n}} denote the space of n × n {\textstyle n\times n} matrices. a skew-symmetric matrix is determined by     n ( n −   ) {\textstyle {\frac { }{ }}n(n- )} scalars (the number of entries above the main diagonal); a symmetric matrix is determined by     n ( n +   ) {\textstyle {\frac { }{ }}n(n+ )} scalars (the number of entries on or above the main diagonal). let skew n {\textstyle {\mbox{skew}}_{n}} denote the space of n × n {\textstyle n\times n} skew-symmetric matrices and sym n {\textstyle {\mbox{sym}}_{n}} denote the space of n × n {\textstyle n\times n} symmetric matrices. if a ∈ mat n {\textstyle a\in {\mbox{mat}}_{n}} then notice that     ( a − a t ) ∈ skew n {\textstyle {\frac { }{ }}\left(a-a^{\textsf {t}}\right)\in {\mbox{skew}}_{n}} and     ( a + a t ) ∈ sym n . {\textstyle {\frac { }{ }}\left(a+a^{\textsf {t}}\right)\in {\mbox{sym}}_{n}.} this is true for every square matrix a {\textstyle a} with entries from any field whose characteristic is different from  . then, since mat n = skew n + sym n {\textstyle {\mbox{mat}}_{n}={\mbox{skew}}_{n}+{\mbox{sym}}_{n}} and skew n ∩ sym n =   , {\textstyle {\mbox{skew}}_{n}\cap {\mbox{sym}}_{n}= ,} "
408,408,Skew lines,2,https://en.wikipedia.org/wiki/Skew_lines,"in three-dimensional geometry, skew lines are two lines that do not intersect and are not parallel. a simple example of a pair of skew lines is the pair of lines through opposite edges of a regular tetrahedron. two lines that both lie in the same plane must either cross each other or be parallel, so skew lines can exist only in three or more dimensions. two lines are skew if and only if they are not coplanar. if four points are chosen at random uniformly within a unit cube, they will almost surely define a pair of skew lines. after the first three points have been chosen, the fourth point will define a non-skew line if, and only if, it is coplanar with the first three points. however, the plane through the first three points forms a subset of measure zero of the cube, and the probability that the fourth point lies on this plane is zero. if it does not, the lines defined by the points will be skew. similarly, in three-dimensional space a very small perturbation of any two parallel or intersecting lines will almost certainly turn them into skew lines. therefore, any four points in general position always form skew lines. in this sense, skew lines are the ""usual"" case, and parallel or intersecting lines are special cases. if each line in a pair of skew lines is defined by two points that it passes through, then these four points must not be coplanar, so they must be the vertices of a tetrahedron of nonzero volume. conversely, any two pairs of points defining a tetrahedron of nonzero volume also define a pair of skew lines. therefore, a test of whether two pairs of points define skew lines is to apply the formula for the volume of a tetrahedron in terms of its four vertices. denoting one point as the  ×  vector a whose three elements are the point's three coordinate values, and likewise denoting b, c, and d for the other points, we can check if the line through a and b is skew to the line through c and d by seeing if the tetrahedron volume formula gives a non-zero result: expressing the two lines as vectors: the cross product of d   {\displaystyle \mathbf {d_{ }} } and d   {\displaystyle \mathbf {d_{ }} } is perpendicular to the lines. the plane formed by the translations of line   along n {\displaystyle \mathbf {n} } contains the point p   {\displaystyle \mathbf {p_{ }} } and is perpendicular to n   = d   × n {\displaystyle \mathbf {n_{ }} =\mathbf {d_{ }} \times \mathbf {n} } . therefore, the intersecting point of line   with the above-mentioned plane, which is also the point on line   that is nearest to line   is given by similarly, the point on line   nearest to line   is given by (where n   = d   × n {\displaystyle \mathbf {n_{ }} =\mathbf {d_{ }} \times \mathbf {n} } ) "
409,409,Slope,1,https://en.wikipedia.org/wiki/Slope,"in mathematics, the slope or gradient of a line is a number that describes both the direction and the steepness of the line. slope is often denoted by the letter m; there is no clear answer to the question why the letter m is used for slope, but its earliest use in english appears in o'brien (    ) who wrote the equation of a straight line as ""y = mx + b"" and it can also be found in todhunter (    ) who wrote it as ""y = mx + c"". slope is calculated by finding the ratio of the ""vertical change"" to the ""horizontal change"" between (any) two distinct points on a line. sometimes the ratio is expressed as a quotient (""rise over run""), giving the same number for every two distinct points on the same line. a line that is decreasing has a negative ""rise"". the line may be practical - as set by a road surveyor, or in a diagram that models a road or a roof either as a description or as a plan. the steepness, incline, or grade of a line is measured by the absolute value of the slope. a slope with a greater absolute value indicates a steeper line. the direction of a line is either increasing, decreasing, horizontal or vertical. the rise of a road between two points is the difference between the altitude of the road at those two points, say y  and y , or in other words, the rise is (y  − y ) = δy. for relatively short distances, where the earth's curvature may be neglected, the run is the difference in distance from a fixed point measured along a level, horizontal line, or in other words, the run is (x  − x ) = δx. here the slope of the road between the two points is simply described as the ratio of the altitude change to the horizontal distance between any two points on the line. in mathematical language, the slope m of the line is the concept of slope applies directly to grades or gradients in geography and civil engineering. through trigonometry, the slope m of a line is related to its angle of incline θ by the tangent function thus, a   ° rising line has a slope of +  and a   ° falling line has a slope of − . as a generalization of this practical description, the mathematics of differential calculus defines the slope of a curve at a point as the slope of the tangent line at that point. when the curve is given by a series of points in a diagram or in a list of the coordinates of points, the slope may be calculated not at a point but between any two given points. when the curve is given as a continuous function, perhaps as an algebraic formula, then the differential calculus provides rules giving a formula for the slope of the curve at any point in the middle of the curve. this generalization of the concept of slope allows very complex constructions to be planned and built that go well beyond static structures that are either horizontals or verticals, but can change in time, move in curves, and change depending on the rate of change of other factors. thereby, the simple idea of slope becomes one of the main basis of the modern world in terms of both technology and the built environment. the slope of a line in the plane containing the x and y axes is generally represented by the letter m, and is defined as the change in the y coordinate divided by the corresponding change in the x coordinate, between two distinct points on the line. this is described by the following equation: "
410,410,solutions of triangles,2,https://en.wikipedia.org/wiki/Solution_of_triangles,"solution of triangles (latin: solutio triangulorum) is the main trigonometric problem of finding the characteristics of a triangle (angles and lengths of sides), when some of these are known. the triangle can be located on a plane or on a sphere. applications requiring triangle solutions include geodesy, astronomy, construction, and navigation. a general form triangle has six main characteristics (see picture): three linear (side lengths a, b, c) and three angular (α, β, γ). the classical plane trigonometry problem is to specify three of the six characteristics and determine the other three. a triangle can be uniquely determined in this sense when given any of the following: for all cases in the plane, at least one of the side lengths must be specified. if only the angles are given, the side lengths cannot be determined, because any similar triangle is a solution. the standard method of solving the problem is to use fundamental relations. there are other (sometimes practically useful) universal relations: the law of cotangents and mollweide's formula. let three side lengths a, b, c be specified. to find the angles α, β, the law of cosines can be used: then angle γ =    ° − α − β. some sources recommend to find angle β from the law of sines but (as note   above states) there is a risk of confusing an acute angle value with an obtuse one. another method of calculating the angles from known sides is to apply the law of cotangents. here the lengths of sides a, b and the angle γ between these sides are known. the third side can be determined from the law of cosines: "
411,411,Spherical Coordinate System,2,https://en.wikipedia.org/wiki/Spherical_coordinate_system," in mathematics, a spherical coordinate system is a coordinate system for three-dimensional space where the position of a point is specified by three numbers: the radial distance of that point from a fixed origin, its polar angle measured from a fixed zenith direction, and the azimuthal angle of its orthogonal projection on a reference plane that passes through the origin and is orthogonal to the zenith, measured from a fixed reference direction on that plane. it can be seen as the three-dimensional version of the polar coordinate system. the radial distance is also called the radius or radial coordinate. the polar angle may be called colatitude, zenith angle, normal angle, or inclination angle. the use of symbols and the order of the coordinates differs among sources and disciplines. this article will use the iso convention frequently encountered in physics: ( r , θ , φ ) {\displaystyle (r,\theta ,\varphi )} gives the radial distance, polar angle, and azimuthal angle. in many mathematics books, ( ρ , θ , φ ) {\displaystyle (\rho ,\theta ,\varphi )} or ( r , θ , φ ) {\displaystyle (r,\theta ,\varphi )} gives the radial distance, azimuthal angle, and polar angle, switching the meanings of θ and φ. other conventions are also used, such as r for radius from the z-axis, so great care needs to be taken to check the meaning of the symbols. according to the conventions of geographical coordinate systems, positions are measured by latitude, longitude, and height (altitude). there are a number of celestial coordinate systems based on different fundamental planes and with different terms for the various coordinates. the spherical coordinate systems used in mathematics normally use radians rather than degrees and measure the azimuthal angle counterclockwise from the x-axis to the y-axis rather than clockwise from north ( °) to east (+  °) like the horizontal coordinate system. the polar angle is often replaced by the elevation angle measured from the reference plane, so that the elevation angle of zero is at the horizon. the spherical coordinate system generalizes the two-dimensional polar coordinate system. it can also be extended to higher-dimensional spaces and is then referred to as a hyperspherical coordinate system. to define a spherical coordinate system, one must choose two orthogonal directions, the zenith and the azimuth reference, and an origin point in space. these choices determine a reference plane that contains the origin and is perpendicular to the zenith. the spherical coordinates of a point p are then defined as follows: the sign of the azimuth is determined by choosing what is a positive sense of turning about the zenith. this choice is arbitrary, and is part of the coordinate system's definition. the elevation angle is    degrees (π/  radians) minus the inclination angle. if the inclination is zero or     degrees (π radians), the azimuth is arbitrary. if the radius is zero, both azimuth and inclination are arbitrary. "
412,412,Square Matrix,1,https://en.wikipedia.org/wiki/Square_matrix,"in mathematics, a square matrix is a matrix with the same number of rows and columns. an n-by-n matrix is known as a square matrix of order n {\displaystyle n} . any two square matrices of the same order can be added and multiplied. square matrices are often used to represent simple linear transformations, such as shearing or rotation. for example, if r {\displaystyle r} is a square matrix representing a rotation (rotation matrix) and v {\displaystyle \mathbf {v} } is a column vector describing the position of a point in space, the product r v {\displaystyle r\mathbf {v} } yields another column vector describing the position of that point after that rotation. if v {\displaystyle \mathbf {v} } is a row vector, the same transformation can be obtained using v r t {\displaystyle \mathbf {v} r^{\mathsf {t}}} , where r t {\displaystyle r^{\mathsf {t}}} is the transpose of r {\displaystyle r} . the entries a i i {\displaystyle a_{ii}} (i =  , …, n) form the main diagonal of a square matrix. they lie on the imaginary line which runs from the top left corner to the bottom right corner of the matrix. for instance, the main diagonal of the  ×  matrix above contains the elements a   =  , a   =   , a   =  , a   =   . the diagonal of a square matrix from the top right to the bottom left corner is called antidiagonal or counterdiagonal. if all entries outside the main diagonal are zero, a {\displaystyle a} is called a diagonal matrix. if only all entries above (or below) the main diagonal are zero, a {\displaystyle a} is called an upper (or lower) triangular matrix. the identity matrix i n {\displaystyle i_{n}} of size n {\displaystyle n} is the n × n {\displaystyle n\times n} matrix in which all the elements on the main diagonal are equal to   and all other elements are equal to  , e.g. it is a square matrix of order n {\displaystyle n} , and also a special kind of diagonal matrix. it is called identity matrix because multiplication with it leaves a matrix unchanged: a square matrix a {\displaystyle a} is called invertible or non-singular if there exists a matrix b {\displaystyle b} such that if b {\displaystyle b} exists, it is unique and is called the inverse matrix of a {\displaystyle a} , denoted a −   {\displaystyle a^{- }} . a square matrix a {\displaystyle a} that is equal to its transpose, i.e., a t = a {\displaystyle a^{\mathsf {t}}=a} , is a symmetric matrix. if instead a t = − a {\displaystyle a^{\mathsf {t}}=-a} , then a {\displaystyle a} is called a skew-symmetric matrix. "
413,413,standard deviation,2,https://en.wikipedia.org/wiki/Standard_deviation," in statistics, the standard deviation is a measure of the amount of variation or dispersion of a set of values. a low standard deviation indicates that the values tend to be close to the mean (also called the expected value) of the set, while a high standard deviation indicates that the values are spread out over a wider range. standard deviation may be abbreviated sd, and is most commonly represented in mathematical texts and equations by the lower case greek letter sigma σ, for the population standard deviation, or the latin letter s, for the sample standard deviation. the standard deviation of a random variable, sample, statistical population, data set, or probability distribution is the square root of its variance. it is algebraically simpler, though in practice less robust, than the average absolute deviation. a useful property of the standard deviation is that, unlike the variance, it is expressed in the same unit as the data. the standard deviation of a population or sample and the standard error of a statistic (e.g., of the sample mean) are quite different, but related. the sample mean's standard error is the standard deviation of the set of means that would be found by drawing an infinite number of repeated samples from the population and computing a mean for each sample. the mean's standard error turns out to equal the population standard deviation divided by the square root of the sample size, and is estimated by using the sample standard deviation divided by the square root of the sample size. for example, a poll's standard error (what is reported as the margin of error of the poll), is the expected standard deviation of the estimated mean if the same poll were to be conducted multiple times. thus, the standard error estimates the standard deviation of an estimate, which itself measures how much the estimate depends on the particular sample that was taken from the population. in science, it is common to report both the standard deviation of the data (as a summary statistic) and the standard error of the estimate (as a measure of potential error in the findings). by convention, only effects more than two standard errors away from a null expectation are considered ""statistically significant"", a safeguard against spurious conclusion that is really due to random sampling error. when only a sample of data from a population is available, the term standard deviation of the sample or sample standard deviation can refer to either the above-mentioned quantity as applied to those data, or to a modified quantity that is an unbiased estimate of the population standard deviation (the standard deviation of the entire population). suppose that the entire population of interest is eight students in a particular class. for a finite set of numbers, the population standard deviation is found by taking the square root of the average of the squared deviations of the values subtracted from their average value. the marks of a class of eight students (that is, a statistical population) are the following eight values: these eight data points have the mean (average) of  : first, calculate the deviations of each data point from the mean, and square the result of each: "
414,414,stars and bars,2,https://en.wikipedia.org/wiki/Stars_and_bars_(combinatorics),"in the context of combinatorial mathematics, stars and bars (also called ""sticks and stones"", ""balls and bars"", and ""dots and dividers"" ) is a graphical aid for deriving certain combinatorial theorems. it was popularized by william feller in his classic book on probability. it can be used to solve many simple counting problems, such as how many ways there are to put n indistinguishable balls into k distinguishable bins. the stars and bars method is often introduced specifically to prove the following two theorems of elementary combinatorics concerning the number of solutions to an equation. for any pair of positive integers n and k, the number of k-tuples of positive integers whose sum is n is equal to the number of (k −  )-element subsets of a set with n −   elements. for example, if n =    {\displaystyle n=  } and k =   {\displaystyle k= } , the theorem gives the number of solutions to: with x   , x   , x   , x   >   {\displaystyle x_{ },x_{ },x_{ },x_{ }> } the answer is given by the binomial coefficient ( n −   k −   ) {\displaystyle {n- } \choose {k- }} . for the above example, there are (    −     −   ) = (     ) =    {\displaystyle {\binom {  - }{ - }}={\binom { }{ }}=  } of them. these consist of the permutations of the tuples (   ,   ,   ,   ) , (   ,   ,   ,   ) , (   ,   ,   ,   ) , (   ,   ,   ,   ) , (   ,   ,   ,   ) , (   ,   ,   ,   ) , (   ,   ,   ,   ) , (   ,   ,   ,   ) , (   ,   ,   ,   ) {\displaystyle ( , , , ),( , , , ),( , , , ),( , , , ),( , , , ),( , , , ),( , , , ),( , , , ),( , , , )} . for any pair of positive integers n and k, the number of k-tuples of non-negative integers whose sum is n is equal to the number of multisets of cardinality k −   taken from a set of size n +  . for example, if n =    {\displaystyle n=  } and k =   {\displaystyle k= } , the theorem gives the number of solutions to: with x   , x   , x   , x   ≥   {\displaystyle x_{ },x_{ },x_{ },x_{ }\geq  } "
415,415,Stationary Point,1,https://en.wikipedia.org/wiki/Stationary_point,"in mathematics, particularly in calculus, a stationary point of a differentiable function of one variable is a point on the graph of the function where the function's derivative is zero. informally, it is a point where the function ""stops"" increasing or decreasing (hence the name). for a differentiable function of several real variables, a stationary point is a point on the surface of the graph where all its partial derivatives are zero (equivalently, the gradient is zero). stationary points are easy to visualize on the graph of a function of one variable: they correspond to the points on the graph where the tangent is horizontal (i.e., parallel to the x-axis). for a function of two variables, they correspond to the points on the graph where the tangent plane is parallel to the xy plane. a turning point is a point at which the derivative changes sign. a turning point may be either a relative maximum or a relative minimum (also known as local minimum and maximum). if the function is differentiable, then a turning point is a stationary point; however not all stationary points are turning points. if the function is twice differentiable, the stationary points that are not turning points are horizontal inflection points. for example, the function x ↦ x   {\displaystyle x\mapsto x^{ }} has a stationary point at x =  , which is also an inflection point, but is not a turning point. isolated stationary points of a c   {\displaystyle c^{ }} real valued function f : r → r {\displaystyle f\colon \mathbb {r} \to \mathbb {r} } are classified into four kinds, by the first derivative test: the first two options are collectively known as ""local extrema"". similarly a point that is either a global (or absolute) maximum or a global (or absolute) minimum is called a global (or absolute) extremum. the last two options—stationary points that are not local extremum—are known as saddle points. by fermat's theorem, global extrema must occur (for a c   {\displaystyle c^{ }} function) on the boundary or at stationary points. determining the position and nature of stationary points aids in curve sketching of differentiable functions. solving the equation f'(x) =   returns the x-coordinates of all stationary points; the y-coordinates are trivially the function values at those x-coordinates. the specific nature of a stationary point at x can in some cases be determined by examining the second derivative f''(x): a more straightforward way of determining the nature of a stationary point is by examining the function values between the stationary points (if the function is defined and continuous between them). a simple example of a point of inflection is the function f(x) = x . there is a clear change of concavity about the point x =  , and we can prove this by means of calculus. the second derivative of f is the everywhere-continuous  x, and at x =  , f′′ =  , and the sign changes about this point. so x =   is a point of inflection. "
416,416,Statistical dispersion,0,https://en.wikipedia.org/wiki/Statistical_dispersion,"in statistics, dispersion (also called variability, scatter, or spread) is the extent to which a distribution is stretched or squeezed. common examples of measures of statistical dispersion are the variance, standard deviation, and interquartile range. for instance, when the variance of data in a set is large, the data is widely scattered. on the other hand, when the variance is small, the data in the set is clustered. dispersion is contrasted with location or central tendency, and together they are the most used properties of distributions. a measure of statistical dispersion is a nonnegative real number that is zero if all the data are the same and increases as the data become more diverse. most measures of dispersion have the same units as the quantity being measured. in other words, if the measurements are in metres or seconds, so is the measure of dispersion. examples of dispersion measures include: these are frequently used (together with scale factors) as estimators of scale parameters, in which capacity they are called estimates of scale. robust measures of scale are those unaffected by a small number of outliers, and include the iqr and mad. all the above measures of statistical dispersion have the useful property that they are location-invariant and linear in scale. this means that if a random variable x has a dispersion of sx then a linear transformation y = ax + b for real a and b should have dispersion sy = |a|sx, where |a| is the absolute value of a, that is, ignores a preceding negative sign –. other measures of dispersion are dimensionless. in other words, they have no units even if the variable itself has units. these include: there are other measures of dispersion: some measures of dispersion have specialized purposes. the allan variance can be used for applications where the noise disrupts convergence. the hadamard variance can be used to counteract linear frequency drift sensitivity. for categorical variables, it is less common to measure dispersion by a single number; see qualitative variation. one measure that does so is the discrete entropy. "
417,417,Statistical Hypothesis Testing,2,https://en.wikipedia.org/wiki/Statistical_hypothesis_testing," a statistical hypothesis test is a method of statistical inference used to decide whether the data at hand sufficiently support a particular hypothesis. while hypothesis testing was popularized early in the   th century, early forms were used in the     s. the first use is credited to john arbuthnot (    ), followed by pierre-simon laplace (    s), in analyzing the human sex ratio at birth; see § human sex ratio. modern significance testing is largely the product of karl pearson (p-value, pearson's chi-squared test), william sealy gosset (student's t-distribution), and ronald fisher (""null hypothesis"", analysis of variance, ""significance test""), while hypothesis testing was developed by jerzy neyman and egon pearson (son of karl). ronald fisher began his life in statistics as a bayesian (zabell     ), but fisher soon grew disenchanted with the subjectivity involved (namely use of the principle of indifference when determining prior probabilities), and sought to provide a more ""objective"" approach to inductive inference. fisher was an agricultural statistician who emphasized rigorous experimental design and methods to extract a result from few samples assuming gaussian distributions. neyman (who teamed with the younger pearson) emphasized mathematical rigor and methods to obtain more results from many samples and a wider range of distributions. modern hypothesis testing is an inconsistent hybrid of the fisher vs neyman/pearson formulation, methods and terminology developed in the early   th century. fisher popularized the ""significance test"". he required a null-hypothesis (corresponding to a population frequency distribution) and a sample. his (now familiar) calculations determined whether to reject the null-hypothesis or not. significance testing did not utilize an alternative hypothesis so there was no concept of a type ii error. the p-value was devised as an informal, but objective, index meant to help a researcher determine (based on other knowledge) whether to modify future experiments or strengthen one's faith in the null hypothesis. hypothesis testing (and type i/ii errors) was devised by neyman and pearson as a more objective alternative to fisher's p-value, also meant to determine researcher behaviour, but without requiring any inductive inference by the researcher. neyman & pearson considered a different problem (which they called ""hypothesis testing""). they initially considered two simple hypotheses (both with frequency distributions). they calculated two probabilities and typically selected the hypothesis associated with the higher probability (the hypothesis more likely to have generated the sample). their method always selected a hypothesis. it also allowed the calculation of both types of error probabilities. fisher and neyman/pearson clashed bitterly. neyman/pearson considered their formulation to be an improved generalization of significance testing. (the defining paper was abstract. mathematicians have generalized and refined the theory for decades. ) fisher thought that it was not applicable to scientific research because often, during the course of the experiment, it is discovered that the initial assumptions about the null hypothesis are questionable due to unexpected sources of error. he believed that the use of rigid reject/accept decisions based on models formulated before data is collected was incompatible with this common scenario faced by scientists and attempts to apply this method to scientific research would lead to mass confusion. the dispute between fisher and neyman–pearson was waged on philosophical grounds, characterized by a philosopher as a dispute over the proper role of models in statistical inference. "
418,418,Statistical Mathematics,2,https://en.wikipedia.org/wiki/Statistics,"statistics is the discipline that concerns the collection, organization, analysis, interpretation, and presentation of data. in applying statistics to a scientific, industrial, or social problem, it is conventional to begin with a statistical population or a statistical model to be studied. populations can be diverse groups of people or objects such as ""all people living in a country"" or ""every atom composing a crystal"". statistics deals with every aspect of data, including the planning of data collection in terms of the design of surveys and experiments. when census data cannot be collected, statisticians collect data by developing specific experiment designs and survey samples. representative sampling assures that inferences and conclusions can reasonably extend from the sample to the population as a whole. an experimental study involves taking measurements of the system under study, manipulating the system, and then taking additional measurements using the same procedure to determine if the manipulation has modified the values of the measurements. in contrast, an observational study does not involve experimental manipulation. two main statistical methods are used in data analysis: descriptive statistics, which summarize data from a sample using indexes such as the mean or standard deviation, and inferential statistics, which draw conclusions from data that are subject to random variation (e.g., observational errors, sampling variation). descriptive statistics are most often concerned with two sets of properties of a distribution (sample or population): central tendency (or location) seeks to characterize the distribution's central or typical value, while dispersion (or variability) characterizes the extent to which members of the distribution depart from its center and each other. inferences on mathematical statistics are made under the framework of probability theory, which deals with the analysis of random phenomena. a standard statistical procedure involves the collection of data leading to test of the relationship between two statistical data sets, or a data set and synthetic data drawn from an idealized model. a hypothesis is proposed for the statistical relationship between the two data sets, and this is compared as an alternative to an idealized null hypothesis of no relationship between two data sets. rejecting or disproving the null hypothesis is done using statistical tests that quantify the sense in which the null can be proven false, given the data that are used in the test. working from a null hypothesis, two basic forms of error are recognized: type i errors (null hypothesis is falsely rejected giving a ""false positive"") and type ii errors (null hypothesis fails to be rejected and an actual relationship between populations is missed giving a ""false negative""). multiple problems have come to be associated with this framework, ranging from obtaining a sufficient sample size to specifying an adequate null hypothesis. measurement processes that generate statistical data are also subject to error. many of these errors are classified as random (noise) or systematic (bias), but other types of errors (e.g., blunder, such as when an analyst reports incorrect units) can also occur. the presence of missing data or censoring may result in biased estimates and specific techniques have been developed to address these problems. statistics is a mathematical body of science that pertains to the collection, analysis, interpretation or explanation, and presentation of data, or as a branch of mathematics. some consider statistics to be a distinct mathematical science rather than a branch of mathematics. while many scientific investigations make use of data, statistics is concerned with the use of data in the context of uncertainty and decision making in the face of uncertainty. in applying statistics to a problem, it is common practice to start with a population or process to be studied. populations can be diverse topics such as ""all people living in a country"" or ""every atom composing a crystal"". ideally, statisticians compile data about the entire population (an operation called census). this may be organized by governmental statistical institutes. descriptive statistics can be used to summarize the population data. numerical descriptors include mean and standard deviation for continuous data (like income), while frequency and percentage are more useful in terms of describing categorical data (like education). when a census is not feasible, a chosen subset of the population called a sample is studied. once a sample that is representative of the population is determined, data is collected for the sample members in an observational or experimental setting. again, descriptive statistics can be used to summarize the sample data. however, drawing the sample contains an element of randomness; hence, the numerical descriptors from the sample are also prone to uncertainty. to draw meaningful conclusions about the entire population, inferential statistics is needed. it uses patterns in the sample data to draw inferences about the population represented while accounting for randomness. these inferences may take the form of answering yes/no questions about the data (hypothesis testing), estimating numerical characteristics of the data (estimation), describing associations within the data (correlation), and modeling relationships within the data (for example, using regression analysis). inference can extend to forecasting, prediction, and estimation of unobserved values either in or associated with the population being studied. it can include extrapolation and interpolation of time series or spatial data, and data mining. mathematical statistics is the application of mathematics to statistics. mathematical techniques used for this include mathematical analysis, linear algebra, stochastic analysis, differential equations, and measure-theoretic probability theory. the early writings on statistical inference date back to arab mathematicians and cryptographers, during the islamic golden age between the  th and   th centuries. al-khalil (   –   ) wrote the book of cryptographic messages, which contains the first use of permutations and combinations, to list all possible arabic words with and without vowels. in his book, manuscript on deciphering cryptographic messages, al-kindi gave a detailed description of how to use frequency analysis to decipher encrypted messages. al-kindi also made the earliest known use of statistical inference, while he and later arab cryptographers developed the early statistical methods for decoding encrypted messages. ibn adlan (    –    ) later made an important contribution, on the use of sample size in frequency analysis. "
419,419,Straight Line,1,https://en.wikipedia.org/wiki/Straight_Lines,straight lines may refer to: 
420,420,"Student,s T-distribution",3,https://en.wikipedia.org/wiki/Student%27s_t-distribution,"     + x γ ( ν +     ) ×   f   (     , ν +     ;     ; − x   ν ) π ν γ ( ν   ) {\displaystyle {\begin{matrix}{\frac { }{ }}+x\gamma \left({\frac {\nu + }{ }}\right)\times \\[ . em]{\frac {\,_{ }f_{ }\left({\frac { }{ }},{\frac {\nu + }{ }};{\frac { }{ }};-{\frac {x^{ }}{\nu }}\right)}{{\sqrt {\pi \nu }}\,\gamma \left({\frac {\nu }{ }}\right)}}\end{matrix}}} ν +     [ ψ (   + ν   ) − ψ ( ν   ) ] + ln ⁡ [ ν b ( ν   ,     ) ] (nats) {\displaystyle {\begin{matrix}{\frac {\nu + }{ }}\left[\psi \left({\frac { +\nu }{ }}\right)-\psi \left({\frac {\nu }{ }}\right)\right]\\[ . em]+\ln {\left[{\sqrt {\nu }}b\left({\frac {\nu }{ }},{\frac { }{ }}\right)\right]}\,{\scriptstyle {\text{(nats)}}}\end{matrix}}} k ν /   ( ν | t | ) ⋅ ( ν | t | ) ν /   γ ( ν /   )   ν /   −   {\displaystyle \textstyle {\frac {k_{\nu / }\left({\sqrt {\nu }}|t|\right)\cdot \left({\sqrt {\nu }}|t|\right)^{\nu / }}{\gamma (\nu / ) ^{\nu / - }}}} for ν >   {\displaystyle \nu > } in probability and statistics, student's t-distribution (or simply the t-distribution) is any member of a family of continuous probability distributions that arise when estimating the mean of a normally distributed population in situations where the sample size is small and the population's standard deviation is unknown. it was developed by english statistician william sealy gosset under the pseudonym ""student"". the t-distribution plays a role in a number of widely used statistical analyses, including student's t-test for assessing the statistical significance of the difference between two sample means, the construction of confidence intervals for the difference between two population means, and in linear regression analysis. student's t-distribution also arises in the bayesian analysis of data from a normal family. if we take a sample of n {\displaystyle n} observations from a normal distribution, then the t-distribution with ν = n −   {\displaystyle \nu =n- } degrees of freedom can be defined as the distribution of the location of the sample mean relative to the true mean, divided by the sample standard deviation, after multiplying by the standardizing term n {\displaystyle {\sqrt {n}}} . in this way, the t-distribution can be used to construct a confidence interval for the true mean. the t-distribution is symmetric and bell-shaped, like the normal distribution. however, the t-distribution has heavier tails, meaning that it is more prone to producing values that fall far from its mean. this makes it useful for understanding the statistical behavior of certain types of ratios of random quantities, in which variation in the denominator is amplified and may produce outlying values when the denominator of the ratio falls close to zero. the student's t-distribution is a special case of the generalised hyperbolic distribution. in statistics, the t-distribution was first derived as a posterior distribution in      by helmert and lüroth. the t-distribution also appeared in a more general form as pearson type iv distribution in karl pearson's      paper. in the english-language literature, the distribution takes its name from william sealy gosset's      paper in biometrika under the pseudonym ""student"". gosset worked at the guinness brewery in dublin, ireland, and was interested in the problems of small samples – for example, the chemical properties of barley where sample sizes might be as few as  . one version of the origin of the pseudonym is that gosset's employer preferred staff to use pen names when publishing scientific papers instead of their real name, so he used the name ""student"" to hide his identity. another version is that guinness did not want their competitors to know that they were using the t-test to determine the quality of raw material. gosset's paper refers to the distribution as the ""frequency distribution of standard deviations of samples drawn from a normal population"". it became well known through the work of ronald fisher, who called the distribution ""student's distribution"" and represented the test value with the letter t. "
421,421,Subset,1,https://en.wikipedia.org/wiki/Subset,"in mathematics, set a is a subset of a set b if all elements of a are also elements of b; b is then a superset of a. it is possible for a and b to be equal; if they are unequal, then a is a proper subset of b. the relationship of one set being a subset of another is called inclusion (or sometimes containment). a is a subset of b may also be expressed as b includes (or contains) a or a is included (or contained) in b. the subset relation defines a partial order on sets. in fact, the subsets of a given set form a boolean algebra under the subset relation, in which the join and meet are given by intersection and union, and the subset relation itself is the boolean inclusion relation. if a and b are sets and every element of a is also an element of b, then: if a is a subset of b, but a is not equal to b (i.e. there exists at least one element of b which is not an element of a), then: for any set s, the inclusion relation ⊆ {\displaystyle \subseteq } is a partial order on the set p ( s ) {\displaystyle {\mathcal {p}}(s)} (the power set of s—the set of all subsets of s ) defined by a ≤ b ⟺ a ⊆ b {\displaystyle a\leq b\iff a\subseteq b} . we may also partially order p ( s ) {\displaystyle {\mathcal {p}}(s)} by reverse set inclusion by defining a ≤ b if and only if b ⊆ a . {\displaystyle a\leq b{\text{ if and only if }}b\subseteq a.} when quantified, a ⊆ b {\displaystyle a\subseteq b} is represented as ∀ x ( x ∈ a ⟹ x ∈ b ) . {\displaystyle \forall x\left(x\in a\implies x\in b\right).} we can prove the statement a ⊆ b {\displaystyle a\subseteq b} by applying a proof technique known as the element argument :let sets a and b be given. to prove that a ⊆ b , {\displaystyle a\subseteq b,} the validity of this technique can be seen as a consequence of universal generalization: the technique shows c ∈ a ⟹ c ∈ b {\displaystyle c\in a\implies c\in b} for an arbitrarily chosen element c. universal generalisation then implies ∀ x ( x ∈ a ⟹ x ∈ b ) , {\displaystyle \forall x\left(x\in a\implies x\in b\right),} which is equivalent to a ⊆ b , {\displaystyle a\subseteq b,} as stated above. some authors use the symbols ⊂ {\displaystyle \subset } and ⊃ {\displaystyle \supset } to indicate subset and superset respectively; that is, with the same meaning and instead of the symbols, ⊆ {\displaystyle \subseteq } and ⊇ . {\displaystyle \supseteq .} for example, for these authors, it is true of every set a that a ⊂ a . {\displaystyle a\subset a.} "
422,422,sums of finite arithmetic and geometric progressions,2,https://en.wikipedia.org/wiki/Summation,"in mathematics, summation is the addition of a sequence of any kind of numbers, called addends or summands; the result is their sum or total. beside numbers, other types of values can be summed as well: functions, vectors, matrices, polynomials and, in general, elements of any type of mathematical objects on which an operation denoted ""+"" is defined. summations of infinite sequences are called series. they involve the concept of limit, and are not considered in this article. the summation of an explicit sequence is denoted as a succession of additions. for example, summation of [ ,  ,  ,  ] is denoted   +   +   +  , and results in  , that is,   +   +   +   =  . because addition is associative and commutative, there is no need of parentheses, and the result is the same irrespective of the order of the summands. summation of a sequence of only one element results in this element itself. summation of an empty sequence (a sequence with no elements), by convention, results in  . very often, the elements of a sequence are defined, through a regular pattern, as a function of their place in the sequence. for simple patterns, summation of long sequences may be represented with most summands replaced by ellipses. for example, summation of the first     natural numbers may be written as   +   +   +   + ⋯ +    +    . otherwise, summation is denoted by using σ notation, where ∑ {\textstyle \sum } is an enlarged capital greek letter sigma. for example, the sum of the first n natural numbers can be denoted as ∑ i =   n i . {\textstyle \sum _{i= }^{n}i.} for long summations, and summations of variable length (defined with ellipses or σ notation), it is a common problem to find closed-form expressions for the result. for example,[a] although such formulas do not always exist, many summation formulas have been discovered—with some of the most common and elementary ones being listed in the remainder of this article. mathematical notation uses a symbol that compactly represents summation of many similar terms: the summation symbol, ∑ {\textstyle \sum } , an enlarged form of the upright capital greek letter sigma. this is defined as where i is the index of summation; ai is an indexed variable representing each term of the sum; m is the lower bound of summation, and n is the upper bound of summation. the ""i = m"" under the summation symbol means that the index i starts out equal to m. the index, i, is incremented by one for each successive term, stopping when i = n.[b] this is read as ""sum of ai, from i = m to n"". here is an example showing the summation of squares: "
423,423,Surface area,1,https://en.wikipedia.org/wiki/Surface_area," the surface area of a solid object is a measure of the total area that the surface of the object occupies. the mathematical definition of surface area in the presence of curved surfaces is considerably more involved than the definition of arc length of one-dimensional curves, or of the surface area for polyhedra (i.e., objects with flat polygonal faces), for which the surface area is the sum of the areas of its faces. smooth surfaces, such as a sphere, are assigned surface area using their representation as parametric surfaces. this definition of surface area is based on methods of infinitesimal calculus and involves partial derivatives and double integration. a general definition of surface area was sought by henri lebesgue and hermann minkowski at the turn of the twentieth century. their work led to the development of geometric measure theory, which studies various notions of surface area for irregular objects of any dimension. an important example is the minkowski content of a surface. while the areas of many simple surfaces have been known since antiquity, a rigorous mathematical definition of area requires a great deal of care. this should provide a function which assigns a positive real number to a certain class of surfaces that satisfies several natural requirements. the most fundamental property of the surface area is its additivity: the area of the whole is the sum of the areas of the parts. more rigorously, if a surface s is a union of finitely many pieces s , …, sr which do not overlap except at their boundaries, then surface areas of flat polygonal shapes must agree with their geometrically defined area. since surface area is a geometric notion, areas of congruent surfaces must be the same and the area must depend only on the shape of the surface, but not on its position and orientation in space. this means that surface area is invariant under the group of euclidean motions. these properties uniquely characterize surface area for a wide class of geometric surfaces called piecewise smooth. such surfaces consist of finitely many pieces that can be represented in the parametric form with a continuously differentiable function r → . {\displaystyle {\vec {r}}.} the area of an individual piece is defined by the formula thus the area of sd is obtained by integrating the length of the normal vector r → u × r → v {\displaystyle {\vec {r}}_{u}\times {\vec {r}}_{v}} to the surface over the appropriate region d in the parametric uv plane. the area of the whole surface is then obtained by adding together the areas of the pieces, using additivity of surface area. the main formula can be specialized to different classes of surfaces, giving, in particular, formulas for areas of graphs z = f(x,y) and surfaces of revolution. one of the subtleties of surface area, as compared to arc length of curves, is that surface area cannot be defined simply as the limit of areas of polyhedral shapes approximating a given smooth surface. it was demonstrated by hermann schwarz that already for the cylinder, different choices of approximating flat surfaces can lead to different limiting values of the area; this example is known as the schwarz lantern. various approaches to a general definition of surface area were developed in the late nineteenth and the early twentieth century by henri lebesgue and hermann minkowski. while for piecewise smooth surfaces there is a unique natural notion of surface area, if a surface is very irregular, or rough, then it may not be possible to assign an area to it at all. a typical example is given by a surface with spikes spread throughout in a dense fashion. many surfaces of this type occur in the study of fractals. extensions of the notion of area which partially fulfill its function and may be defined even for very badly irregular surfaces are studied in geometric measure theory. a specific example of such an extension is the minkowski content of the surface. "
424,424,Surface Integral,3,https://en.wikipedia.org/wiki/Surface_integral,"in mathematics, particularly multivariable calculus, a surface integral is a generalization of multiple integrals to integration over surfaces. it can be thought of as the double integral analogue of the line integral. given a surface, one may integrate a scalar field (that is, a function of position which returns a scalar as a value) over the surface, or a vector field (that is, a function which returns a vector as value). if a region r is not flat, then it is called a surface as shown in the illustration. surface integrals have applications in physics, particularly with the theories of classical electromagnetism. to find an explicit formula for the surface integral over a surface s, we need to parameterize s by defining a system of curvilinear coordinates on s, like the latitude and longitude on a sphere. let such a parameterization be r(s, t), where (s, t) varies in some region t in the plane. then, the surface integral is given by where the expression between bars on the right-hand side is the magnitude of the cross product of the partial derivatives of r(s, t), and is known as the surface element (which would, for example, yield a smaller value near the poles of a sphere. where the lines of longitude converge more dramatically and latitudinal coordinates are more compactly spaced). the surface integral can also be expressed in the equivalent form where g is the determinant of the first fundamental form of the surface mapping r(s, t). for example, if we want to find the surface area of the graph of some scalar function, say z = f(x, y), we have where r = (x, y, z) = (x, y, f(x, y)). so that ∂ r ∂ x = (   ,   , f x ( x , y ) ) {\displaystyle {\partial \mathbf {r} \over \partial x}=( , ,f_{x}(x,y))} , and ∂ r ∂ y = (   ,   , f y ( x , y ) ) {\displaystyle {\partial \mathbf {r} \over \partial y}=( , ,f_{y}(x,y))} . so, which is the standard formula for the area of a surface described this way. one can recognize the vector in the second-last line above as the normal vector to the surface. note that because of the presence of the cross product, the above formulas only work for surfaces embedded in three-dimensional space. this can be seen as integrating a riemannian volume form on the parameterized surface, where the metric tensor is given by the first fundamental form of the surface. "
425,425,Symmetric Matrix,1,https://en.wikipedia.org/wiki/Symmetric_matrix," in linear algebra, a symmetric matrix is a square matrix that is equal to its transpose. formally, a is symmetric ⟺ a = a t . {\displaystyle a{\text{ is symmetric}}\iff a=a^{\textsf {t}}.} because equal matrices have equal dimensions, only square matrices can be symmetric. the entries of a symmetric matrix are symmetric with respect to the main diagonal. so if a i j {\displaystyle a_{ij}} denotes the entry in the i {\displaystyle i} th row and j {\displaystyle j} th column then a is symmetric ⟺ for every i , j , a j i = a i j {\displaystyle a{\text{ is symmetric}}\iff {\text{ for every }}i,j,\quad a_{ji}=a_{ij}} for all indices i {\displaystyle i} and j . {\displaystyle j.} every square diagonal matrix is symmetric, since all off-diagonal elements are zero. similarly in characteristic different from  , each diagonal element of a skew-symmetric matrix must be zero, since each is its own negative. in linear algebra, a real symmetric matrix represents a self-adjoint operator over a real inner product space. the corresponding object for a complex inner product space is a hermitian matrix with complex-valued entries, which is equal to its conjugate transpose. therefore, in linear algebra over the complex numbers, it is often assumed that a symmetric matrix refers to one which has real-valued entries. symmetric matrices appear naturally in a variety of applications, and typical numerical linear algebra software makes special accommodations for them. the following   ×   {\displaystyle  \times  } matrix is symmetric: "
426,426,Symmetric Polynomial,0,https://en.wikipedia.org/wiki/Symmetric_polynomial,"in mathematics, a symmetric polynomial is a polynomial p(x , x , …, xn) in n variables, such that if any of the variables are interchanged, one obtains the same polynomial. formally, p is a symmetric polynomial if for any permutation σ of the subscripts  ,  , ..., n one has p(xσ( ), xσ( ), …, xσ(n)) = p(x , x , …, xn). symmetric polynomials arise naturally in the study of the relation between the roots of a polynomial in one variable and its coefficients, since the coefficients can be given by polynomial expressions in the roots, and all roots play a similar role in this setting. from this point of view the elementary symmetric polynomials are the most fundamental symmetric polynomials. a theorem states that any symmetric polynomial can be expressed in terms of elementary symmetric polynomials, which implies that every symmetric polynomial expression in the roots of a monic polynomial can alternatively be given as a polynomial expression in the coefficients of the polynomial. symmetric polynomials also form an interesting structure by themselves, independently of any relation to the roots of a polynomial. in this context other collections of specific symmetric polynomials, such as complete homogeneous, power sum, and schur polynomials play important roles alongside the elementary ones. the resulting structures, and in particular the ring of symmetric functions, are of great importance in combinatorics and in representation theory. the following polynomials in two variables x  and x  are symmetric: as is the following polynomial in three variables x , x , x : there are many ways to make specific symmetric polynomials in any number of variables (see the various types below). an example of a somewhat different flavor is where first a polynomial is constructed that changes sign under every exchange of variables, and taking the square renders it completely symmetric (if the variables represent the roots of a monic polynomial, this polynomial gives its discriminant). on the other hand, the polynomial in two variables is not symmetric, since if one exchanges x   {\displaystyle x_{ }} and x   {\displaystyle x_{ }} one gets a different polynomial, x   − x   {\displaystyle x_{ }-x_{ }} . similarly in three variables has only symmetry under cyclic permutations of the three variables, which is not sufficient to be a symmetric polynomial. however, the following is symmetric: "
427,427,Systems of Linear Equations,1,https://en.wikipedia.org/wiki/System_of_linear_equations,"in mathematics, a system of linear equations (or linear system) is a collection of one or more linear equations involving the same variables. for example, is a system of three equations in the three variables x, y, z. a solution to a linear system is an assignment of values to the variables such that all the equations are simultaneously satisfied. a solution to the system above is given by since it makes all three equations valid. the word ""system"" indicates that the equations are to be considered collectively, rather than individually. in mathematics, the theory of linear systems is the basis and a fundamental part of linear algebra, a subject which is used in most parts of modern mathematics. computational algorithms for finding the solutions are an important part of numerical linear algebra, and play a prominent role in engineering, physics, chemistry, computer science, and economics. a system of non-linear equations can often be approximated by a linear system (see linearization), a helpful technique when making a mathematical model or computer simulation of a relatively complex system. very often, the coefficients of the equations are real or complex numbers and the solutions are searched in the same set of numbers, but the theory and the algorithms apply for coefficients and solutions in any field. for solutions in an integral domain like the ring of the integers, or in other algebraic structures, other theories have been developed, see linear equation over a ring. integer linear programming is a collection of methods for finding the ""best"" integer solution (when there are many). gröbner basis theory provides algorithms when coefficients and unknowns are polynomials. also tropical geometry is an example of linear algebra in a more exotic structure. the system of one equation in one unknown has the solution however, a linear system is commonly considered as having at least two equations. the simplest kind of nontrivial linear system involves two equations and two variables: one method for solving such a system is as follows. first, solve the top equation for x {\displaystyle x} in terms of y {\displaystyle y} : "
428,428,equations of tangent,1,https://en.wikipedia.org/wiki/Tangent," in geometry, the tangent line (or simply tangent) to a plane curve at a given point is the straight line that ""just touches"" the curve at that point. leibniz defined it as the line through a pair of infinitely close points on the curve. more precisely, a straight line is said to be a tangent of a curve y = f(x) at a point x = c if the line passes through the point (c, f(c)) on the curve and has slope f'(c), where f' is the derivative of f. a similar definition applies to space curves and curves in n-dimensional euclidean space. as it passes through the point where the tangent line and the curve meet, called the point of tangency, the tangent line is ""going in the same direction"" as the curve, and is thus the best straight-line approximation to the curve at that point. the tangent line to a point on a differentiable curve can also be thought of as a tangent line approximation, the graph of the affine function that best approximates the original function at the given point. similarly, the tangent plane to a surface at a given point is the plane that ""just touches"" the surface at that point. the concept of a tangent is one of the most fundamental notions in differential geometry and has been extensively generalized; see tangent space. the word ""tangent"" comes from the latin tangere, ""to touch"". euclid makes several references to the tangent (ἐφαπτομένη ephaptoménē) to a circle in book iii of the elements (c.     bc). in apollonius' work conics (c.     bc) he defines a tangent as being a line such that no other straight line could fall between it and the curve. archimedes (c.     – c.     bc) found the tangent to an archimedean spiral by considering the path of a point moving along the curve. in the     s fermat developed the technique of adequality to calculate tangents and other problems in analysis and used this to calculate tangents to the parabola. the technique of adequality is similar to taking the difference between f ( x + h ) {\displaystyle f(x+h)} and f ( x ) {\displaystyle f(x)} and dividing by a power of h {\displaystyle h} . independently descartes used his method of normals based on the observation that the radius of a circle is always normal to the circle itself. these methods led to the development of differential calculus in the   th century. many people contributed. roberval discovered a general method of drawing tangents, by considering a curve as described by a moving point whose motion is the resultant of several simpler motions. rené-françois de sluse and johannes hudde found algebraic algorithms for finding tangents. further developments included those of john wallis and isaac barrow, leading to the theory of isaac newton and gottfried leibniz. "
429,429,half-angle formula,2,https://en.wikipedia.org/wiki/Tangent_half-angle_formula,"in trigonometry, tangent half-angle formulas relate the tangent of half of an angle to trigonometric functions of the entire angle. the tangent of half an angle is the stereographic projection of the circle onto a line. among these formulas are the following: from these one can derive identities expressing the sine, cosine, and tangent as functions of tangents of half-angles: using double-angle formulae and the pythagorean identity   + tan   ⁡ α =   / cos   ⁡ α {\displaystyle  +\tan ^{ }\alpha = {\big /}\cos ^{ }\alpha } gives taking the quotient of the formulae for sine and cosine yields combining the pythagorean identity with the double-angle formula for the cosine, cos ⁡   α = cos   ⁡ α − sin   ⁡ α =   −   sin   ⁡ α =   cos   ⁡ α −   {\displaystyle \cos  \alpha =\cos ^{ }\alpha -\sin ^{ }\alpha = - \sin ^{ }\alpha = \cos ^{ }\alpha - } , rearranging, and taking the square roots yields which, upon division gives alternatively, the absolute value signs may be dropped when working only in the first quadrant. also, using the angle addition and subtraction formulae for both the sine and cosine one obtains: "
430,430,Tangent lines to circles,2,https://en.wikipedia.org/wiki/Tangent_lines_to_circles,"in euclidean plane geometry, a tangent line to a circle is a line that touches the circle at exactly one point, never entering the circle's interior. tangent lines to circles form the subject of several theorems, and play an important role in many geometrical constructions and proofs. since the tangent line to a circle at a point p is perpendicular to the radius to that point, theorems involving tangent lines often involve radial lines and orthogonal circles. a tangent line t to a circle c intersects the circle at a single point t. for comparison, secant lines intersect a circle at two points, whereas another line may not intersect a circle at all. this property of tangent lines is preserved under many geometrical transformations, such as scalings, rotation, translations, inversions, and map projections. in technical language, these transformations do not change the incidence structure of the tangent line and circle, even though the line and circle may be deformed. the radius of a circle is perpendicular to the tangent line through its endpoint on the circle's circumference. conversely, the perpendicular to a radius through the same endpoint is a tangent line. the resulting geometrical figure of circle and tangent line has a reflection symmetry about the axis of the radius. no tangent line can be drawn through a point within a circle, since any such line must be a secant line. however, two tangent lines can be drawn to a circle from a point p outside of the circle. the geometrical figure of a circle and both tangent lines likewise has a reflection symmetry about the radial axis joining p to the center point o of the circle. thus the lengths of the segments from p to the two tangent points are equal. by the secant-tangent theorem, the square of this tangent length equals the power of the point p in the circle c. this power equals the product of distances from p to any two intersection points of the circle with a secant line passing through p. the tangent line t and the tangent point t have a conjugate relationship to one another, which has been generalized into the idea of pole points and polar lines. the same reciprocal relation exists between a point p outside the circle and the secant line joining its two points of tangency. if a point p is exterior to a circle with center o, and if the tangent lines from p touch the circle at points t and s, then ∠tps and ∠tos are supplementary (sum to    °). if a chord tm is drawn from the tangency point t of exterior point p and ∠ptm ≤   ° then ∠ptm = ( / )∠tom. suppose that the equation of the circle is ( x − a )   + ( y − b )   = r   {\displaystyle (x-a)^{ }+(y-b)^{ }=r^{ }} with center at ( a , b ) {\displaystyle (a,b)} . then the tangent line of the circle at ( x   , y   ) {\displaystyle (x_{ },y_{ })} is this can be proved by taking the implicit derivative of the circle. "
431,431,Tangential and normal components,2,https://en.wikipedia.org/wiki/Tangential_and_normal_components,"in mathematics, given a vector at a point on a curve, that vector can be decomposed uniquely as a sum of two vectors, one tangent to the curve, called the tangential component of the vector, and another one perpendicular to the curve, called the normal component of the vector. similarly a vector at a point on a surface can be broken down the same way. more generally, given a submanifold n of a manifold m, and a vector in the tangent space to m at a point of n, it can be decomposed into the component tangent to n and the component normal to n. more formally, let s {\displaystyle s} be a surface, and x {\displaystyle x} be a point on the surface. let v {\displaystyle \mathbf {v} } be a vector at x . {\displaystyle x.} then one can write uniquely v {\displaystyle \mathbf {v} } as a sum where the first vector in the sum is the tangential component and the second one is the normal component. it follows immediately that these two vectors are perpendicular to each other. to calculate the tangential and normal components, consider a unit normal to the surface, that is, a unit vector n ^ {\displaystyle {\hat {n}}} perpendicular to s {\displaystyle s} at x . {\displaystyle x.} then, and thus where "" ⋅ {\displaystyle \cdot } "" denotes the dot product. another formula for the tangential component is where "" × {\displaystyle \times } "" denotes the cross product. note that these formulas do not depend on the particular unit normal n ^ {\displaystyle {\hat {n}}} used (there exist two unit normals to any surface at a given point, pointing in opposite directions, so one of the unit normals is the negative of the other one). more generally, given a submanifold n of a manifold m and a point p ∈ n {\displaystyle p\in n} , we get a short exact sequence involving the tangent spaces: "
432,432,Taylor series,3,https://en.wikipedia.org/wiki/Taylor_series," in mathematics, the taylor series of a function is an infinite sum of terms that are expressed in terms of the function's derivatives at a single point. for most common functions, the function and the sum of its taylor series are equal near this point. taylor series are named after brook taylor, who introduced them in     . if   is the point where the derivatives are considered, a taylor series is also called a maclaurin series, after colin maclaurin, who made extensive use of this special case of taylor series in the   th century. the partial sum formed by the first n +   terms of a taylor series is a polynomial of degree n that is called the nth taylor polynomial of the function. taylor polynomials are approximations of a function, which become generally better as n increases. taylor's theorem gives quantitative estimates on the error introduced by the use of such approximations. if the taylor series of a function is convergent, its sum is the limit of the infinite sequence of the taylor polynomials. a function may differ from the sum of its taylor series, even if its taylor series is convergent. a function is analytic at a point x if it is equal to the sum of its taylor series in some open interval (or open disk in the complex plane) containing x. this implies that the function is analytic at every point of the interval (or disk). the taylor series of a real or complex-valued function f (x) that is infinitely differentiable at a real or complex number a is the power series where n! denotes the factorial of n. in the more compact sigma notation, this can be written as where f(n)(a) denotes the nth derivative of f evaluated at the point a. (the derivative of order zero of f is defined to be f itself and (x − a)  and  ! are both defined to be  .) when a =  , the series is also called a maclaurin series. the taylor series for any polynomial is the polynomial itself. the maclaurin series for  /  − x is the geometric series "
433,433,Telescoping Series,2,https://en.wikipedia.org/wiki/Telescoping_series,"in mathematics, a telescoping series is a series whose general term t n {\displaystyle t_{n}} can be written as t n = a n − a n +   {\displaystyle t_{n}=a_{n}-a_{n+ }} , i.e. the difference of two consecutive terms of a sequence ( a n ) {\displaystyle (a_{n})} .[citation needed] as a consequence the partial sums only consists of two terms of ( a n ) {\displaystyle (a_{n})} after cancellation. the cancellation technique, with part of each term cancelling with part of the next term, is known as the method of differences. for example, the series (the series of reciprocals of pronic numbers) simplifies as an early statement of the formula for the sum or partial sums of a telescoping series can be found in a      work by evangelista torricelli, de dimensione parabolae. telescoping sums are finite sums in which pairs of consecutive terms cancel each other, leaving only the initial and final terms. let a n {\displaystyle a_{n}} be a sequence of numbers. then, if a n →   {\displaystyle a_{n}\rightarrow  } telescoping products are finite products in which consecutive terms cancel denominator with numerator, leaving only the initial and final terms. let a n {\displaystyle a_{n}} be a sequence of numbers. then, "
434,434,Tetrahedron,1,https://en.wikipedia.org/wiki/Tetrahedron," in geometry, a tetrahedron (plural: tetrahedra or tetrahedrons), also known as a triangular pyramid, is a polyhedron composed of four triangular faces, six straight edges, and four vertex corners. the tetrahedron is the simplest of all the ordinary convex polyhedra and the only one that has fewer than   faces. the tetrahedron is the three-dimensional case of the more general concept of a euclidean simplex, and may thus also be called a  -simplex. the tetrahedron is one kind of pyramid, which is a polyhedron with a flat polygon base and triangular faces connecting the base to a common point. in the case of a tetrahedron the base is a triangle (any of the four faces can be considered the base), so a tetrahedron is also known as a ""triangular pyramid"". like all convex polyhedra, a tetrahedron can be folded from a single sheet of paper. it has two such nets. for any tetrahedron there exists a sphere (called the circumsphere) on which all four vertices lie, and another sphere (the insphere) tangent to the tetrahedron's faces. a regular tetrahedron is a tetrahedron in which all four faces are equilateral triangles. it is one of the five regular platonic solids, which have been known since antiquity. in a regular tetrahedron, all faces are the same size and shape (congruent) and all edges are the same length. regular tetrahedra alone do not tessellate (fill space), but if alternated with regular octahedra in the ratio of two tetrahedra to one octahedron, they form the alternated cubic honeycomb, which is a tessellation. some tetrahedra that are not regular, including the schläfli orthoscheme and the hill tetrahedron, can tessellate. the regular tetrahedron is self-dual, which means that its dual is another regular tetrahedron. the compound figure comprising two such dual tetrahedra form a stellated octahedron or stella octangula. "
435,435,Matrices as a rectangular array of real numbers,1,https://en.wikipedia.org/wiki/The_Matrix," the matrix is a      science fiction action film written and directed by the wachowskis.[a] it is the first installment in the matrix film series, starring keanu reeves, laurence fishburne, carrie-anne moss, hugo weaving, and joe pantoliano. it depicts a dystopian future in which humanity is unknowingly trapped inside a simulated reality, the matrix, which intelligent machines have created to distract humans while using their bodies as an energy source. when computer programmer thomas anderson, under the hacker alias ""neo"", uncovers the truth, he joins a rebellion against the machines along with other people who have been freed from the matrix. the matrix is an example of the cyberpunk subgenre of science fiction. the wachowskis' approach to action scenes was influenced by japanese animation and martial arts films, and the film's use of fight choreographers and wire fu techniques from hong kong action cinema influenced subsequent hollywood action film productions. the film popularized a visual effect known as ""bullet time"", in which the heightened perception of certain characters is represented by allowing the action within a shot to progress in slow-motion while the camera appears to move through the scene at normal speed, allowing the sped-up movements of certain characters to be perceived normally. the matrix opened in theaters in the united states on march   ,      to widespread acclaim from critics, who praised its innovative visual effects, action sequences, cinematography and entertainment value, and was a massive success at the box office, grossing over $    million on a $   million budget. at the   nd academy awards, the film won all four categories it was nominated for, best visual effects, best film editing, best sound, and best sound editing. the film was also the recipient of numerous other accolades, including best sound and best special visual effects at the   rd british academy film awards, and the wachowskis were awarded best director and best science fiction film at the   th saturn awards. the film is considered to be among the greatest science fiction films of all time, and in     , the film was selected for preservation in the national film registry by the library of congress for being ""culturally, historically, and aesthetically significant."" the film's success led to two feature film sequels being released in     , the matrix reloaded and the matrix revolutions, which were also written and directed by the wachowskis. the matrix franchise was further expanded through the production of comic books, video games and animated short films, with which the wachowskis were heavily involved. the franchise has also inspired books and theories expanding on some of the religious and philosophical ideas alluded to in the films. a fourth film, titled the matrix resurrections, was released on december   ,     . at an abandoned hotel, a police squad corners trinity, who overpowers them with superhuman abilities. she flees, pursued by the police and a group of suited agents capable of similar superhuman feats. she answers a ringing public telephone and vanishes. computer programmer thomas anderson, known by his hacking alias ""neo"", is puzzled by repeated online encounters with the phrase ""the matrix"". trinity contacts him and tells him a man named morpheus has the answers neo seeks. a team of agents and police, led by agent smith, arrives at neo's workplace in search of him. though morpheus attempts to guide neo to safety, neo surrenders rather than risk a dangerous escape. the agents attempt to coerce neo into helping them locate morpheus, who they claim is a terrorist. when neo refuses, the agents fuse his mouth shut and implant a robotic ""bug"" in his stomach. neo wakes up from what he believes to be a nightmare. soon after, neo is taken by trinity to meet morpheus, and she removes the bug from neo, indicating that the ""nightmare"" he experienced was apparently real. morpheus offers neo a choice between two pills: red to reveal the truth about the matrix, and blue to forget everything and return to his former life. as neo takes the red pill, his reality begins to distort, and he soon awakens in a liquid-filled pod among countless other pods, containing other humans. he is then brought aboard morpheus's flying ship, the nebuchadnezzar. as neo recuperates from a lifetime of physical inactivity in the pod in the aftermath of being redpilled, morpheus explains the situation: in the early   st century, a war broke out between humanity and intelligent machines. after humans blocked the machines' access to solar energy, the machines responded by capturing humans and harvesting their bioelectric power, while keeping their minds pacified in the matrix, a shared simulated reality modeled on the world as it was in     . the machines won the war, and the remaining free humans took refuge in the underground city of zion. morpheus and his crew are a group of rebels who hack into the matrix to ""unplug"" enslaved humans and recruit them; their understanding of the matrix's simulated nature allows them to bend its physical laws. morpheus warns neo that death within the matrix kills the physical body, too, and explains that the agents are sentient programs that eliminate threats to the system, while machines called sentinels eliminate rebels in the real world. neo's prowess during virtual training cements morpheus's belief that neo is ""the one,"" a human prophesied to free humankind. the group enters the matrix to visit the oracle, the prophet who predicted that the one would emerge. she implies to neo that he is not the one and warns that he will have to choose between morpheus's life and his own. before they can leave the matrix, agents and police ambush the group, tipped off by cypher, a disgruntled crew member who has betrayed morpheus in exchange for a comfortable life in the matrix. in an attempt to buy time for the others, morpheus fights smith and is captured. cypher exits the matrix and murders the other crew members as they lie unconscious. before cypher can kill neo and trinity, crew member tank regains consciousness and kills him before pulling neo and trinity from the matrix. "
436,436,Three dimensional space,2,https://en.wikipedia.org/wiki/Three-dimensional_space,"three-dimensional space (also:  d space,  -space or, rarely, tri-dimensional space) is a geometric setting in which three values (called parameters) are required to determine the position of an element (i.e., point). this is the informal meaning of the term dimension. in mathematics, a tuple of n numbers can be understood as the cartesian coordinates of a location in a n-dimensional euclidean space. the set of these n-tuples is commonly denoted r n , {\displaystyle \mathbb {r} ^{n},} and can be identified to the n-dimensional euclidean space. when n =  , this space is called three-dimensional euclidean space (or simply euclidean space when the context is clear). it serves as a model of the physical universe (when relativity theory is not considered), in which all known matter exists. while this space remains the most compelling and useful way to model the world as it is experienced, it is only one example of a large variety of spaces in three dimensions called  -manifolds. in this classical example, when the three values refer to measurements in different directions (coordinates), any three directions can be chosen, provided that vectors in these directions do not all lie in the same  -space (plane). furthermore, in this case, these three values can be labeled by any combination of three chosen from the terms width/breadth, height/depth, and length. in mathematics, analytic geometry (also called cartesian geometry) describes every point in three-dimensional space by means of three coordinates. three coordinate axes are given, each perpendicular to the other two at the origin, the point at which they cross. they are usually labeled x, y, and z. relative to these axes, the position of any point in three-dimensional space is given by an ordered triple of real numbers, each number giving the distance of that point from the origin measured along the given axis, which is equal to the distance of that point from the plane determined by the other two axes. other popular methods of describing the location of a point in three-dimensional space include cylindrical coordinates and spherical coordinates, though there are an infinite number of possible methods. for more, see euclidean space. below are images of the above-mentioned systems. cartesian coordinate system cylindrical coordinate system spherical coordinate system two distinct points always determine a (straight) line. three distinct points are either collinear or determine a unique plane. on the other hand, four distinct points can either be collinear, coplanar, or determine the entire space. two distinct lines can either intersect, be parallel or be skew. two parallel lines, or two intersecting lines, lie in a unique plane, so skew lines are lines that do not meet and do not lie in a common plane. "
437,437,Transformation Function,0,https://en.wikipedia.org/wiki/Transformation_(function,
438,438,Transformation in Functions,3,https://en.wikipedia.org/wiki/Transformation_(function),"in mathematics, a transformation is a function f, usually with some geometrical underpinning, that maps a set x to itself, i.e. f : x → x. examples include linear transformations of vector spaces and geometric transformations, which include projective transformations, affine transformations, and specific affine transformations, such as rotations, reflections and translations. while it is common to use the term transformation for any function of a set into itself (especially in terms like ""transformation semigroup"" and similar), there exists an alternative form of terminological convention in which the term ""transformation"" is reserved only for bijections. when such a narrow notion of transformation is generalized to partial functions, then a partial transformation is a function f: a → b, where both a and b are subsets of some set x. the set of all transformations on a given base set, together with function composition, forms a regular semigroup. for a finite set of cardinality n, there are nn transformations and (n+ )n partial transformations. "
439,439,shift of origin,0,https://en.wikipedia.org/wiki/Translation_of_axes,"in mathematics, a translation of axes in two dimensions is a mapping from an xy-cartesian coordinate system to an x'y'-cartesian coordinate system in which the x' axis is parallel to the x axis and k units away, and the y' axis is parallel to the y axis and h units away. this means that the origin o' of the new coordinate system has coordinates (h, k) in the original system. the positive x' and y' directions are taken to be the same as the positive x and y directions. a point p has coordinates (x, y) with respect to the original system and coordinates (x', y') with respect to the new system, where ( )or equivalently "
440,440,Transpose of a Matrix,1,https://en.wikipedia.org/wiki/Transpose,"in linear algebra, the transpose of a matrix is an operator which flips a matrix over its diagonal; that is, it switches the row and column indices of the matrix a by producing another matrix, often denoted by at (among other notations). the transpose of a matrix was introduced in      by the british mathematician arthur cayley. in the case of a logical matrix representing a binary relation r, the transpose corresponds to the converse relation rt. the transpose of a matrix a, denoted by at, ⊤a, a⊤, a ⊺ {\displaystyle a^{\intercal }} , a′, atr, ta or at, may be constructed by any one of the following methods: formally, the i-th row, j-th column element of at is the j-th row, i-th column element of a: if a is an m × n matrix, then at is an n × m matrix. in the case of square matrices, at may also denote the tth power of the matrix a. for avoiding a possible confusion, many authors use left upperscripts, that is, they denote the transpose as ta. an advantage of this notation is that no parentheses are needed when exponents are involved: as (ta)n = t(an), notation tan is not ambiguous. in this article this confusion is avoided by never using the symbol t as a variable name. a square matrix whose transpose is equal to itself is called a symmetric matrix; that is, a is symmetric if a square matrix whose transpose is equal to its negative is called a skew-symmetric matrix; that is, a is skew-symmetric if a square complex matrix whose transpose is equal to the matrix with every entry replaced by its complex conjugate (denoted here with an overline) is called a hermitian matrix (equivalent to the matrix being equal to its conjugate transpose); that is, a is hermitian if "
441,441,angle between two lines,3,https://en.wikipedia.org/wiki/Transversal_(geometry),"right interior adjacent vertical complementary supplementary in geometry, a transversal is a line that passes through two lines in the same plane at two distinct points. transversals play a role in establishing whether two or more other lines in the euclidean plane are parallel. the intersections of a transversal with two lines create various types of pairs of angles: consecutive interior angles, consecutive exterior angles, corresponding angles, and alternate angles. as a consequence of euclid's parallel postulate, if the two lines are parallel, consecutive interior angles are supplementary, corresponding angles are equal, and alternate angles are equal. are always congruent.) a transversal produces   angles, as shown in the graph at the above left: a transversal that cuts two parallel lines at right angles is called a perpendicular transversal. in this case, all   angles are right angles when the lines are parallel, a case that is often considered, a transversal produces several congruent and several supplementary angles. some of these angle pairs have specific names and are discussed below: corresponding angles, alternate angles, and consecutive angles. : art.    alternate angles are the four pairs of angles that: if the two angles of one pair are congruent (equal in measure), then the angles of each of the other pairs are also congruent. proposition  .   of euclid's elements, a theorem of absolute geometry (hence valid in both hyperbolic and euclidean geometry), proves that if the angles of a pair of alternate angles of a transversal are congruent then the two lines are parallel (non-intersecting). "
442,442,Relations between sides and angles of a triangle,0,https://en.wikipedia.org/wiki/Triangle," a triangle is a polygon with three edges and three vertices. it is one of the basic shapes in geometry. a triangle with vertices a, b, and c is denoted △ a b c {\displaystyle \triangle abc} . in euclidean geometry, any three points, when non-collinear, determine a unique triangle and simultaneously, a unique plane (i.e. a two-dimensional euclidean space). in other words, there is only one plane that contains that triangle, and every triangle is contained in some plane. if the entire geometry is only the euclidean plane, there is only one plane and all triangles are contained in it; however, in higher-dimensional euclidean spaces, this is no longer true. this article is about triangles in euclidean geometry, and in particular, the euclidean plane, except where otherwise noted. the terminology for categorizing triangles is more than two thousand years old, having been defined on the very first page of euclid's elements. the names used for modern classification are either a direct transliteration of euclid's greek or their latin translations. ancient greek mathematician euclid defined three types of triangle according to the lengths of their sides: greek: τῶν δὲ τριπλεύρων σχημάτων ἰσόπλευρον μὲν τρίγωνόν ἐστι τὸ τὰς τρεῖς ἴσας ἔχον πλευράς, ἰσοσκελὲς δὲ τὸ τὰς δύο μόνας ἴσας ἔχον πλευράς, σκαληνὸν δὲ τὸ τὰς τρεῖς ἀνίσους ἔχον πλευράς, lit. 'of trilateral figures, an isopleuron [equilateral] triangle is that which has its three sides equal, an isosceles that which has two of its sides alone equal, and a scalene that which has its three sides unequal.' equilateral triangle isosceles triangle scalene triangle hatch marks, also called tick marks, are used in diagrams of triangles and other geometric figures to identify sides of equal lengths. a side can be marked with a pattern of ""ticks"", short line segments in the form of tally marks; two sides have equal lengths if they are both marked with the same pattern. in a triangle, the pattern is usually no more than   ticks. an equilateral triangle has the same pattern on all   sides, an isosceles triangle has the same pattern on just   sides, and a scalene triangle has different patterns on all sides since no sides are equal. "
443,443,Triangle centre,0,https://en.wikipedia.org/wiki/Triangle_center," in geometry, a triangle center (or triangle centre) is a point in the plane that is in some sense a center of a triangle akin to the centers of squares and circles, that is, a point that is in the middle of the figure by some measure. for example, the centroid, circumcenter, incenter and orthocenter were familiar to the ancient greeks, and can be obtained by simple constructions. each of these classical centers has the property that it is invariant (more precisely equivariant) under similarity transformations. in other words, for any triangle and any similarity transformation (such as a rotation, reflection, dilation, or translation), the center of the transformed triangle is the same point as the transformed center of the original triangle. this invariance is the defining property of a triangle center. it rules out other well-known points such as the brocard points which are not invariant under reflection and so fail to qualify as triangle centers. for an equilateral triangle, all triangle centers coincide at its centroid. however the triangle centers generally take different positions from each other on all other triangles. the definitions and properties of thousands of triangle centers have been collected in the encyclopedia of triangle centers. even though the ancient greeks discovered the classic centers of a triangle they had not formulated any definition of a triangle center. after the ancient greeks, several special points associated with a triangle like the fermat point, nine-point center, lemoine point, gergonne point, and feuerbach point were discovered. during the revival of interest in triangle geometry in the     s it was noticed that these special points share some general properties that now form the basis for a formal definition of triangle center. as of    july     [update], clark kimberling's encyclopedia of triangle centers contains an annotated list of   ,    triangle centers. a real-valued function f of three real variables a, b, c may have the following properties: if a non-zero f has both these properties it is called a triangle center function. if f is a triangle center function and a, b, c are the side-lengths of a reference triangle then the point whose trilinear coordinates are f(a,b,c) : f(b,c,a) : f(c,a,b) is called a triangle center. this definition ensures that triangle centers of similar triangles meet the invariance criteria specified above. by convention only the first of the three trilinear coordinates of a triangle center is quoted since the other two are obtained by cyclic permutation of a, b, c. this process is known as cyclicity. every triangle center function corresponds to a unique triangle center. this correspondence is not bijective. different functions may define the same triangle center. for example, the functions f (a,b,c) =  /a and f (a,b,c) = bc both correspond to the centroid. two triangle center functions define the same triangle center if and only if their ratio is a function symmetric in a, b and c. even if a triangle center function is well-defined everywhere the same cannot always be said for its associated triangle center. for example, let f(a, b, c) be   if a/b and a/c are both rational and   otherwise. then for any triangle with integer sides the associated triangle center evaluates to  : :  which is undefined. in some cases these functions are not defined on the whole of ℝ . for example, the trilinears of x    are a /  : b /  : c /  so a, b, c cannot be negative. furthermore, in order to represent the sides of a triangle they must satisfy the triangle inequality. so, in practice, every function's domain is restricted to the region of ℝ  where a ≤ b + c, b ≤ c + a, and c ≤ a + b. this region t is the domain of all triangles, and it is the default domain for all triangle-based functions. "
444,444,Triangle inequality,1,https://en.wikipedia.org/wiki/Triangle_inequality,"in mathematics, the triangle inequality states that for any triangle, the sum of the lengths of any two sides must be greater than or equal to the length of the remaining side. this statement permits the inclusion of degenerate triangles, but some authors, especially those writing about elementary geometry, will exclude this possibility, thus leaving out the possibility of equality. if x, y, and z are the lengths of the sides of the triangle, with no side being greater than z, then the triangle inequality states that with equality only in the degenerate case of a triangle with zero area. in euclidean geometry and some other geometries, the triangle inequality is a theorem about distances, and it is written using vectors and vector lengths (norms): where the length z of the third side has been replaced by the vector sum x + y. when x and y are real numbers, they can be viewed as vectors in r , and the triangle inequality expresses a relationship between absolute values. in euclidean geometry, for right triangles the triangle inequality is a consequence of the pythagorean theorem, and for general triangles, a consequence of the law of cosines, although it may be proven without these theorems. the inequality can be viewed intuitively in either r  or r . the figure at the right shows three examples beginning with clear inequality (top) and approaching equality (bottom). in the euclidean case, equality occurs only if the triangle has a    ° angle and two  ° angles, making the three vertices collinear, as shown in the bottom example. thus, in euclidean geometry, the shortest distance between two points is a straight line. in spherical geometry, the shortest distance between two points is an arc of a great circle, but the triangle inequality holds provided the restriction is made that the distance between two points on a sphere is the length of a minor spherical line segment (that is, one with central angle in [ , π]) with those endpoints. the triangle inequality is a defining property of norms and measures of distance. this property must be established as a theorem for any function proposed for such purposes for each particular space: for example, spaces such as the real numbers, euclidean spaces, the lp spaces (p ≥  ), and inner product spaces. euclid proved the triangle inequality for distances in plane geometry using the construction in the figure. beginning with triangle abc, an isosceles triangle is constructed with one side taken as bc and the other equal leg bd along the extension of side ab. it then is argued that angle β has larger measure than angle α, so side ad is longer than side ac. but ad = ab + bd = ab + bc, so the sum of the lengths of sides ab and bc is larger than the length of ac. this proof appears in euclid's elements, book  , proposition   . for a proper triangle, the triangle inequality, as stated in words, literally translates into three inequalities (given that a proper triangle has side lengths a, b, c that are all positive and excludes the degenerate case of zero area): a more succinct form of this inequality system can be shown to be another way to state it is "
445,445,Triangular Distribution,1,https://en.wikipedia.org/wiki/Triangular_distribution,"in probability theory and statistics, the triangular distribution is a continuous probability distribution with lower limit a, upper limit b and mode c, where a < b and a ≤ c ≤ b. the distribution simplifies when c = a or c = b. for example, if a =  , b =   and c =  , then the pdf and cdf become: this distribution for a =  , b =   and c =   is the distribution of x = |x  − x |, where x , x  are two independent random variables with standard uniform distribution. the symmetric case arises when c = (a + b) /  . in this case, an alternate form of the distribution function is: this distribution for a =  , b =   and c =  . —the mode (i.e., the peak) is exactly in the middle of the interval—corresponds to the distribution of the mean of two standard uniform variables, i.e., the distribution of x = (x  + x ) /  , where x , x  are two independent random variables with standard uniform distribution in [ ,  ]. given a random variate u drawn from the uniform distribution in the interval ( ,  ), then the variate where f ( c ) = ( c − a ) / ( b − a ) {\displaystyle f(c)=(c-a)/(b-a)} , has a triangular distribution with parameters a , b {\displaystyle a,b} and c {\displaystyle c} . this can be obtained from the cumulative distribution function. the triangular distribution is typically used as a subjective description of a population for which there is only limited sample data, and especially in cases where the relationship between variables is known but data is scarce (possibly because of the high cost of collection). it is based on a knowledge of the minimum and maximum and an ""inspired guess"" as to the modal value. for these reasons, the triangle distribution has been called a ""lack of knowledge"" distribution. the triangular distribution is therefore often used in business decision making, particularly in simulations. generally, when not much is known about the distribution of an outcome (say, only its smallest and largest values), it is possible to use the uniform distribution. but if the most likely outcome is also known, then the outcome can be simulated by a triangular distribution. see for example under corporate finance. the triangular distribution, along with the pert distribution, is also widely used in project management (as an input into pert and hence critical path method (cpm)) to model events which take place within an interval defined by a minimum and maximum value. "
446,446,Right Angled Definition of Trignometric functions,1,https://en.wikipedia.org/wiki/Trigonometric_functions," in mathematics, the trigonometric functions (also called circular functions, angle functions or goniometric functions ) are real functions which relate an angle of a right-angled triangle to ratios of two side lengths. they are widely used in all sciences that are related to geometry, such as navigation, solid mechanics, celestial mechanics, geodesy, and many others. they are among the simplest periodic functions, and as such are also widely used for studying periodic phenomena through fourier analysis. the trigonometric functions most widely used in modern mathematics are the sine, the cosine, and the tangent. their reciprocals are respectively the cosecant, the secant, and the cotangent, which are less used. each of these six trigonometric functions has a corresponding inverse function, and an analog among the hyperbolic functions. the oldest definitions of trigonometric functions, related to right-angle triangles, define them only for acute angles. to extend the sine and cosine functions to functions whose domain is the whole real line, geometrical definitions using the standard unit circle (i.e., a circle with radius   unit) are often used; then the domain of the other functions is the real line with some isolated points removed. modern definitions express trigonometric functions as infinite series or as solutions of differential equations. this allows extending the domain of sine and cosine functions to the whole complex plane, and the domain of the other trigonometric functions to the complex plane with some isolated points removed. traditionally, a three letter abbreviation of their name is used as a symbol for representing trigonometric function in formulas, namely ""sin"", ""cos"", ""tan"", ""sec"", ""csc"", and ""cot"" for sine, cosine, tangent, secant, cosecant, and cotangent, respectively. (the cosecant function may instead be abbreviated to the five-letter ""cosec"".) moreover, functional notation, such as sin(x), is used. parentheses may be omitted when no confusion may occur; an example of a case requiring brackets is sin ⁡ x + y {\displaystyle \sin x+y} : this expression is ambiguous as it can be interpreted as sin ⁡ ( x ) + y {\displaystyle \sin(x)+y} or sin ⁡ ( x + y ) . {\displaystyle \sin(x+y).} unlike the general functional notation, a positive integer appearing as a superscript after the symbol of the function does not denote a power under function composition but an iterated multiplication. for example sin   ⁡ x {\displaystyle \sin ^{ }x} and sin   ⁡ ( x ) {\displaystyle \sin ^{ }(x)} denote sin ⁡ ( x ) sin ⁡ ( x ) , {\displaystyle \sin(x)\sin(x),} not sin ⁡ ( sin ⁡ x ) . {\displaystyle \sin(\sin x).} however, the exponent −   {\displaystyle -\! } is an alternative notation for the corresponding inverse trigonometric function. that is, sin −   ⁡ x {\displaystyle \sin ^{- }x} and sin −   ⁡ ( x ) {\displaystyle \sin ^{- }(x)} denote arcsin ⁡ x , {\displaystyle \arcsin x,} the arcsine. in this context, the exponent could be considered as denoting a composed or iterated function (for which the notation sin [ −   ] ⁡ ( x ) {\displaystyle \sin ^{[- ]}(x)} can be used instead), since an inverse function can be considered to be the function iterated −   {\displaystyle -\! } times. however, the utilization of superscript values to represent levels of iteration other than -  does not appear to be in common use. if the acute angle θ is given, then any right triangles that have an angle of θ are similar to each other. this means that the ratio of any two side lengths depends only on θ. thus these six ratios define six functions of θ, which are the trigonometric functions. in the following definitions, the hypotenuse is the length of the side opposite the right angle, opposite represents the side opposite the given angle θ, and adjacent represents the side between the angle θ and the right angle. in a right-angled triangle, the sum of the two acute angles is a right angle, that is,   ° or π/  radians. therefore sin ⁡ ( θ ) {\displaystyle \sin(\theta )} and cos ⁡ (    ∘ − θ ) {\displaystyle \cos(  ^{\circ }-\theta )} represent the same ratio, and thus are equal. this identity and analogous relationships between the other trigonometric functions are summarized in the following table. in geometric applications, the argument of a trigonometric function is generally the measure of an angle. for this purpose, any angular unit is convenient, and angles are most commonly measured in conventional units of degrees in which a right angle is   ° and a complete turn is    ° (particularly in elementary mathematics). "
447,447,Trigonometry,1,https://en.wikipedia.org/wiki/Trigonometry," trigonometry (from ancient greek τρίγωνον (trígōnon) 'triangle', and μέτρον (métron) 'measure') is a branch of mathematics that studies relationships between side lengths and angles of triangles. the field emerged in the hellenistic world during the  rd century bc from applications of geometry to astronomical studies. the greeks focused on the calculation of chords, while mathematicians in india created the earliest-known tables of values for trigonometric ratios (also called trigonometric functions) such as sine. throughout history, trigonometry has been applied in areas such as geodesy, surveying, celestial mechanics, and navigation. trigonometry is known for its many identities. these trigonometric identities are commonly used for rewriting trigonometrical expressions with the aim to simplify an expression, to find a more useful form of an expression, or to solve an equation. sumerian astronomers studied angle measure, using a division of circles into     degrees. they, and later the babylonians, studied the ratios of the sides of similar triangles and discovered some properties of these ratios but did not turn that into a systematic method for finding sides and angles of triangles. the ancient nubians used a similar method. in the  rd century bc, hellenistic mathematicians such as euclid and archimedes studied the properties of chords and inscribed angles in circles, and they proved theorems that are equivalent to modern trigonometric formulae, although they presented them geometrically rather than algebraically. in     bc, hipparchus (from nicaea, asia minor) gave the first tables of chords, analogous to modern tables of sine values, and used them to solve problems in trigonometry and spherical trigonometry. in the  nd century ad, the greco-egyptian astronomer ptolemy (from alexandria, egypt) constructed detailed trigonometric tables (ptolemy's table of chords) in book  , chapter    of his almagest. ptolemy used chord length to define his trigonometric functions, a minor difference from the sine convention we use today. (the value we call sin(θ) can be found by looking up the chord length for twice the angle of interest ( θ) in ptolemy's table, and then dividing that value by two.) centuries passed before more detailed tables were produced, and ptolemy's treatise remained in use for performing trigonometric calculations in astronomy throughout the next      years in the medieval byzantine, islamic, and, later, western european worlds. the modern sine convention is first attested in the surya siddhanta, and its properties were further documented by the  th century (ad) indian mathematician and astronomer aryabhata. these greek and indian works were translated and expanded by medieval islamic mathematicians. by the   th century, islamic mathematicians were using all six trigonometric functions, had tabulated their values, and were applying them to problems in spherical geometry. the persian polymath nasir al-din al-tusi has been described as the creator of trigonometry as a mathematical discipline in its own right. nasīr al-dīn al-tūsī was the first to treat trigonometry as a mathematical discipline independent from astronomy, and he developed spherical trigonometry into its present form. he listed the six distinct cases of a right-angled triangle in spherical trigonometry, and in his on the sector figure, he stated the law of sines for plane and spherical triangles, discovered the law of tangents for spherical triangles, and provided proofs for both these laws. knowledge of trigonometric functions and methods reached western europe via latin translations of ptolemy's greek almagest as well as the works of persian and arab astronomers such as al battani and nasir al-din al-tusi. one of the earliest works on trigonometry by a northern european mathematician is de triangulis by the   th century german mathematician regiomontanus, who was encouraged to write, and provided with a copy of the almagest, by the byzantine greek scholar cardinal basilios bessarion with whom he lived for several years. at the same time, another translation of the almagest from greek into latin was completed by the cretan george of trebizond. trigonometry was still so little known in   th-century northern europe that nicolaus copernicus devoted two chapters of de revolutionibus orbium coelestium to explain its basic concepts. driven by the demands of navigation and the growing need for accurate maps of large geographic areas, trigonometry grew into a major branch of mathematics. bartholomaeus pitiscus was the first to use the word, publishing his trigonometria in     . gemma frisius described for the first time the method of triangulation still used today in surveying. it was leonhard euler who fully incorporated complex numbers into trigonometry. the works of the scottish mathematicians james gregory in the   th century and colin maclaurin in the   th century were influential in the development of trigonometric series. also in the   th century, brook taylor defined the general taylor series. trigonometric ratios are the ratios between edges of a right triangle. these ratios are given by the following trigonometric functions of the known angle a, where a, b and c refer to the lengths of the sides in the accompanying figure: the hypotenuse is the side opposite to the    degree angle in a right triangle; it is the longest side of the triangle and one of the two sides adjacent to angle a. the adjacent leg is the other side that is adjacent to angle a. the opposite side is the side that is opposite to angle a. the terms perpendicular and base are sometimes used for the opposite and adjacent sides respectively. see below under mnemonics. "
448,448,Scalar Triple Product,2,https://en.wikipedia.org/wiki/Triple_product,"in geometry and algebra, the triple product is a product of three  -dimensional vectors, usually euclidean vectors. the name ""triple product"" is used for two different products, the scalar-valued scalar triple product and, less often, the vector-valued vector triple product. the scalar triple product (also called the mixed product, box product, or triple scalar product) is defined as the dot product of one of the vectors with the cross product of the other two. geometrically, the scalar triple product is the (signed) volume of the parallelepiped defined by the three vectors given. here, the parentheses may be omitted without causing ambiguity, since the dot product cannot be evaluated first. if it were, it would leave the cross product of a scalar and a vector, which is not defined. although the scalar triple product gives the volume of the parallelepiped, it is the signed volume, the sign depending on the orientation of the frame or the parity of the permutation of the vectors. this means the product is negated if the orientation is reversed, for example by a parity transformation, and so is more properly described as a pseudoscalar if the orientation can change. this also relates to the handedness of the cross product; the cross product transforms as a pseudovector under parity transformations and so is properly described as a pseudovector. the dot product of two vectors is a scalar but the dot product of a pseudovector and a vector is a pseudoscalar, so the scalar triple product must be pseudoscalar-valued. if t is a rotation operator, then but if t is an improper rotation, then in exterior algebra and geometric algebra the exterior product of two vectors is a bivector, while the exterior product of three vectors is a trivector. a bivector is an oriented plane element and a trivector is an oriented volume element, in the same way that a vector is an oriented line element. given vectors a, b and c, the product "
449,449,truth table,0,https://en.wikipedia.org/wiki/Truth_table,"a truth table is a mathematical table used in logic—specifically in connection with boolean algebra, boolean functions, and propositional calculus—which sets out the functional values of logical expressions on each of their functional arguments, that is, for each combination of values taken by their logical variables. in particular, truth tables can be used to show whether a propositional expression is true for all legitimate input values, that is, logically valid. a truth table has one column for each input variable (for example, p and q), and one final column showing all of the possible results of the logical operation that the table represents (for example, p xor q). each row of the truth table contains one possible configuration of the input variables (for instance, p=true q=false), and the result of the operation for those values. see the examples below for further clarification. ludwig wittgenstein is generally credited with inventing and popularizing the truth table in his tractatus logico-philosophicus, which was completed in      and published in     . such a system was also independently proposed in      by emil leon post. an even earlier iteration of the truth table has also been found in unpublished manuscripts by charles sanders peirce from     , antedating both publications by nearly    years. there are   unary operations: the output value is always true, regardless of the input value of p the output value is never true: that is, always false, regardless of the input value of p logical identity is an operation on one logical value p, for which the output value remains p. the truth table for the logical identity operator is as follows: logical negation is an operation on one logical value, typically the value of a proposition, that produces a value of true if its operand is false and a value of false if its operand is true. the truth table for not p (also written as ¬p, np, fpq, or ~p) is as follows: there are    possible truth functions of two binary variables: "
450,450,Two Dimensional Space,1,https://en.wikipedia.org/wiki/Two-dimensional_space,"two-dimensional space (also known as  d space,  -space, or bi-dimensional space) is a geometric setting in which two values (called parameters) are required to determine the position of an element (i.e., point). the set r   {\displaystyle \mathbb {r} ^{ }} of pairs of real numbers with appropriate structure often serves as the canonical example of a two-dimensional euclidean space. for a generalization of the concept, see dimension. two-dimensional space can be seen as a projection of the physical universe onto a plane. usually, it is thought of as a euclidean space and the two dimensions are called length and width. books i through iv and vi of euclid's elements dealt with two-dimensional geometry, developing such notions as similarity of shapes, the pythagorean theorem (proposition   ), equality of angles and areas, parallelism, the sum of the angles in a triangle, and the three cases in which triangles are ""equal"" (have the same area), among many other topics. later, the plane was described in a so-called cartesian coordinate system, a coordinate system that specifies each point uniquely in a plane by a pair of numerical coordinates, which are the signed distances from the point to two fixed perpendicular directed lines, measured in the same unit of length. each reference line is called a coordinate axis or just axis of the system, and the point where they meet is its origin, usually at ordered pair ( ,  ). the coordinates can also be defined as the positions of the perpendicular projections of the point onto the two axes, expressed as signed distances from the origin. the idea of this system was developed in      in writings by descartes and independently by pierre de fermat, although fermat also worked in three dimensions, and did not publish the discovery. both authors used a single axis in their treatments[citation needed] and have a variable length measured in reference to this axis. the concept of using a pair of axes was introduced later, after descartes' la géométrie was translated into latin in      by frans van schooten and his students. these commentators introduced several concepts while trying to clarify the ideas contained in descartes' work. later, the plane was thought of as a field, where any two points could be multiplied and, except for  , divided. this was known as the complex plane. the complex plane is sometimes called the argand plane because it is used in argand diagrams. these are named after jean-robert argand (    –    ), although they were first described by danish-norwegian land surveyor and mathematician caspar wessel (    –    ). argand diagrams are frequently used to plot the positions of the poles and zeroes of a function in the complex plane. in mathematics, analytic geometry (also called cartesian geometry) describes every point in two-dimensional space by means of two coordinates. two perpendicular coordinate axes are given which cross each other at the origin. they are usually labeled x and y. relative to these axes, the position of any point in two-dimensional space is given by an ordered pair of real numbers, each number giving the distance of that point from the origin measured along the given axis, which is equal to the distance of that point from the other axis. another widely used coordinate system is the polar coordinate system, which specifies a point in terms of its distance from the origin and its angle relative to a rightward reference ray. cartesian coordinate system polar coordinate system "
451,451,Sets unions,1,https://en.wikipedia.org/wiki/Union_(set_theory),"in set theory, the union (denoted by ∪) of a collection of sets is the set of all elements in the collection. it is one of the fundamental operations through which sets can be combined and related to each other. a nullary union refers to a union of zero (   {\displaystyle  } ) sets and it is by definition equal to the empty set. for explanation of the symbols used in this article, refer to the table of mathematical symbols. the union of two sets a and b is the set of elements which are in a, in b, or in both a and b. in symbols, for example, if a = { ,  ,  ,  } and b = { ,  ,  ,  ,  } then a ∪ b = { ,  ,  ,  ,  ,  ,  }. a more elaborate example (involving two infinite sets) is: as another example, the number   is not contained in the union of the set of prime numbers { ,  ,  ,  ,   , ...} and the set of even numbers { ,  ,  ,  ,   , ...}, because   is neither prime nor even. sets cannot have duplicate elements, so the union of the sets { ,  ,  } and { ,  ,  } is { ,  ,  ,  }. multiple occurrences of identical elements have no effect on the cardinality of a set or its contents. binary union is an associative operation; that is, for any sets a , b , and c , {\displaystyle a,b,{\text{ and }}c,} thus the parentheses may be omitted without ambiguity: either of the above can be written as a ∪ b ∪ c . {\displaystyle a\cup b\cup c.} also, union is commutative, so the sets can be written in any order. the empty set is an identity element for the operation of union. that is, a ∪ ∅ = a , {\displaystyle a\cup \varnothing =a,} for any set a . {\displaystyle a.} also, the union operation is idempotent: a ∪ a = a . {\displaystyle a\cup a=a.} all these properties follow from analogous facts about logical disjunction. intersection distributes over union the power set of a set u , {\displaystyle u,} together with the operations given by union, intersection, and complementation, is a boolean algebra. in this boolean algebra, union can be expressed in terms of intersection and complementation by the formula "
452,452,Unitary method,1,https://en.wikipedia.org/wiki/Unitary_method,"the unitary method is a technique for solving a problem by first finding the value of a single unit, and then finding the necessary value by multiplying the single unit value. for example, to solve the problem: ""a man walks   miles in   hours. how far does he walk in   hours?"", one would first calculate how far the man walks in   hour. one can safely assume that he would walk half the distance in half the time. therefore, dividing by  , the man walks  .  miles in   hour. multiplying by   for   hours, the man walks  × . =  .  miles, or consider the distance traveled by the man be x, then divide it given distance that is   (x/ ). it is equal to the time taken to travel x distance that is   hours divided by the time taken to travel   miles, that is   hours ( / ), therefore x/ = / , hence x=  .  miles. the same method can be applied to the problem: ""a man walks at   miles per hour. how long would it take him to cover   miles?"". dividing by   shows that the man covers   mile in a quarter ( .  ) of an hour. multiplying by   shows that the man, therefore, takes   hour and a quarter ( .   hours) to cover   miles. similarly, by the second method, we can find the value of time taken to cover   miles. the first method is preferable and easier. "
453,453,variance,2,https://en.wikipedia.org/wiki/Variance,"in probability theory and statistics, variance is the expectation of the squared deviation of a random variable from its population mean or sample mean. variance is a measure of dispersion, meaning it is a measure of how far a set of numbers is spread out from their average value. variance has a central role in statistics, where some ideas that use it include descriptive statistics, statistical inference, hypothesis testing, goodness of fit, and monte carlo sampling. variance is an important tool in the sciences, where statistical analysis of data is common. the variance is the square of the standard deviation, the second central moment of a distribution, and the covariance of the random variable with itself, and it is often represented by σ   {\displaystyle \sigma ^{ }} , s   {\displaystyle s^{ }} , var ⁡ ( x ) {\displaystyle \operatorname {var} (x)} , v ( x ) {\displaystyle v(x)} , or v ( x ) {\displaystyle \mathbb {v} (x)} . an advantage of variance as a measure of dispersion is that it is more amenable to algebraic manipulation than other measures of dispersion such as the expected absolute deviation; for example, the variance of a sum of uncorrelated random variables is equal to the sum of their variances. a disadvantage of the variance for practical applications is that, unlike the standard deviation, its units differ from the random variable, which is why the standard deviation is more commonly reported as a measure of dispersion once the calculation is finished. there are two distinct concepts that are both called ""variance"". one, as discussed above, is part of a theoretical probability distribution and is defined by an equation. the other variance is a characteristic of a set of observations. when variance is calculated from observations, those observations are typically measured from a real world system. if all possible observations of the system are present then the calculated variance is called the population variance. normally, however, only a subset is available, and the variance calculated from this is called the sample variance. the variance calculated from a sample is considered an estimate of the full population variance. there are multiple ways to calculate an estimate of the population variance, as discussed in the section below. the two kinds of variance are closely related. to see how, consider that a theoretical probability distribution can be used as a generator of hypothetical observations. if an infinite number of observations are generated using a distribution, then the sample variance calculated from that infinite set will match the value calculated using the distribution's equation for variance. the variance of a random variable x {\displaystyle x} is the expected value of the squared deviation from the mean of x {\displaystyle x} , μ = e ⁡ [ x ] {\displaystyle \mu =\operatorname {e} [x]} : this definition encompasses random variables that are generated by processes that are discrete, continuous, neither, or mixed. the variance can also be thought of as the covariance of a random variable with itself: the variance is also equivalent to the second cumulant of a probability distribution that generates x {\displaystyle x} . the variance is typically designated as var ⁡ ( x ) {\displaystyle \operatorname {var} (x)} , or sometimes as v ( x ) {\displaystyle v(x)} or v ( x ) {\displaystyle \mathbb {v} (x)} , or symbolically as σ x   {\displaystyle \sigma _{x}^{ }} or simply σ   {\displaystyle \sigma ^{ }} (pronounced ""sigma squared""). the expression for the variance can be expanded as follows: in other words, the variance of x is equal to the mean of the square of x minus the square of the mean of x. this equation should not be used for computations using floating point arithmetic, because it suffers from catastrophic cancellation if the two components of the equation are similar in magnitude. for other numerically stable alternatives, see algorithms for calculating variance. if the generator of random variable x {\displaystyle x} is discrete with probability mass function x   ↦ p   , x   ↦ p   , … , x n ↦ p n {\displaystyle x_{ }\mapsto p_{ },x_{ }\mapsto p_{ },\ldots ,x_{n}\mapsto p_{n}} , then where μ {\displaystyle \mu } is the expected value. that is, "
454,454,Vectors in maths,1,https://en.wikipedia.org/wiki/Vector_(mathematics_and_physics),"in mathematics and physics, a vector is an element of a vector space. for many specific vector spaces, the vectors have received specific names, which are listed below. in general, a euclidean vector is a geometric object with both length and direction, which is frequently represented as an arrow whose starting point is arbitrary and thus chosen for convenience. such vectors can be added to each other or scaled using vector algebra. correspondingly, an ensemble of vectors is called a vector space. these objects are the subject of linear algebra and can be characterized by their dimension. historically, vectors were introduced in geometry and physics (typically in mechanics) before the formalization of the concept of a vector space. (in fact, the latin word vector means ""carrier"".) therefore, one often talks about vectors without specifying the vector space to which they belong. specifically, in a euclidean space, one considers spatial vectors, also called euclidean vectors which are used to represent quantities that have both magnitude and direction, and may be added, subtracted and scaled (i.e. multiplied by a real number) for forming a vector space. in mathematics, physics and engineering, a euclidean vector or simply a vector (sometimes called a geometric vector or spatial vector ) is a geometric object that has magnitude (or length) and direction. vectors can be added to other vectors according to vector algebra. a euclidean vector is frequently represented by a directed line segment, or graphically as an arrow connecting an initial point a with a terminal point b, and denoted by a b → {\displaystyle {\overrightarrow {ab}}} . a vector is what is needed to ""carry"" the point a to the point b; the latin word vector means ""carrier"". it was first used by   th century astronomers investigating planetary revolution around the sun. the magnitude of the vector is the distance between the two points, and the direction refers to the direction of displacement from a to b. many algebraic operations on real numbers such as addition, subtraction, multiplication, and negation have close analogues for vectors, operations which obey the familiar algebraic laws of commutativity, associativity, and distributivity. these operations and associated laws qualify euclidean vectors as an example of the more generalized concept of vectors defined simply as elements of a vector space. in mathematics, physics, and engineering, a vector space (also called a linear space) is a set of objects called vectors, which may be added together and multiplied (""scaled"") by numbers called scalars. scalars are often real numbers, but some vector spaces have scalar multiplication by complex numbers or, generally, by a scalar from any mathematic field. the operations of vector addition and scalar multiplication must satisfy certain requirements, called vector axioms (listed below in § notation and definition). to specify whether the scalars in a particular vector space are real numbers or complex numbers, the terms real vector space and complex vector space are often used. certain sets of euclidean vectors are common examples of a vector space. they represent physical quantities such as forces, where any two forces of the same type can be added to yield a third, and the multiplication of a force vector by a real multiplier is another force vector. in the same way (but in a more geometric sense), vectors representing displacements in the plane or three-dimensional space also form vector spaces. vectors in vector spaces do not necessarily have to be arrow-like objects as they appear in the mentioned examples: vectors are regarded as abstract mathematical objects with particular properties, which in some cases can be visualized as arrows. vector spaces are the subject of linear algebra and are well characterized by their dimension, which, roughly speaking, specifies the number of independent directions in the space. infinite-dimensional vector spaces arise naturally in mathematical analysis as function spaces, whose vectors are functions. these vector spaces are generally endowed with some additional structure such as a topology, which allows the consideration of issues of proximity and continuity. among these topologies, those that are defined by a norm or inner product are more commonly used (being equipped with a notion of distance between two vectors). this is particularly the case of banach spaces and hilbert spaces, which are fundamental in mathematical analysis. historically, the first ideas leading to vector spaces can be traced back as far as the   th century's analytic geometry, matrices, systems of linear equations, and euclidean vectors. the modern, more abstract treatment, first formulated by giuseppe peano in     , encompasses more general objects than euclidean space, but much of the theory can be seen as an extension of classical geometric ideas like lines, planes and their higher-dimensional analogs. today, vector spaces are applied throughout mathematics, science and engineering. they are the appropriate linear-algebraic notion to deal with systems of linear equations. they offer a framework for fourier expansion, which is employed in image compression routines, and they provide an environment that can be used for solution techniques for partial differential equations. furthermore, vector spaces furnish an abstract, coordinate-free way of dealing with geometrical and physical objects such as tensors. this in turn allows the examination of local properties of manifolds by linearization techniques. vector spaces may be generalized in several ways, leading to more advanced notions in geometry and abstract algebra. every algebra over a field is a vector space, but elements of an algebra are generally not called vectors. however, in some cases, they are called vectors, mainly due to historical reasons. "
455,455,Vector algebra,2,https://en.wikipedia.org/wiki/Vector_algebra,"in mathematics, vector algebra may mean: "
456,456,Vector calculus,2,https://en.wikipedia.org/wiki/Vector_calculus,"vector calculus, or vector analysis, is concerned with differentiation and integration of vector fields, primarily in  -dimensional euclidean space r   . {\displaystyle \mathbb {r} ^{ }.} the term ""vector calculus"" is sometimes used as a synonym for the broader subject of multivariable calculus, which spans vector calculus as well as partial differentiation and multiple integration. vector calculus plays an important role in differential geometry and in the study of partial differential equations. it is used extensively in physics and engineering, especially in the description of electromagnetic fields, gravitational fields, and fluid flow. vector calculus was developed from quaternion analysis by j. willard gibbs and oliver heaviside near the end of the   th century, and most of the notation and terminology was established by gibbs and edwin bidwell wilson in their      book, vector analysis. in the conventional form using cross products, vector calculus does not generalize to higher dimensions, while the alternative approach of geometric algebra which uses exterior products does (see § generalizations below for more). a scalar field associates a scalar value to every point in a space. the scalar is a mathematical number representing a physical quantity. examples of scalar fields in applications include the temperature distribution throughout space, the pressure distribution in a fluid, and spin-zero quantum fields (known as scalar bosons), such as the higgs field. these fields are the subject of scalar field theory. a vector field is an assignment of a vector to each point in a space. a vector field in the plane, for instance, can be visualized as a collection of arrows with a given magnitude and direction each attached to a point in the plane. vector fields are often used to model, for example, the speed and direction of a moving fluid throughout space, or the strength and direction of some force, such as the magnetic or gravitational force, as it changes from point to point. this can be used, for example, to calculate work done over a line. in more advanced treatments, one further distinguishes pseudovector fields and pseudoscalar fields, which are identical to vector fields and scalar fields, except that they change sign under an orientation-reversing map: for example, the curl of a vector field is a pseudovector field, and if one reflects a vector field, the curl points in the opposite direction. this distinction is clarified and elaborated in geometric algebra, as described below. the algebraic (non-differential) operations in vector calculus are referred to as vector algebra, being defined for a vector space and then globally applied to a vector field. the basic algebraic operations consist of: also commonly used are the two triple products: vector calculus studies various differential operators defined on scalar or vector fields, which are typically expressed in terms of the del operator ( ∇ {\displaystyle \nabla } ), also known as ""nabla"". the three basic vector operators are: also commonly used are the two laplace operators: a quantity called the jacobian matrix is useful for studying functions when both the domain and range of the function are multivariable, such as a change of variables during integration. "
457,457,Vector Space,1,https://en.wikipedia.org/wiki/Vector_space,"in mathematics, physics, and engineering, a vector space (also called a linear space) is a set of objects called vectors, which may be added together and multiplied (""scaled"") by numbers called scalars. scalars are often real numbers, but some vector spaces have scalar multiplication by complex numbers or, generally, by a scalar from any mathematic field. the operations of vector addition and scalar multiplication must satisfy certain requirements, called vector axioms (listed below in § notation and definition). to specify whether the scalars in a particular vector space are real numbers or complex numbers, the terms real vector space and complex vector space are often used. certain sets of euclidean vectors are common examples of a vector space. they represent physical quantities such as forces, where any two forces of the same type can be added to yield a third, and the multiplication of a force vector by a real multiplier is another force vector. in the same way (but in a more geometric sense), vectors representing displacements in the plane or three-dimensional space also form vector spaces. vectors in vector spaces do not necessarily have to be arrow-like objects as they appear in the mentioned examples: vectors are regarded as abstract mathematical objects with particular properties, which in some cases can be visualized as arrows. vector spaces are the subject of linear algebra and are well characterized by their dimension, which, roughly speaking, specifies the number of independent directions in the space. infinite-dimensional vector spaces arise naturally in mathematical analysis as function spaces, whose vectors are functions. these vector spaces are generally endowed with some additional structure such as a topology, which allows the consideration of issues of proximity and continuity. among these topologies, those that are defined by a norm or inner product are more commonly used (being equipped with a notion of distance between two vectors). this is particularly the case of banach spaces and hilbert spaces, which are fundamental in mathematical analysis. historically, the first ideas leading to vector spaces can be traced back as far as the   th century's analytic geometry, matrices, systems of linear equations, and euclidean vectors. the modern, more abstract treatment, first formulated by giuseppe peano in     , encompasses more general objects than euclidean space, but much of the theory can be seen as an extension of classical geometric ideas like lines, planes and their higher-dimensional analogs. today, vector spaces are applied throughout mathematics, science and engineering. they are the appropriate linear-algebraic notion to deal with systems of linear equations. they offer a framework for fourier expansion, which is employed in image compression routines, and they provide an environment that can be used for solution techniques for partial differential equations. furthermore, vector spaces furnish an abstract, coordinate-free way of dealing with geometrical and physical objects such as tensors. this in turn allows the examination of local properties of manifolds by linearization techniques. vector spaces may be generalized in several ways, leading to more advanced notions in geometry and abstract algebra. this article deals mainly with finite-dimensional vector spaces. however, many of the principles are also valid for infinite-dimensional vector spaces. two typical vector space examples are described first. the first example of a vector space consists of arrows in a fixed plane, starting at one fixed point. this is used in physics to describe forces or velocities. given any two such arrows, v and w, the parallelogram spanned by these two arrows contains one diagonal arrow that starts at the origin, too. this new arrow is called the sum of the two arrows, and is denoted v + w. in the special case of two arrows on the same line, their sum is the arrow on this line whose length is the sum or the difference of the lengths, depending on whether the arrows have the same direction. another operation that can be done with arrows is scaling: given any positive real number a, the arrow that has the same direction as v, but is dilated or shrunk by multiplying its length by a, is called multiplication of v by a. it is denoted av. when a is negative, av is defined as the arrow pointing in the opposite direction instead. the following shows a few examples: if a =  , the resulting vector aw has the same direction as w, but is stretched to the double length of w (right image below). equivalently,  w is the sum w + w. moreover, (− )v = −v has the opposite direction and the same length as v (blue vector pointing down in the right image). a second key example of a vector space is provided by pairs of real numbers x and y. (the order of the components x and y is significant, so such a pair is also called an ordered pair.) such a pair is written as (x, y). the sum of two such pairs and multiplication of a pair with a number is defined as follows: "
458,458,Vieta Formula,1,https://en.wikipedia.org/wiki/Vieta%27s_formulas,"in mathematics, vieta's formulas are formulas that relate the coefficients of a polynomial to sums and products of its roots. named after françois viète (more commonly referred to by the latinised form of his name, ""franciscus vieta""), the formulas are used specifically in algebra. any general polynomial of degree n (with the coefficients being real or complex numbers and an ≠  ) has n (not necessarily distinct) complex roots r , r , ..., rn by the fundamental theorem of algebra. vieta's formulas relate polynomial's coefficients to signed sums of products of the roots r , r , ..., rn as follows: vieta's formulas can equivalently be written as for k =  ,  , ..., n (the indices ik are sorted in increasing order to ensure each product of k roots is used exactly once). the left-hand sides of vieta's formulas are the elementary symmetric polynomials of the roots. vieta's formulas are frequently used with polynomials with coefficients in any integral domain r. then, the quotients a i / a n {\displaystyle a_{i}/a_{n}} belong to the field of fractions of r (and possibly are in r itself if a n {\displaystyle a_{n}} happens to be invertible in r) and the roots r i {\displaystyle r_{i}} are taken in an algebraically closed extension. typically, r is the ring of the integers, the field of fractions is the field of the rational numbers and the algebraically closed field is the field of the complex numbers. vieta's formulas are then useful because they provide relations between the roots without having to compute them. for polynomials over a commutative ring that is not an integral domain, vieta's formulas are only valid when a n {\displaystyle a_{n}} is not a zero-divisor and p ( x ) {\displaystyle p(x)} factors as a n ( x − r   ) ( x − r   ) … ( x − r n ) {\displaystyle a_{n}(x-r_{ })(x-r_{ })\dots (x-r_{n})} . for example, in the ring of the integers modulo  , the quadratic polynomial p ( x ) = x   −   {\displaystyle p(x)=x^{ }- } has four roots:  ,  ,  , and  . vieta's formulas are not true if, say, r   =   {\displaystyle r_{ }= } and r   =   {\displaystyle r_{ }= } , because p ( x ) ≠ ( x −   ) ( x −   ) {\displaystyle p(x)\neq (x- )(x- )} . however, p ( x ) {\displaystyle p(x)} does factor as ( x −   ) ( x −   ) {\displaystyle (x- )(x- )} and also as ( x −   ) ( x −   ) {\displaystyle (x- )(x- )} , and vieta's formulas hold if we set either r   =   {\displaystyle r_{ }= } and r   =   {\displaystyle r_{ }= } or r   =   {\displaystyle r_{ }= } and r   =   {\displaystyle r_{ }= } . vieta's formulas applied to quadratic and cubic polynomials: "
459,459,Wald-Wolfowitz Run Test,1,https://en.wikipedia.org/wiki/Wald%E2%80%93Wolfowitz_runs_test,"the wald–wolfowitz runs test (or simply runs test), named after statisticians abraham wald and jacob wolfowitz is a non-parametric statistical test that checks a randomness hypothesis for a two-valued data sequence. more precisely, it can be used to test the hypothesis that the elements of the sequence are mutually independent. a run of a sequence is a maximal non-empty segment of the sequence consisting of adjacent equal elements. for example, the   -element-long sequence consists of   runs,   of which consist of ""+"" and the others of ""−"". the run test is based on the null hypothesis that each element in the sequence is independently drawn from the same distribution. under the null hypothesis, the number of runs in a sequence of n elements[note  ] is a random variable whose conditional distribution given the observation of n+ positive values[note  ] and n− negative values (n = n+ + n−) is approximately normal, with: these parameters do not assume that the positive and negative elements have equal probabilities of occurring, but only assume that the elements are independent and identically distributed. if the number of runs is significantly higher or lower than expected, the hypothesis of statistical independence of the elements may be rejected. runs tests can be used to test: the kolmogorov–smirnov test has been shown to be more powerful than the wald–wolfowitz test for detecting differences between distributions that differ solely in their location. however, the reverse is true if the distributions differ in variance and have at the most only a small difference in location.[citation needed] the wald–wolfowitz runs test has been extended for use with several samples. "
460,460,Weibull distribution,2,https://en.wikipedia.org/wiki/Weibull_distribution,"in probability theory and statistics, the weibull distribution /ˈwaɪbʊl/ is a continuous probability distribution. it is named after swedish mathematician waloddi weibull, who described it in detail in     , although it was first identified by fréchet and first applied by rosin & rammler (    ) to describe a particle size distribution. the probability density function of a weibull random variable is: where k >   is the shape parameter and λ >   is the scale parameter of the distribution. its complementary cumulative distribution function is a stretched exponential function. the weibull distribution is related to a number of other probability distributions; in particular, it interpolates between the exponential distribution (k =  ) and the rayleigh distribution (k =   and λ =   σ {\displaystyle \lambda ={\sqrt { }}\sigma } ). if the quantity x is a ""time-to-failure"", the weibull distribution gives a distribution for which the failure rate is proportional to a power of time. the shape parameter, k, is that power plus one, and so this parameter can be interpreted directly as follows: in the field of materials science, the shape parameter k of a distribution of strengths is known as the weibull modulus. in the context of diffusion of innovations, the weibull distribution is a ""pure"" imitation/rejection model. applications in medical statistics and econometrics often adopt a different parameterization. the shape parameter k is the same as above, while the scale parameter is b = λ − k {\displaystyle b=\lambda ^{-k}} . in this case, for x ≥  , the probability density function is the cumulative distribution function is the hazard function is and the mean is a third parameterization can also be found. the shape parameter k is the same as in the standard case, while the scale parameter λ is replaced with a rate parameter β =  /λ. then, for x ≥  , the probability density function is "
461,461,Weighted Arithmetic Mean,1,https://en.wikipedia.org/wiki/Weighted_arithmetic_mean,"the weighted arithmetic mean is similar to an ordinary arithmetic mean (the most common type of average), except that instead of each of the data points contributing equally to the final average, some data points contribute more than others. the notion of weighted mean plays a role in descriptive statistics and also occurs in a more general form in several other areas of mathematics. if all the weights are equal, then the weighted mean is the same as the arithmetic mean. while weighted means generally behave in a similar fashion to arithmetic means, they do have a few counterintuitive properties, as captured for instance in simpson's paradox. given two school classes — one with    students, one with    students — and test grades in each class as follows: the mean for the morning class is    and the mean of the afternoon class is   . the unweighted mean of the two means is   . however, this does not account for the difference in number of students in each class (   versus   ); hence the value of    does not reflect the average student grade (independent of class). the average student grade can be obtained by averaging all the grades, without regard to classes (add all the grades up and divide by the total number of students): or, this can be accomplished by weighting the class means by the number of students in each class. the larger class is given more ""weight"": thus, the weighted mean makes it possible to find the mean average student grade without knowing each student's score. only the class means and the number of students in each class are needed. since only the relative weights are relevant, any weighted mean can be expressed using coefficients that sum to one. such a linear combination is called a convex combination. using the previous example, we would get the following weights: "
462,462,Wiener-Khinchin theorem,3,https://en.wikipedia.org/wiki/Wiener%E2%80%93Khinchin_theorem,"in applied mathematics, the wiener–khinchin theorem, also known as the wiener–khintchine theorem and sometimes as the wiener–khinchin–einstein theorem or the khinchin–kolmogorov theorem, states that the autocorrelation function of a wide-sense-stationary random process has a spectral decomposition given by the power spectrum of that process. norbert wiener proved this theorem for the case of a deterministic function in     ; aleksandr khinchin later formulated an analogous result for stationary stochastic processes and published that probabilistic analogue in     . albert einstein explained, without proofs, the idea in a brief two-page memo in     . for continuous time, the wiener–khinchin theorem says that if x {\displaystyle x} is a wide-sense stochastic process whose autocorrelation function (sometimes called autocovariance) defined in terms of statistical expected value, r x x ( τ ) = e ⁡ [ x ( t ) ∗ x ( t − τ ) ] {\displaystyle r_{xx}(\tau )=\operatorname {e} {\big [}x(t)^{*}x(t-\tau ){\big ]}} (the asterisk denotes complex conjugate, and of course it can be omitted if the random process is real-valued), exists and is finite at every lag τ {\displaystyle \tau } , then there exists a monotone function f ( f ) {\displaystyle f(f)} in the frequency domain − ∞ < f < ∞ {\displaystyle -\infty <f<\infty } such that where the integral is a riemann–stieltjes integral. this is a kind of spectral decomposition of the auto-correlation function. f is called the power spectral distribution function and is a statistical distribution function. it is sometimes called the integrated spectrum. the fourier transform of x ( t ) {\displaystyle x(t)} does not exist in general, because stochastic random functions are not generally either square-integrable or absolutely integrable. nor is r x x {\displaystyle r_{xx}} assumed to be absolutely integrable, so it need not have a fourier transform either. but if f ( f ) {\displaystyle f(f)} is absolutely continuous, for example, if the process is purely indeterministic, then f {\displaystyle f} is differentiable almost everywhere. in this case, one can define s ( f ) {\displaystyle s(f)} , the power spectral density of x ( t ) {\displaystyle x(t)} , by taking the averaged derivative of f {\displaystyle f} . because the left and right derivatives of f {\displaystyle f} exist everywhere, we can put s ( f ) =     ( lim ε ↓     ε ( f ( f + ε ) − f ( f ) ) + lim ε ↑     ε ( f ( f + ε ) − f ( f ) ) ) {\displaystyle s(f)={\frac { }{ }}\left(\lim _{\varepsilon \downarrow  }{\frac { }{\varepsilon }}{\big (}f(f+\varepsilon )-f(f){\big )}+\lim _{\varepsilon \uparrow  }{\frac { }{\varepsilon }}{\big (}f(f+\varepsilon )-f(f){\big )}\right)} everywhere, (obtaining that f is the integral of its averaged derivative ), and the theorem simplifies to if now one assumes that r and s satisfy the necessary conditions for fourier inversion to be valid, the wiener–khinchin theorem takes the simple form of saying that r and s are a fourier-transform pair, and for the discrete-time case, the power spectral density of the function with discrete values x [ n ] {\displaystyle x[n]} is where is the discrete autocorrelation function of x [ n ] {\displaystyle x[n]} , provided this is absolutely integrable. being a sampled and discrete-time sequence, the spectral density is periodic in the frequency domain. this is due to the problem of aliasing: the contribution of any frequency higher than the nyquist frequency seems to be equal to that of its alias between   and  . for this reason, the domain of the function s {\displaystyle s} is usually restricted to lie between   and   or between − .  and  . . "
463,463,Zermelo Frankel Set Theory,0,https://en.wikipedia.org/wiki/Zermelo%E2%80%93Fraenkel_set_theory,"in set theory, zermelo–fraenkel set theory, named after mathematicians ernst zermelo and abraham fraenkel, is an axiomatic system that was proposed in the early twentieth century in order to formulate a theory of sets free of paradoxes such as russell's paradox. today, zermelo–fraenkel set theory, with the historically controversial axiom of choice (ac) included, is the standard form of axiomatic set theory and as such is the most common foundation of mathematics. zermelo–fraenkel set theory with the axiom of choice included is abbreviated zfc, where c stands for ""choice"", and zf refers to the axioms of zermelo–fraenkel set theory with the axiom of choice excluded. zermelo–fraenkel set theory is intended to formalize a single primitive notion, that of a hereditary well-founded set, so that all entities in the universe of discourse are such sets. thus the axioms of zermelo–fraenkel set theory refer only to pure sets and prevent its models from containing urelements (elements of sets that are not themselves sets). furthermore, proper classes (collections of mathematical objects defined by a property shared by their members where the collections are too big to be sets) can only be treated indirectly. specifically, zermelo–fraenkel set theory does not allow for the existence of a universal set (a set containing all sets) nor for unrestricted comprehension, thereby avoiding russell's paradox. von neumann–bernays–gödel set theory (nbg) is a commonly used conservative extension of zermelo–fraenkel set theory that does allow explicit treatment of proper classes. there are many equivalent formulations of the axioms of zermelo–fraenkel set theory. most of the axioms state the existence of particular sets defined from other sets. for example, the axiom of pairing says that given any two sets a {\displaystyle a} and b {\displaystyle b} there is a new set { a , b } {\displaystyle \{a,b\}} containing exactly a {\displaystyle a} and b {\displaystyle b} . other axioms describe properties of set membership. a goal of the axioms is that each axiom should be true if interpreted as a statement about the collection of all sets in the von neumann universe (also known as the cumulative hierarchy). formally, zfc is a one-sorted theory in first-order logic. the signature has equality and a single primitive binary relation, set membership, which is usually denoted ∈ {\displaystyle \in } . the formula a ∈ b {\displaystyle a\in b} means that the set a {\displaystyle a} is a member of the set b {\displaystyle b} (which is also read, "" a {\displaystyle a} is an element of b {\displaystyle b} "" or "" a {\displaystyle a} is in b {\displaystyle b} ""). the metamathematics of zermelo–fraenkel set theory has been extensively studied. landmark results in this area established the logical independence of the axiom of choice from the remaining zermelo-fraenkel axioms (see axiom of choice § independence) and of the continuum hypothesis from zfc. the consistency of a theory such as zfc cannot be proved within the theory itself, as shown by gödel's second incompleteness theorem. the modern study of set theory was initiated by georg cantor and richard dedekind in the     s. however, the discovery of paradoxes in naive set theory, such as russell's paradox, led to the desire for a more rigorous form of set theory that was free of these paradoxes. in     , ernst zermelo proposed the first axiomatic set theory, zermelo set theory. however, as first pointed out by abraham fraenkel in a      letter to zermelo, this theory was incapable of proving the existence of certain sets and cardinal numbers whose existence was taken for granted by most set theorists of the time, notably the cardinal number ℵ ω {\displaystyle \aleph _{\omega }} and the set { z   , p ( z   ) , p ( p ( z   ) ) , p ( p ( p ( z   ) ) ) , . . . } , {\displaystyle \{z_{ },{\mathcal {p}}(z_{ }),{\mathcal {p}}({\mathcal {p}}(z_{ })),{\mathcal {p}}({\mathcal {p}}({\mathcal {p}}(z_{ }))),...\},} where z   {\displaystyle z_{ }} is any infinite set and p {\displaystyle {\mathcal {p}}} is the power set operation. moreover, one of zermelo's axioms invoked a concept, that of a ""definite"" property, whose operational meaning was not clear. in     , fraenkel and thoralf skolem independently proposed operationalizing a ""definite"" property as one that could be formulated as a well-formed formula in a first-order logic whose atomic formulas were limited to set membership and identity. they also independently proposed replacing the axiom schema of specification with the axiom schema of replacement. appending this schema, as well as the axiom of regularity (first proposed by john von neumann), to zermelo set theory yields the theory denoted by zf. adding to zf either the axiom of choice (ac) or a statement that is equivalent to it yields zfc. there are many equivalent formulations of the zfc axioms; for a discussion of this see fraenkel, bar-hillel & lévy     . the following particular axiom set is from kunen (    ). the axioms per se are expressed in the symbolism of first order logic. the associated english prose is only intended to aid the intuition. all formulations of zfc imply that at least one set exists. kunen includes an axiom that directly asserts the existence of a set, in addition to the axioms given below (although he notes that he does so only ""for emphasis""). its omission here can be justified in two ways. first, in the standard semantics of first-order logic in which zfc is typically formalized, the domain of discourse must be nonempty. hence, it is a logical theorem of first-order logic that something exists — usually expressed as the assertion that something is identical to itself, ∃ x ( x = x ) {\displaystyle \exists x(x=x)} . consequently, it is a theorem of every first-order theory that something exists. however, as noted above, because in the intended semantics of zfc there are only sets, the interpretation of this logical theorem in the context of zfc is that some set exists. hence, there is no need for a separate axiom asserting that a set exists. second, however, even if zfc is formulated in so-called free logic, in which it is not provable from logic alone that something exists, the axiom of infinity (below) asserts that an infinite set exists. this implies that a set exists and so, once again, it is superfluous to include an axiom asserting as much. two sets are equal (are the same set) if they have the same elements. the converse of this axiom follows from the substitution property of equality. if the background logic[definition needed] does not include equality "" = {\displaystyle =} "", x = y {\displaystyle x=y} may be defined as an abbreviation for the following formula: ∀ z [ z ∈ x ⇔ z ∈ y ] ∧ ∀ w [ x ∈ w ⇔ y ∈ w ] . {\displaystyle \forall z[z\in x\leftrightarrow z\in y]\land \forall w[x\in w\leftrightarrow y\in w].} "
